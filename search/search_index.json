{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Connectome analysis","text":""},{"location":"#general-analises-of-connectomes-from-a-topological-perspective","title":"General analises of connectomes from a topological perspective","text":""},{"location":"#overview","title":"Overview","text":"<p>This package provides a library of general functions to analyze connectomes. Functions are divided into three groups:</p> <ul> <li>Modelling: Functions to model (parametrize) the connectivity of connectomes</li> <li>Randomization: Generation of randomized controls of connectomes</li> <li>Network: Network analyses based on metrics of different types</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Check out a few short tutorials showing: </p> <ul> <li>How to implement most unweighted topological network metrics.</li> <li>How to compute  triad counts.</li> <li>How to  model and extract distance dependent parameters from connectoms with a geometric embedding.</li> <li>How to generate  randomized controls of a given connectome.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install, in your terminal run: </p> <pre><code>pip install git+https://github.com/danielaegassan/connectome_analysis.git\n</code></pre> <p>For the installation to work you require: </p> <ul> <li>gcc 9+</li> <li>CMake</li> <li>Python 3.8+</li> </ul> <p>Python requirements will be installed directly and are listed  here.</p>"},{"location":"modelling/","title":"Modelling","text":""},{"location":"modelling/#this-page-describes-functions-contained-in-the-modelling-module-used-to-model-or-parametrize-the-connectivity-of-connectomes","title":"This page describes functions contained in the <code>modelling</code> module used to  model  or parametrize the connectivity of connectomes","text":""},{"location":"modelling/#src.connalysis.modelling.modelling._build_2nd_order","title":"<code>_build_2nd_order(p_conn_dist, dist_bins, **_)</code>","text":"<p>Build 2nd order model (exponential distance-dependent conn. prob.).</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _build_2nd_order(p_conn_dist, dist_bins, **_):\n    \"\"\"Build 2nd order model (exponential distance-dependent conn. prob.).\"\"\"\n    bin_offset = 0.5 * np.diff(dist_bins[:2])[0]\n\n    exp_model = lambda x, a, b: a * np.exp(-b * np.array(x))\n    X = dist_bins[:-1][np.isfinite(p_conn_dist)] + bin_offset\n    y = p_conn_dist[np.isfinite(p_conn_dist)]\n    try:\n        (exp_model_scale, exp_model_exponent), _ = opt.curve_fit(exp_model, X, y, p0=[0.0, 0.0])\n    except:\n        logging.error(f'Exception while fitting model (\"{sys.exc_info()[1]}\")')\n        exp_model_scale = exp_model_exponent = np.nan\n\n    logging.info(f'MODEL FIT: f(x) = {exp_model_scale:.6f} * exp(-{exp_model_exponent:.6f} * x)')\n\n    model = 'exp_model_scale * np.exp(-exp_model_exponent * np.array(d))'\n    model_inputs = ['d']\n    model_params = {'exp_model_scale': exp_model_scale, 'exp_model_exponent': exp_model_exponent}\n\n    return {'model': model, 'model_inputs': model_inputs, 'model_params': model_params}\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._build_3rd_order","title":"<code>_build_3rd_order(p_conn_dist_bip, dist_bins, **_)</code>","text":"<p>Build 3rd order model (bipolar exp. distance-dependent conn. prob.).</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _build_3rd_order(p_conn_dist_bip, dist_bins, **_):\n    \"\"\"Build 3rd order model (bipolar exp. distance-dependent conn. prob.).\"\"\"\n    bin_offset = 0.5 * np.diff(dist_bins[:2])[0]\n\n    X = dist_bins[:-1][np.all(np.isfinite(p_conn_dist_bip), 1)] + bin_offset\n    y = p_conn_dist_bip[np.all(np.isfinite(p_conn_dist_bip), 1), :]\n\n    exp_model = lambda x, a, b: a * np.exp(-b * np.array(x))\n    try:\n        (bip_neg_exp_model_scale, bip_neg_exp_model_exponent), _ = opt.curve_fit(exp_model, X, y[:, 0], p0=[0.0, 0.0])\n        (bip_pos_exp_model_scale, bip_pos_exp_model_exponent), _ = opt.curve_fit(exp_model, X, y[:, 1], p0=[0.0, 0.0])\n    except:\n        logging.error(f'Exception while fitting model (\"{sys.exc_info()[1]}\")')\n        bip_neg_exp_model_scale = bip_neg_exp_model_exponent = np.nan\n        bip_pos_exp_model_scale = bip_pos_exp_model_exponent = np.nan\n\n    logging.info(f'BIPOLAR MODEL FIT: f(x, dz) = {bip_neg_exp_model_scale:.6f} * exp(-{bip_neg_exp_model_exponent:.6f} * x) if dz &lt; 0')\n    logging.info(f'                              {bip_pos_exp_model_scale:.6f} * exp(-{bip_pos_exp_model_exponent:.6f} * x) if dz &gt; 0')\n    logging.info('                              AVERAGE OF BOTH MODELS  if dz == 0')\n\n    model = 'np.select([np.array(dz) &lt; 0, np.array(dz) &gt; 0, np.array(dz) == 0], [bip_neg_exp_model_scale * np.exp(-bip_neg_exp_model_exponent * np.array(d)), bip_pos_exp_model_scale * np.exp(-bip_pos_exp_model_exponent * np.array(d)), 0.5 * (bip_neg_exp_model_scale * np.exp(-bip_neg_exp_model_exponent * np.array(d)) + bip_pos_exp_model_scale * np.exp(-bip_pos_exp_model_exponent * np.array(d)))])'\n    model_inputs = ['d', 'dz']\n    model_params = {'bip_neg_exp_model_scale': bip_neg_exp_model_scale, 'bip_neg_exp_model_exponent': bip_neg_exp_model_exponent, 'bip_pos_exp_model_scale': bip_pos_exp_model_scale, 'bip_pos_exp_model_exponent': bip_pos_exp_model_exponent}\n\n    return {'model': model, 'model_inputs': model_inputs, 'model_params': model_params}\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._compute_bip_matrix","title":"<code>_compute_bip_matrix(src_depths, tgt_depths)</code>","text":"<p>Computes bipolar matrix between pairs of neurons based on depth difference delta_d:   POST-synaptic neuron below (delta_d &lt; 0) or above (delta_d &gt; 0) PRE-synaptic neuron</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _compute_bip_matrix(src_depths, tgt_depths):\n    \"\"\"\n    Computes bipolar matrix between pairs of neurons based on depth difference delta_d:\n      POST-synaptic neuron below (delta_d &lt; 0) or above (delta_d &gt; 0) PRE-synaptic neuron\n    \"\"\"\n    bip_mat = np.sign(-np.diff(np.meshgrid(src_depths, tgt_depths, indexing='ij'), axis=0)[0, :, :])\n\n    return bip_mat\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._compute_dist_matrix","title":"<code>_compute_dist_matrix(src_nrn_pos, tgt_nrn_pos)</code>","text":"<p>Computes distance matrix between pairs of neurons.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _compute_dist_matrix(src_nrn_pos, tgt_nrn_pos):\n    \"\"\"Computes distance matrix between pairs of neurons.\"\"\"\n    dist_mat = spt.distance_matrix(src_nrn_pos, tgt_nrn_pos)\n    dist_mat[dist_mat == 0.0] = np.nan # Exclude autaptic connections\n\n    return dist_mat\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._compute_dist_matrix_symmetric","title":"<code>_compute_dist_matrix_symmetric(nrn_pos)</code>","text":"<p>Computes symmetric distance matrix between pairs of neurons. Faster implementation to be used when source and target neurons are the same.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _compute_dist_matrix_symmetric(nrn_pos):\n    \"\"\"Computes symmetric distance matrix between pairs of neurons.\n       Faster implementation to be used when source and target neurons\n       are the same.\"\"\"\n    dist_mat = spt.distance.squareform(spt.distance.pdist(nrn_pos))\n    dist_mat[dist_mat == 0.0] = np.nan # Exclude autaptic connections\n\n    return dist_mat\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._extract_2nd_order","title":"<code>_extract_2nd_order(adj, node_properties, bin_size_um=100, max_range_um=None, coord_names=None, split_indices=None, part_idx=None, **_)</code>","text":"<p>Extract distance-dependent connection probability (2nd order) from a sample of pairs of neurons.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _extract_2nd_order(adj, node_properties, bin_size_um=100, max_range_um=None, coord_names=None, split_indices=None, part_idx=None, **_):\n    \"\"\"Extract distance-dependent connection probability (2nd order) from a sample of pairs of neurons.\"\"\"\n\n    if coord_names is None:\n        coord_names = ['x', 'y', 'z'] # Default names of coordinatate system axes as in node_properties\n    if isinstance(split_indices, list):\n        N_split = len(split_indices)\n    else:\n        N_split = 0 # Don't split\n    if part_idx is not None: # Run only data extraction of given part idx\n        assert 0 &lt;= part_idx &lt; N_split, 'ERROR: Part index out of range!'\n\n    pos_table = node_properties[coord_names].to_numpy()\n\n    if N_split == 0: # Compute all at once\n        # Compute distance matrix\n        dist_mat = _compute_dist_matrix_symmetric(pos_table)\n\n        # Extract distance-dependent connection probabilities\n        if max_range_um is None:\n            max_range_um = np.nanmax(dist_mat)\n        num_bins = np.ceil(max_range_um / bin_size_um).astype(int)\n        dist_bins = np.arange(0, num_bins + 1) * bin_size_um\n\n        p_conn_dist, count_conn, count_all = _extract_dependent_p_conn(adj, [dist_mat], [dist_bins])\n\n    else: # Split computation into N_split data splits (to reduce memory consumption)\n        assert max_range_um is not None, f'ERROR: Max. range must be specified if data extraction splitted into {N_split} parts!'\n        num_bins = np.ceil(max_range_um / bin_size_um).astype(int)\n        dist_bins = np.arange(0, num_bins + 1) * bin_size_um\n\n        count_conn = np.zeros(num_bins, dtype=int)\n        count_all = np.zeros(num_bins, dtype=int)\n        for sidx, split_sel in enumerate(split_indices):\n            if part_idx is not None and part_idx != sidx:\n                continue\n            logging.info(f'&lt;SPLIT {sidx + 1} of {N_split}&gt;')\n\n            # Compute distance matrix\n            dist_mat_split = _compute_dist_matrix(pos_table[split_sel, :], pos_table)\n\n            # Extract distance-dependent connection counts\n            _, count_conn_split, count_all_split = _extract_dependent_p_conn(adj[split_sel, :], [dist_mat_split], [dist_bins])\n            count_conn += count_conn_split\n            count_all += count_all_split\n\n        # Compute overall connection probabilities\n        p_conn_dist = np.array(count_conn / count_all)\n#         p_conn_dist[np.isnan(p_conn_dist)] = 0.0\n\n    return {'p_conn_dist': p_conn_dist, 'count_conn': count_conn, 'count_all': count_all, 'dist_bins': dist_bins}\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._extract_2nd_order_pathway","title":"<code>_extract_2nd_order_pathway(adj, node_properties_src, node_properties_tgt, bin_size_um=100, max_range_um=None, coord_names=None, split_indices=None, part_idx=None, **_)</code>","text":"<p>Extract distance-dependent connection probability (2nd order) from a sample of pairs of neurons for separate pathways (i.e., non-symmetric adj).</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _extract_2nd_order_pathway(adj, node_properties_src, node_properties_tgt, bin_size_um=100, max_range_um=None, coord_names=None, split_indices=None, part_idx=None, **_):\n    \"\"\"Extract distance-dependent connection probability (2nd order) from a sample of pairs of neurons\n       for separate pathways (i.e., non-symmetric adj).\"\"\"\n\n    if coord_names is None:\n        coord_names = ['x', 'y', 'z'] # Default names of coordinatate system axes as in node_properties\n\n    assert split_indices is None and part_idx is None, 'ERROR: Data splitting not supported!'\n\n    pos_table_src = node_properties_src[coord_names].to_numpy()\n    pos_table_tgt = node_properties_tgt[coord_names].to_numpy()\n\n    # Compute distance matrix\n    dist_mat = _compute_dist_matrix(pos_table_src, pos_table_tgt)\n\n    # Extract distance-dependent connection probabilities\n    if max_range_um is None:\n        max_range_um = np.nanmax(dist_mat)\n    num_bins = np.ceil(max_range_um / bin_size_um).astype(int)\n    dist_bins = np.arange(0, num_bins + 1) * bin_size_um\n\n    p_conn_dist, count_conn, count_all = _extract_dependent_p_conn(adj, [dist_mat], [dist_bins])\n\n    return {'p_conn_dist': p_conn_dist, 'count_conn': count_conn, 'count_all': count_all, 'dist_bins': dist_bins}\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._extract_3rd_order","title":"<code>_extract_3rd_order(adj, node_properties, bin_size_um=100, max_range_um=None, coord_names=None, depth_name=None, split_indices=None, part_idx=None, **_)</code>","text":"<p>Extract distance-dependent connection probability (3rd order) from a sample of pairs of neurons.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _extract_3rd_order(adj, node_properties, bin_size_um=100, max_range_um=None, coord_names=None, depth_name=None, split_indices=None, part_idx=None, **_):\n    \"\"\"Extract distance-dependent connection probability (3rd order) from a sample of pairs of neurons.\"\"\"\n\n    if coord_names is None:\n        coord_names = ['x', 'y', 'z'] # Default names of coordinatate system axes as in node_properties\n    if depth_name is None:\n        depth_name = 'depth' # Default name of depth column in node_properties\n    if isinstance(split_indices, list):\n        N_split = len(split_indices)\n    else:\n        N_split = 0 # Don't split\n    if part_idx is not None: # Run only data extraction of given part idx\n        assert 0 &lt;= part_idx &lt; N_split, 'ERROR: Part index out of range!'\n\n    pos_table = node_properties[coord_names].to_numpy()\n    depth_table = node_properties[depth_name].to_numpy()\n\n    if N_split == 0: # Compute all at once\n        # Compute distance matrix\n        dist_mat = _compute_dist_matrix_symmetric(pos_table)\n\n        # Compute bipolar matrix (post-synaptic neuron below (delta_d &lt; 0) or above (delta_d &gt; 0) pre-synaptic neuron)\n        bip_mat = _compute_bip_matrix(depth_table, depth_table)\n\n        # Extract bipolar distance-dependent connection probabilities\n        if max_range_um is None:\n            max_range_um = np.nanmax(dist_mat)\n        num_dist_bins = np.ceil(max_range_um / bin_size_um).astype(int)\n        dist_bins = np.arange(0, num_dist_bins + 1) * bin_size_um\n        bip_bins = [np.nanmin(bip_mat), 0, np.nanmax(bip_mat)]\n\n        p_conn_dist_bip, count_conn, count_all = _extract_dependent_p_conn(adj, [dist_mat, bip_mat], [dist_bins, bip_bins])\n\n    else: # Split computation into N_split data splits (to reduce memory consumption)\n        assert max_range_um is not None, f'ERROR: Max. range must be specified if data extraction splitted into {N_split} parts!'\n        num_dist_bins = np.ceil(max_range_um / bin_size_um).astype(int)\n        dist_bins = np.arange(0, num_dist_bins + 1) * bin_size_um\n        bip_bins = [-1, 0, 1]\n\n        count_conn = np.zeros([num_dist_bins, 2], dtype=int)\n        count_all = np.zeros([num_dist_bins, 2], dtype=int)\n        for sidx, split_sel in enumerate(split_indices):\n            if part_idx is not None and part_idx != sidx:\n                continue\n            logging.info(f'&lt;SPLIT {sidx + 1} of {N_split}&gt;')\n\n            # Compute distance matrix\n            dist_mat_split = _compute_dist_matrix(pos_table[split_sel, :], pos_table)\n\n            # Compute bipolar matrix (post-synaptic neuron below (delta_d &lt; 0) or above (delta_d &gt; 0) pre-synaptic neuron)\n            bip_mat_split = _compute_bip_matrix(depth_table[split_sel], depth_table)\n\n            # Extract distance-dependent connection counts\n            _, count_conn_split, count_all_split = _extract_dependent_p_conn(adj[split_sel, :], [dist_mat_split, bip_mat_split], [dist_bins, bip_bins])\n            count_conn += count_conn_split\n            count_all += count_all_split\n\n        # Compute overall connection probabilities\n        p_conn_dist_bip = np.array(count_conn / count_all)\n#         p_conn_dist_bip[np.isnan(p_conn_dist_bip)] = 0.0\n\n    return {'p_conn_dist_bip': p_conn_dist_bip, 'count_conn': count_conn, 'count_all': count_all, 'dist_bins': dist_bins, 'bip_bins': bip_bins}\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._extract_3rd_order_pathway","title":"<code>_extract_3rd_order_pathway(adj, node_properties_src, node_properties_tgt, bin_size_um=100, max_range_um=None, coord_names=None, depth_name=None, split_indices=None, part_idx=None, **_)</code>","text":"<p>Extract distance-dependent connection probability (3rd order) from a sample of pairs of neurons for separate pathways (i.e., non-symmetric adj).</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _extract_3rd_order_pathway(adj, node_properties_src, node_properties_tgt, bin_size_um=100, max_range_um=None, coord_names=None, depth_name=None, split_indices=None, part_idx=None, **_):\n    \"\"\"Extract distance-dependent connection probability (3rd order) from a sample of pairs of neurons\n       for separate pathways (i.e., non-symmetric adj).\"\"\"\n\n    if coord_names is None:\n        coord_names = ['x', 'y', 'z'] # Default names of coordinatate system axes as in node_properties\n    if depth_name is None:\n        depth_name = 'depth' # Default name of depth column in node_properties\n\n    assert split_indices is None and part_idx is None, 'ERROR: Data splitting not supported!'\n\n    pos_table_src = node_properties_src[coord_names].to_numpy()\n    pos_table_tgt = node_properties_tgt[coord_names].to_numpy()\n    depth_table_src = node_properties_src[depth_name].to_numpy()\n    depth_table_tgt = node_properties_tgt[depth_name].to_numpy()\n\n    # Compute distance matrix\n    dist_mat = _compute_dist_matrix(pos_table_src, pos_table_tgt)\n\n    # Compute bipolar matrix (post-synaptic neuron below (delta_d &lt; 0) or above (delta_d &gt; 0) pre-synaptic neuron)\n    bip_mat = _compute_bip_matrix(depth_table_src, depth_table_tgt)\n\n    # Extract bipolar distance-dependent connection probabilities\n    if max_range_um is None:\n        max_range_um = np.nanmax(dist_mat)\n    num_dist_bins = np.ceil(max_range_um / bin_size_um).astype(int)\n    dist_bins = np.arange(0, num_dist_bins + 1) * bin_size_um\n    bip_bins = [np.nanmin(bip_mat), 0, np.nanmax(bip_mat)]\n\n    p_conn_dist_bip, count_conn, count_all = _extract_dependent_p_conn(adj, [dist_mat, bip_mat], [dist_bins, bip_bins])\n\n    return {'p_conn_dist_bip': p_conn_dist_bip, 'count_conn': count_conn, 'count_all': count_all, 'dist_bins': dist_bins, 'bip_bins': bip_bins}\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._extract_dependent_p_conn","title":"<code>_extract_dependent_p_conn(adj, dep_matrices, dep_bins)</code>","text":"<p>Extract D-dimensional conn. prob. dependent on D property matrices between source-target pairs of neurons within given range of bins.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _extract_dependent_p_conn(adj, dep_matrices, dep_bins):\n    \"\"\"Extract D-dimensional conn. prob. dependent on D property matrices between source-target pairs of neurons within given range of bins.\"\"\"\n    num_dep = len(dep_matrices)\n    assert len(dep_bins) == num_dep, 'ERROR: Dependencies/bins mismatch!'\n    assert np.all([dep_matrices[dim].shape == adj.shape for dim in range(num_dep)]), 'ERROR: Matrix dimension mismatch!'\n\n    # Extract connection probability\n    num_bins = [len(b) - 1 for b in dep_bins]\n    bin_indices = [list(range(n)) for n in num_bins]\n    count_all = np.full(num_bins, -1) # Count of all pairs of neurons for each combination of dependencies\n    count_conn = np.full(num_bins, -1) # Count of connected pairs of neurons for each combination of dependencies\n\n    logging.info(f'Extracting {num_dep}-dimensional ({\"x\".join([str(n) for n in num_bins])}) connection probabilities...')\n    pbar = progressbar.ProgressBar(maxval=np.prod(num_bins) - 1)\n    for idx in pbar(itertools.product(*bin_indices)):\n        dep_sel = np.full(adj.shape, True)\n        for dim in range(num_dep):\n            lower = dep_bins[dim][idx[dim]]\n            upper = dep_bins[dim][idx[dim] + 1]\n            dep_sel = np.logical_and(dep_sel, np.logical_and(dep_matrices[dim] &gt;= lower, (dep_matrices[dim] &lt; upper) if idx[dim] &lt; num_bins[dim] - 1 else (dep_matrices[dim] &lt;= upper))) # Including last edge\n        sidx, tidx = np.nonzero(dep_sel)\n        count_all[idx] = np.sum(dep_sel)\n        ### count_conn[idx] = np.sum(adj[sidx, tidx]) # ERROR in scipy/sparse/compressed.py if len(sidx) &gt;= 2**31: \"ValueError: could not convert integer scalar\"\n        # [WORKAROUND]: Split indices into parts of 2**31-1 length and sum them separately\n        sidx_split = np.split(sidx, np.arange(0, len(sidx), 2**31-1)[1:])\n        tidx_split = np.split(tidx, np.arange(0, len(tidx), 2**31-1)[1:])\n        count_split = 0\n        for s, t in zip(sidx_split, tidx_split):\n            count_split = count_split + np.sum(adj[s, t])\n        count_conn[idx] = count_split\n    p_conn = np.array(count_conn / count_all)\n#     p_conn[np.isnan(p_conn)] = 0.0\n\n    return p_conn, count_conn, count_all\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._generate_seeds","title":"<code>_generate_seeds(num_seeds, num_digits=6, meta_seed=0)</code>","text":"<p>Helper function to generate list of unique random seeds with given number of digits.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _generate_seeds(num_seeds, num_digits=6, meta_seed=0):\n    \"\"\"Helper function to generate list of unique random seeds with given number of digits.\"\"\"\n\n    assert isinstance(num_seeds, int) and num_seeds &gt; 0, 'ERROR: Number of seeds must be a positive integer!'\n    assert isinstance(num_digits, int) and num_digits &gt; 0, 'ERROR: Number of digits must be a positive integer!'\n\n    np.random.seed(meta_seed)\n    sample_seeds = list(sorted(np.random.choice(10**num_digits - 10**(num_digits - 1), num_seeds, replace=False) + 10**(num_digits - 1))) # Sorted, 6-digit seeds\n\n    return sample_seeds\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._get_data_part_name","title":"<code>_get_data_part_name(N_split, part_idx)</code>","text":"<p>Returns part name of a data split.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _get_data_part_name(N_split, part_idx):\n    \"\"\"Returns part name of a data split.\"\"\"\n    num_dig = len(str(N_split))\n    return f'__part-{N_split}-{part_idx:0{num_dig}}'\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._get_model_function","title":"<code>_get_model_function(model, model_inputs, model_params)</code>","text":"<p>Returns model function from string representation [so any model function can be saved to file].</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _get_model_function(model, model_inputs, model_params):\n    \"\"\"Returns model function from string representation [so any model function can be saved to file].\"\"\"\n    input_str = ','.join(model_inputs + ['model_params=model_params']) # String representation of input variables\n    input_param_str = ','.join(model_inputs + list(model_params.keys())) # String representation of input variables and model parameters\n    model_param_str = ','.join(model_inputs + ['**model_params']) # String representation propagating model parameters\n\n    inner_model_str = f'lambda {input_param_str}: {model}'\n    full_model_str = f'lambda {input_str}: ({inner_model_str})({model_param_str})' # Use nested lambdas to bind local variables\n\n    model_fct = eval(full_model_str) # Build function\n\n    # logging.info(f'Model function: {inner_model_str}')\n\n    return model_fct\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._merge_data","title":"<code>_merge_data(part_dir, model_name, spec_name, part_list)</code>","text":"<p>Merges data from different data splits.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _merge_data(part_dir, model_name, spec_name, part_list):\n    \"\"\"Merges data from different data splits.\"\"\"\n    logging.info(f'Merging {len(part_list)} data parts...')\n\n    count_conn_key = 'count_conn' # (fixed name; independent of model)\n    count_all_key = 'count_all' # (fixed name; independent of model)\n    p_key = None # Name of conn. prob. entry (model-dependent; starting with \"p_\")\n    for part in part_list:\n        part_file = os.path.join(part_dir, f'{model_name}__{spec_name}{part}.pickle')\n        assert os.path.exists(part_file), f'ERROR: Data part \"{part_file}\" not found!'\n        with open(part_file, 'rb') as f:\n            part_dict = pickle.load(f)\n\n        # Determine key names and initialize\n        if p_key is None:\n            part_keys = list(part_dict.keys())\n            p_idx = np.where(np.array([str.find(key, 'p_') for key in part_keys]) == 0)[0]\n            assert len(p_idx) == 1, 'ERROR: Conn. prob. entry could not be determined!'\n            p_key = part_keys[p_idx[0]]\n            other_keys = np.setdiff1d(part_keys, [p_key, count_conn_key, count_all_key]).tolist()\n            other_dict = {key: part_dict[key] for key in other_keys}\n            num_bins = len(part_dict[p_key])\n            count_conn = np.zeros_like(part_dict[count_conn_key])\n            count_all = np.zeros_like(part_dict[count_all_key])\n\n        # Check consistency\n        assert p_key in part_dict.keys(), f'ERROR: \"{p_key}\" not found in part \"{part}\"!'\n        assert count_conn_key in part_dict.keys(), f'ERROR: \"{count_conn_key}\" not found in part \"{part}\"!'\n        assert count_all_key in part_dict.keys(), f'ERROR: \"{count_all_key}\" not found in part \"{part}\"!'\n        for key in other_keys:\n            assert np.array_equal(other_dict[key], part_dict[key]), f'ERROR: \"{key}\" mismatch in part \"{part}\"!'\n        assert part_dict[count_conn_key].shape == part_dict[count_all_key].shape == count_conn.shape == count_all.shape, f'ERROR: Bin count mismatch in part \"{part}\"!'\n\n        # Add counts\n        count_conn += part_dict[count_conn_key]\n        count_all += part_dict[count_all_key]\n\n    # Compute overall (merged) connection probabilities\n    p_conn = np.array(count_conn / count_all)\n#     p_conn[np.isnan(p_conn)] = 0.0\n\n    # Create (merged) data dict\n    data_dict = {p_key: p_conn, count_conn_key: count_conn, count_all_key: count_all, **other_dict}\n\n    return data_dict\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._plot_2nd_order","title":"<code>_plot_2nd_order(adj, node_properties, model_name, p_conn_dist, count_conn, count_all, dist_bins, model, model_inputs, model_params, plot_dir=None, **_)</code>","text":"<p>Visualize data vs. model (2nd order).</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _plot_2nd_order(adj, node_properties, model_name, p_conn_dist, count_conn, count_all, dist_bins, model, model_inputs, model_params, plot_dir=None, **_):\n    \"\"\"Visualize data vs. model (2nd order).\"\"\"\n    if plot_dir is not None:\n        if not os.path.exists(plot_dir):\n            os.makedirs(plot_dir)\n\n    bin_offset = 0.5 * np.diff(dist_bins[:2])[0]\n    dist_model = np.linspace(dist_bins[0], dist_bins[-1], 100)\n\n    model_str = f'f(x) = {model_params[\"exp_model_scale\"]:.3f} * exp(-{model_params[\"exp_model_exponent\"]:.3f} * x)'\n    model_fct = _get_model_function(model, model_inputs, model_params)\n\n    if isinstance(node_properties, list):\n        N_pre = node_properties[0].shape[0]  # Pre-synaptic population\n        N_post = node_properties[1].shape[0]  # Post-synaptic population\n    else:\n        N_pre = N_post = node_properties.shape[0]\n\n    plt.figure(figsize=(12, 4), dpi=300)\n\n    # Data vs. model\n    plt.subplot(1, 2, 1)\n    plt.step(dist_bins, np.hstack([p_conn_dist[0], p_conn_dist]), color=DATA_COLOR, label=f'Data: N = {N_pre}x{N_post} cells')\n    plt.plot(dist_bins[:-1] + bin_offset, p_conn_dist, '.', color=DATA_COLOR)\n    plt.plot(dist_model, model_fct(dist_model), '--', color=MODEL_COLOR, label='Model: ' + model_str)\n    plt.grid()\n    plt.xlabel('Distance ($\\\\mu$m)')\n    plt.ylabel('Conn. prob.')\n    plt.title('Data vs. model fit')\n    plt.legend()\n\n    # 2D connection probability (model)\n    plt.subplot(1, 2, 2)\n    plot_range = 500 # (um)\n    r_markers = [200, 400] # (um)\n    dx = np.linspace(-plot_range, plot_range, 201)\n    dz = np.linspace(plot_range, -plot_range, 201)\n    xv, zv = np.meshgrid(dx, dz)\n    vdist = np.sqrt(xv**2 + zv**2)\n    pdist = model_fct(vdist)\n    plt.imshow(pdist, interpolation='bilinear', extent=(-plot_range, plot_range, -plot_range, plot_range), cmap=PROB_CMAP, vmin=0.0)\n    for r in r_markers:\n        plt.gca().add_patch(plt.Circle((0, 0), r, edgecolor='w', linestyle='--', fill=False))\n        plt.text(0, r, f'{r} $\\\\mu$m', color='w', ha='center', va='bottom')\n    plt.xticks([])\n    plt.yticks([])\n    plt.xlabel('$\\\\Delta$x')\n    plt.ylabel('$\\\\Delta$z')\n    plt.title('2D model')\n    plt.colorbar(label='Conn. prob.')\n\n    plt.suptitle(f'Distance-dependent connection probability model (2nd order)')\n    plt.tight_layout()\n    if plot_dir is not None:\n        out_fn = os.path.abspath(os.path.join(plot_dir, model_name + '__data_vs_model.png'))\n        plt.savefig(out_fn)\n        logging.info(f'Figure saved to {out_fn}')\n\n    # Data counts\n    plt.figure(figsize=(12, 4), dpi=300)\n    plt.bar(dist_bins[:-1] + bin_offset, count_all, width=2.0 * bin_offset, edgecolor='k', label='All pair count')\n    plt.bar(dist_bins[:-1] + bin_offset, count_conn, width=1.5 * bin_offset, label='Connection count')\n    plt.gca().set_yscale('log')\n    plt.xticks(dist_bins, rotation=45)\n    plt.grid()\n    plt.xlabel('Distance ($\\\\mu$m)')\n    plt.ylabel('Count')\n    plt.title(f'Distance-dependent connection counts (N = {N_pre}x{N_post} cells)')\n    plt.legend()\n    plt.tight_layout()\n    if plot_dir is not None:\n        out_fn = os.path.abspath(os.path.join(plot_dir, model_name + '__data_counts.png'))\n        plt.savefig(out_fn)\n        logging.info(f'Figure saved to {out_fn}')\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._plot_3rd_order","title":"<code>_plot_3rd_order(adj, node_properties, model_name, p_conn_dist_bip, count_conn, count_all, dist_bins, model, model_inputs, model_params, plot_dir=None, **_)</code>","text":"<p>Visualize data vs. model (3rd order).</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _plot_3rd_order(adj, node_properties, model_name, p_conn_dist_bip, count_conn, count_all, dist_bins, model, model_inputs, model_params, plot_dir=None, **_):\n    \"\"\"Visualize data vs. model (3rd order).\"\"\"\n    if plot_dir is not None:\n        if not os.path.exists(plot_dir):\n            os.makedirs(plot_dir)\n\n    bin_offset = 0.5 * np.diff(dist_bins[:2])[0]\n    dist_model = np.linspace(dist_bins[0], dist_bins[-1], 100)\n\n    model_strN = f'{model_params[\"bip_neg_exp_model_scale\"]:.3f} * exp(-{model_params[\"bip_neg_exp_model_exponent\"]:.3f} * x)'\n    model_strP = f'{model_params[\"bip_pos_exp_model_scale\"]:.3f} * exp(-{model_params[\"bip_pos_exp_model_exponent\"]:.3f} * x)'\n    model_fct = _get_model_function(model, model_inputs, model_params)\n\n    if isinstance(node_properties, list):\n        N_pre = node_properties[0].shape[0]  # Pre-synaptic population\n        N_post = node_properties[1].shape[0]  # Post-synaptic population\n    else:\n        N_pre = N_post = node_properties.shape[0]\n\n    plt.figure(figsize=(12, 4), dpi=300)\n\n    # Data vs. model\n    plt.subplot(1, 2, 1)\n    bip_dist = np.concatenate((-dist_bins[:-1][::-1] - bin_offset, [0.0], dist_bins[:-1] + bin_offset))\n    bip_data = np.concatenate((p_conn_dist_bip[::-1, 0], [np.nan], p_conn_dist_bip[:, 1]))\n    all_bins = np.concatenate((-dist_bins[1:][::-1], [0.0], dist_bins[1:]))\n    bin_data = np.concatenate((p_conn_dist_bip[::-1, 0], p_conn_dist_bip[:, 1]))\n    plt.step(all_bins, np.hstack([bin_data[0], bin_data]), color=DATA_COLOR, label=f'Data: N = {N_pre}x{N_post} cells')\n    plt.plot(bip_dist, bip_data, '.', color=DATA_COLOR)\n    plt.plot(-dist_model, model_fct(dist_model, np.sign(-dist_model)), '--', color=MODEL_COLOR, label='Model: ' + model_strN)\n    plt.plot(dist_model, model_fct(dist_model, np.sign(dist_model)), '--', color=MODEL_COLOR2, label='Model: ' + model_strP)\n    plt.grid()\n    plt.xlabel('sign($\\\\Delta$z) * Distance [$\\\\mu$m]')\n    plt.ylabel('Conn. prob.')\n    plt.title('Data vs. model fit')\n    plt.legend(loc='upper left', fontsize=8)\n\n    # 2D connection probability (model)\n    plt.subplot(1, 2, 2)\n    plot_range = 500 # (um)\n    r_markers = [200, 400] # (um)\n    dx = np.linspace(-plot_range, plot_range, 201)\n    dz = np.linspace(plot_range, -plot_range, 201)\n    xv, zv = np.meshgrid(dx, dz)\n    vdist = np.sqrt(xv**2 + zv**2)\n    pdist = model_fct(vdist, np.sign(zv))\n    plt.imshow(pdist, interpolation='bilinear', extent=(-plot_range, plot_range, -plot_range, plot_range), cmap=PROB_CMAP, vmin=0.0)\n    plt.plot(plt.xlim(), np.zeros(2), 'w', linewidth=0.5)\n    for r in r_markers:\n        plt.gca().add_patch(plt.Circle((0, 0), r, edgecolor='w', linestyle='--', fill=False))\n        plt.text(0, r, f'{r} $\\\\mu$m', color='w', ha='center', va='bottom')\n    plt.xticks([])\n    plt.yticks([])\n    plt.xlabel('$\\\\Delta$x')\n    plt.ylabel('$\\\\Delta$z')\n    plt.title('2D model')\n    plt.colorbar(label='Conn. prob.')\n\n    plt.suptitle(f'Bipolar distance-dependent connection probability model (3rd order)')\n    plt.tight_layout()\n    if plot_dir is not None:\n        out_fn = os.path.abspath(os.path.join(plot_dir, model_name + '__data_vs_model.png'))\n        plt.savefig(out_fn)\n        logging.info(f'Figure saved to {out_fn}')\n\n    # Data counts\n    bip_count = np.concatenate((count_conn[::-1, 0], [np.nan], count_conn[:, 1]))\n    bip_count_all = np.concatenate((count_all[::-1, 0], [np.nan], count_all[:, 1]))\n    plt.figure(figsize=(12, 4), dpi=300)\n    plt.bar(bip_dist, bip_count_all, width=2.0 * bin_offset, edgecolor='k', label='All pair count')\n    plt.bar(bip_dist, bip_count, width=1.5 * bin_offset, label='Connection count')\n    plt.gca().set_yscale('log')\n    plt.grid()\n    plt.xlabel('sign($\\\\Delta$z) * Distance [$\\\\mu$m]')\n    plt.ylabel('Count')\n    plt.title(f'Bipolar distance-dependent connection counts (N = {N_pre}x{N_post} cells)')\n    plt.legend()\n    plt.tight_layout()\n    if plot_dir is not None:\n        out_fn = os.path.abspath(os.path.join(plot_dir, model_name + '__data_counts.png'))\n        plt.savefig(out_fn)\n        logging.info(f'Figure saved to {out_fn}')\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling._save_data","title":"<code>_save_data(save_dict, save_dir, model_name, save_spec=None)</code>","text":"<p>Writes data/model dict to pickled data file</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def _save_data(save_dict, save_dir, model_name, save_spec=None):\n    \"\"\"Writes data/model dict to pickled data file\"\"\"\n    if not save_dir:\n        return\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    if save_spec is None:\n        save_spec = ''\n    else:\n        save_spec = '__' + save_spec\n\n    save_file = os.path.join(save_dir, f'{model_name}{save_spec}.pickle')\n    with open(save_file, 'wb') as f:\n        pickle.dump(save_dict, f)\n\n    logging.info(f'Pickled dict written to {save_file}')\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_2nd_order_model","title":"<code>conn_prob_2nd_order_model(adj, node_properties, **kwargs)</code>","text":"<p>Wrapper function for 2nd-order probability model building to be used within a processing pipeline, optionally for multiple random subsets of neurons.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse</code> <p>Sparse (symmetric) adjacency matrix of the circuit</p> required <code>node_properties</code> <code>DataFrame</code> <p>Data frame with neuron properties</p> required <code>kwargs</code> <code>dict</code> <p>Additional model building settings; see Notes for details</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with model paramters (columns) for different seeds (rows) (No plotting and data/model/figures saving supported)</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the adjacency matrix is not a square matrix matching the length of the neuron properties table</p> <code>AssertionError</code> <p>If invalid arguments given in kwargs which are internally used by this wrapper (like model_order, ...)</p> <code>AssertionError</code> <p>If model fitting error occurs</p> <code>AssertionError</code> <p>If sample_seeds provided as scalar but is not a positive integer</p> <code>KeyError</code> <p>If name(s) of coordinates not in columns of neuron properties table</p> <code>Warning</code> <p>If sample_seeds provided as list with duplicates</p> <code>Warning</code> <p>If sample_seeds provided but ignored because subsampling not applicable</p> Notes <p>The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.</p> <p>The 2nd-order model as defined in [1]_ describes connection probabilities as a function of distance between pre- and post-synaptic neurons. Specifically, we use here an exponential distance-dependent model of the form: $$ p(d) = \\mbox{scale} * exp(-\\mbox{exponent} * d) $$ with <code>d</code> as distance in \\(\\mu m\\), and the model parameters <code>scale</code> defining the connection probability at distance zero, and <code>exponent</code> the exponent of distance-dependent decay in \\(\\mu m^{-1}\\).</p> <p><code>kwargs</code> may contain following (optional) settings:</p> <ul> <li><code>bin_size_um</code> Bin size in um for depth binning (optional; default: 100)</li> <li><code>max_range_um</code> Max. distance range in um to consider (optional; default: full distance range)</li> <li><code>sample_size</code> Size of random subset of neurons to consider (optional; default: no subsampling)</li> <li><code>sample_seeds</code> Integer number of seeds to randomly generate, or list of specific random seeds, for reproducible selection of random subset of neurons (optional)</li> <li><code>meta_seed</code> Meta seed for generating N random seeds, if integer number N of sample_seeds is provided (optional; default: 0)</li> <li><code>coord_names</code> Names of the coordinates (columns in neuron properties table) based on which to compute Euclidean distance (optional; default: [\"x\", \"y\", \"z\"])</li> <li><code>N_split</code> Number of data splits (&gt; 1) to sequentially extract data from, to reduce memory consumption (optional; default: no splitting)</li> </ul> See Also <p>conn_prob_2nd_order_pathway_model : 2nd-order model building function wrapper for different source/target node populations</p> <p>conn_prob_model : Underlying generic model building function wrapper</p> References <p>.. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_2nd_order_model(adj, node_properties, **kwargs):\n    r\"\"\"Wrapper function for 2nd-order probability model building to be used within a processing pipeline, optionally for multiple random subsets of neurons.\n\n    Parameters\n    ----------\n    adj : scipy.sparse\n        Sparse (symmetric) adjacency matrix of the circuit\n    node_properties : pandas.DataFrame\n        Data frame with neuron properties\n    kwargs : dict, optional\n        Additional model building settings; see Notes for details\n\n    Returns\n    -------\n    pandas.DataFrame\n        Data frame with model paramters (columns) for different seeds (rows)\n        (No plotting and data/model/figures saving supported)\n\n    Raises\n    ------\n    AssertionError\n        If the adjacency matrix is not a square matrix matching the length of the neuron properties table\n    AssertionError\n        If invalid arguments given in kwargs which are internally used by this wrapper (like model_order, ...)\n    AssertionError\n        If model fitting error occurs\n    AssertionError\n        If sample_seeds provided as scalar but is not a positive integer\n    KeyError\n        If name(s) of coordinates not in columns of neuron properties table\n    Warning\n        If sample_seeds provided as list with duplicates\n    Warning\n        If sample_seeds provided but ignored because subsampling not applicable\n\n    Notes\n    -----\n    The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.\n\n    The 2nd-order model as defined in [1]_ describes connection probabilities as a function of distance between pre- and post-synaptic neurons. Specifically, we use here an exponential distance-dependent model of the form:\n    $$\n    p(d) = \\mbox{scale} * exp(-\\mbox{exponent} * d)\n    $$\n    with `d` as distance in $\\mu m$, and the model parameters `scale` defining the connection probability at distance zero, and `exponent` the exponent of distance-dependent decay in $\\mu m^{-1}$.\n\n    `kwargs` may contain following (optional) settings:\n\n    - `bin_size_um` Bin size in um for depth binning (optional; default: 100)\n    - `max_range_um` Max. distance range in um to consider (optional; default: full distance range)\n    - `sample_size` Size of random subset of neurons to consider (optional; default: no subsampling)\n    - `sample_seeds` Integer number of seeds to randomly generate, or list of specific random seeds, for reproducible selection of random subset of neurons (optional)\n    - `meta_seed` Meta seed for generating N random seeds, if integer number N of sample_seeds is provided (optional; default: 0)\n    - `coord_names` Names of the coordinates (columns in neuron properties table) based on which to compute Euclidean distance (optional; default: [\"x\", \"y\", \"z\"])\n    - `N_split` Number of data splits (&gt; 1) to sequentially extract data from, to reduce memory consumption (optional; default: no splitting)\n\n    See Also\n    --------\n    conn_prob_2nd_order_pathway_model : 2nd-order model building function wrapper for different source/target node populations\n\n    conn_prob_model : Underlying generic model building function wrapper\n\n    References\n    ----------\n    .. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.\n\n    \"\"\"\n\n    assert 'model_order' not in kwargs.keys(), f'ERROR: Invalid argument \"model_order\" in kwargs!'\n\n    return conn_prob_model(adj, node_properties, model_order=2, **kwargs)\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_2nd_order_pathway_model","title":"<code>conn_prob_2nd_order_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs)</code>","text":"<p>Wrapper function for 2nd-order probability model building to be used within a processing pipeline for pathways with different source and target node populations, optionally for multiple random subsets of neurons.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse</code> <p>Sparse adjacency matrix of the circuit (may be non-symmetric)</p> required <code>node_properties_src</code> <code>DataFrame</code> <p>Data frame with source neuron properties (corresponding to the rows in adj)</p> required <code>node_properties_tgt</code> <code>DataFrame</code> <p>Data frame with target neuron properties (corresponding to the columns in adj)</p> required <code>kwargs</code> <code>dict</code> <p>Additional model building settings; see \"See Also\" for details</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with model paramters (columns) for different seeds (rows) (No plotting and data/model/figures saving supported)</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the rows/columns of the adjacency matrix are not matching the lengths of the source/target neuron properties tables</p> <code>AssertionError</code> <p>If invalid arguments given in kwargs which are internally used by this wrapper (like model_order, ...)</p> <code>AssertionError</code> <p>If model fitting error occurs</p> <code>AssertionError</code> <p>If data splitting selected, which is not supported for pathway model building</p> <code>AssertionError</code> <p>If sample_seeds provided as scalar but is not a positive integer</p> <code>KeyError</code> <p>If name(s) of coordinates not in columns of neuron properties table</p> <code>Warning</code> <p>If sample_seeds provided as list with duplicates</p> <code>Warning</code> <p>If sample_seeds provided but ignored because subsampling not applicable</p> Notes <p>The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.</p> <p>The 2nd-order model as defined in [1]_. See \"See Also\" for details.</p> See Also <p>conn_prob_2nd_order_model : Special case of 2nd-order model building function wrapper for same source/target node population; further details to be found here</p> References <p>.. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_2nd_order_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs):\n    \"\"\"Wrapper function for 2nd-order probability model building to be used within a processing pipeline for pathways with different source and target node populations, optionally for multiple random subsets of neurons.\n\n    Parameters\n    ----------\n    adj : scipy.sparse\n        Sparse adjacency matrix of the circuit (may be non-symmetric)\n    node_properties_src : pandas.DataFrame\n        Data frame with source neuron properties (corresponding to the rows in adj)\n    node_properties_tgt : pandas.DataFrame\n        Data frame with target neuron properties (corresponding to the columns in adj)\n    kwargs : dict, optional\n        Additional model building settings; see \"See Also\" for details\n\n    Returns\n    -------\n    pandas.DataFrame\n        Data frame with model paramters (columns) for different seeds (rows)\n        (No plotting and data/model/figures saving supported)\n\n    Raises\n    ------\n    AssertionError\n        If the rows/columns of the adjacency matrix are not matching the lengths of the source/target neuron properties tables\n    AssertionError\n        If invalid arguments given in kwargs which are internally used by this wrapper (like model_order, ...)\n    AssertionError\n        If model fitting error occurs\n    AssertionError\n        If data splitting selected, which is not supported for pathway model building\n    AssertionError\n        If sample_seeds provided as scalar but is not a positive integer\n    KeyError\n        If name(s) of coordinates not in columns of neuron properties table\n    Warning\n        If sample_seeds provided as list with duplicates\n    Warning\n        If sample_seeds provided but ignored because subsampling not applicable\n\n    Notes\n    -----\n    The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.\n\n    The 2nd-order model as defined in [1]_. See \"See Also\" for details.\n\n    See Also\n    --------\n    conn_prob_2nd_order_model : Special case of 2nd-order model building function wrapper for same source/target node population; further details to be found here\n\n    References\n    ----------\n    .. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.\n\n    \"\"\"\n\n    assert 'model_order' not in kwargs.keys(), f'ERROR: Invalid argument \"model_order\" in kwargs!'\n\n    return conn_prob_pathway_model(adj, node_properties_src, node_properties_tgt, model_order=2, **kwargs)\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_3rd_order_model","title":"<code>conn_prob_3rd_order_model(adj, node_properties, **kwargs)</code>","text":"<p>Wrapper function for 3rd-order probability model building to be used within a processing pipeline, optionally for multiple random subsets of neurons.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse</code> <p>Sparse (symmetric) adjacency matrix of the circuit</p> required <code>node_properties</code> <code>DataFrame</code> <p>Data frame with neuron properties</p> required <code>kwargs</code> <code>dict</code> <p>Additional model building settings; see Notes for details</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with model paramters (columns) for different seeds (rows) (No plotting and data/model/figures saving supported)</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the adjacency matrix is not a square matrix matching the length of the neuron properties table</p> <code>AssertionError</code> <p>If invalid arguments given in kwargs which are internally used by this wrapper (like model_order, ...)</p> <code>AssertionError</code> <p>If model fitting error occurs</p> <code>AssertionError</code> <p>If sample_seeds provided as scalar but is not a positive integer</p> <code>KeyError</code> <p>If name(s) of coordinates not in columns of neuron properties table</p> <code>Warning</code> <p>If sample_seeds provided as list with duplicates</p> <code>Warning</code> <p>If sample_seeds provided but ignored because subsampling not applicable</p> Notes <p>The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.</p> <p>The 3rd-order model as defined in [1]_ describes connection probabilities as a bipolar function of distance between pre- and post-synaptic neurons. Specifically, we use here an bipolar exponential distance-dependent model of the form: $$ p(d, \\Delta depth) = \\mbox{scale}_N * exp(-\\mbox{exponent}_N * d)~\\mbox{if}~\\Delta depth &lt; 0 $$ $$ p(d, \\Delta depth) = \\mbox{scale}_P * exp(-\\mbox{exponent}_P * d)~\\mbox{if}~\\Delta depth &gt; 0 $$ $$ p(d, \\Delta depth) = \\mbox{Average of both}~\\mbox{if}~\\Delta depth = 0 $$ with <code>d</code> as distance in \\(\\mu m\\), \\(\\Delta depth\\) as difference in depth coordinate (arbitrary unit, as only sign is used; post-synaptic neuron below (\\(\\Delta depth &lt; 0\\)) or above (\\(\\Delta depth &gt; 0\\)) pre-synaptic neuron), and the model parameters <code>scale</code> defining the connection probability at distance zero, and <code>exponent</code> the exponent of distance-dependent decay in \\(\\mu m^{-1}\\) for both cases.</p> <p><code>kwargs</code> may contain following (optional) settings:</p> <ul> <li><code>bin_size_um</code> Bin size in um for depth binning (optional; default: 100)</li> <li><code>max_range_um</code> Max. distance range in um to consider (optional; default: full distance range)</li> <li><code>sample_size</code> Size of random subset of neurons to consider (optional; default: no subsampling)</li> <li><code>sample_seeds</code> Integer number of seeds to randomly generate, or list of specific random seeds, for reproducible selection of random subset of neurons (optional)</li> <li><code>meta_seed</code> Meta seed for generating N random seeds, if integer number N of sample_seeds is provided (optional; default: 0)</li> <li><code>coord_names</code> Names of the coordinates (columns in neuron properties table) based on which to compute Euclidean distance (optional; default: [\"x\", \"y\", \"z\"])</li> <li><code>depth_name</code> Name of depth coordinate (column in neuron properties table) to use in 3rd-order (bipolar) model (optional; default: \"depth\")</li> <li><code>N_split</code> Number of data splits (&gt; 1) to sequentially extract data from, to reduce memory consumption (optional; default: no splitting)</li> </ul> See Also <p>conn_prob_3rd_order_pathway_model : 3rd-order model building function wrapper for different source/target node populations conn_prob_model : Underlying generic model building function wrapper</p> References <p>.. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_3rd_order_model(adj, node_properties, **kwargs):\n    r\"\"\"Wrapper function for 3rd-order probability model building to be used within a processing pipeline, optionally for multiple random subsets of neurons.\n\n    Parameters\n    ----------\n    adj : scipy.sparse\n        Sparse (symmetric) adjacency matrix of the circuit\n    node_properties : pandas.DataFrame\n        Data frame with neuron properties\n    kwargs : dict, optional\n        Additional model building settings; see Notes for details\n\n    Returns\n    -------\n    pandas.DataFrame\n        Data frame with model paramters (columns) for different seeds (rows)\n        (No plotting and data/model/figures saving supported)\n\n    Raises\n    ------\n    AssertionError\n        If the adjacency matrix is not a square matrix matching the length of the neuron properties table\n    AssertionError\n        If invalid arguments given in kwargs which are internally used by this wrapper (like model_order, ...)\n    AssertionError\n        If model fitting error occurs\n    AssertionError\n        If sample_seeds provided as scalar but is not a positive integer\n    KeyError\n        If name(s) of coordinates not in columns of neuron properties table\n    Warning\n        If sample_seeds provided as list with duplicates\n    Warning\n        If sample_seeds provided but ignored because subsampling not applicable\n\n    Notes\n    -----\n    The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.\n\n    The 3rd-order model as defined in [1]_ describes connection probabilities as a bipolar function of distance between pre- and post-synaptic neurons. Specifically, we use here an bipolar exponential distance-dependent model of the form:\n    $$\n    p(d, \\Delta depth) = \\mbox{scale}_N * exp(-\\mbox{exponent}_N * d)~\\mbox{if}~\\Delta depth &lt; 0\n    $$\n    $$\n    p(d, \\Delta depth) = \\mbox{scale}_P * exp(-\\mbox{exponent}_P * d)~\\mbox{if}~\\Delta depth &gt; 0\n    $$\n    $$\n    p(d, \\Delta depth) = \\mbox{Average of both}~\\mbox{if}~\\Delta depth = 0\n    $$\n    with `d` as distance in $\\mu m$, $\\Delta depth$ as difference in depth coordinate (arbitrary unit, as only sign is used; post-synaptic neuron below ($\\Delta depth &lt; 0$) or above ($\\Delta depth &gt; 0$) pre-synaptic neuron), and the model parameters `scale` defining the connection probability at distance zero, and `exponent` the exponent of distance-dependent decay in $\\mu m^{-1}$ for both cases.\n\n    `kwargs` may contain following (optional) settings:\n\n    - `bin_size_um` Bin size in um for depth binning (optional; default: 100)\n    - `max_range_um` Max. distance range in um to consider (optional; default: full distance range)\n    - `sample_size` Size of random subset of neurons to consider (optional; default: no subsampling)\n    - `sample_seeds` Integer number of seeds to randomly generate, or list of specific random seeds, for reproducible selection of random subset of neurons (optional)\n    - `meta_seed` Meta seed for generating N random seeds, if integer number N of sample_seeds is provided (optional; default: 0)\n    - `coord_names` Names of the coordinates (columns in neuron properties table) based on which to compute Euclidean distance (optional; default: [\"x\", \"y\", \"z\"])\n    - `depth_name` Name of depth coordinate (column in neuron properties table) to use in 3rd-order (bipolar) model (optional; default: \"depth\")\n    - `N_split` Number of data splits (&gt; 1) to sequentially extract data from, to reduce memory consumption (optional; default: no splitting)\n\n    See Also\n    --------\n    conn_prob_3rd_order_pathway_model : 3rd-order model building function wrapper for different source/target node populations\n    conn_prob_model : Underlying generic model building function wrapper\n\n    References\n    ----------\n    .. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.\n\n    \"\"\"\n\n    assert 'model_order' not in kwargs.keys(), f'ERROR: Invalid argument \"model_order\" in kwargs!'\n\n    return conn_prob_model(adj, node_properties, model_order=3, **kwargs)\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_3rd_order_pathway_model","title":"<code>conn_prob_3rd_order_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs)</code>","text":"<p>Wrapper function for 3rd-order probability model building to be used within a processing pipeline for pathways with different source and target node populations, optionally for multiple random subsets of neurons.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse</code> <p>Sparse adjacency matrix of the circuit (may be non-symmetric)</p> required <code>node_properties_src</code> <code>DataFrame</code> <p>Data frame with source neuron properties (corresponding to the rows in adj)</p> required <code>node_properties_tgt</code> <code>DataFrame</code> <p>Data frame with target neuron properties (corresponding to the columns in adj)</p> required <code>kwargs</code> <code>dict</code> <p>Additional model building settings; see \"See Also\" for details</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with model paramters (columns) for different seeds (rows) (No plotting and data/model/figures saving supported)</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the rows/columns of the adjacency matrix are not matching the lengths of the source/target neuron properties tables</p> <code>AssertionError</code> <p>If invalid arguments given in kwargs which are internally used by this wrapper (like model_order, ...)</p> <code>AssertionError</code> <p>If model fitting error occurs</p> <code>AssertionError</code> <p>If data splitting selected, which is not supported for pathway model building</p> <code>AssertionError</code> <p>If sample_seeds provided as scalar but is not a positive integer</p> <code>KeyError</code> <p>If name(s) of coordinates not in columns of neuron properties table</p> <code>Warning</code> <p>If sample_seeds provided as list with duplicates</p> <code>Warning</code> <p>If sample_seeds provided but ignored because subsampling not applicable</p> Notes <p>The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.</p> <p>The 3rd-order model as defined in [1]_. See \"See Also\" for details.</p> See Also <p>conn_prob_3rd_order_model : Special case of 3rd-order model building function wrapper for same source/target node population; further details to be found here</p> References <p>.. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_3rd_order_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs):\n    \"\"\"Wrapper function for 3rd-order probability model building to be used within a processing pipeline for pathways with different source and target node populations, optionally for multiple random subsets of neurons.\n\n    Parameters\n    ----------\n    adj : scipy.sparse\n        Sparse adjacency matrix of the circuit (may be non-symmetric)\n    node_properties_src : pandas.DataFrame\n        Data frame with source neuron properties (corresponding to the rows in adj)\n    node_properties_tgt : pandas.DataFrame\n        Data frame with target neuron properties (corresponding to the columns in adj)\n    kwargs : dict, optional\n        Additional model building settings; see \"See Also\" for details\n\n    Returns\n    -------\n    pandas.DataFrame\n        Data frame with model paramters (columns) for different seeds (rows)\n        (No plotting and data/model/figures saving supported)\n\n    Raises\n    ------\n    AssertionError\n        If the rows/columns of the adjacency matrix are not matching the lengths of the source/target neuron properties tables\n    AssertionError\n        If invalid arguments given in kwargs which are internally used by this wrapper (like model_order, ...)\n    AssertionError\n        If model fitting error occurs\n    AssertionError\n        If data splitting selected, which is not supported for pathway model building\n    AssertionError\n        If sample_seeds provided as scalar but is not a positive integer\n    KeyError\n        If name(s) of coordinates not in columns of neuron properties table\n    Warning\n        If sample_seeds provided as list with duplicates\n    Warning\n        If sample_seeds provided but ignored because subsampling not applicable\n\n    Notes\n    -----\n    The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.\n\n    The 3rd-order model as defined in [1]_. See \"See Also\" for details.\n\n    See Also\n    --------\n    conn_prob_3rd_order_model : Special case of 3rd-order model building function wrapper for same source/target node population; further details to be found here\n\n    References\n    ----------\n    .. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.\n\n    \"\"\"\n\n    assert 'model_order' not in kwargs.keys(), f'ERROR: Invalid argument \"model_order\" in kwargs!'\n\n    return conn_prob_pathway_model(adj, node_properties_src, node_properties_tgt, model_order=3, **kwargs)\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_model","title":"<code>conn_prob_model(adj, node_properties, **kwargs)</code>","text":"<p>Wrapper function for generic probability model building to be used within a processing pipeline, optionally for multiple random subsets of neurons.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse</code> <p>Sparse (symmetric) adjacency matrix of the circuit</p> required <code>node_properties</code> <code>DataFrame</code> <p>Data frame with neuron properties</p> required <code>kwargs</code> <code>dict</code> <p>Additional model building settings; see Notes for details</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with model paramters (columns) for different seeds (rows) (No plotting and data/model/figures saving supported)</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the adjacency matrix is not a square matrix matching the length of the neuron properties table</p> <code>AssertionError</code> <p>If invalid arguments given in kwargs which are internally used by this wrapper</p> <code>AssertionError</code> <p>If model fitting error occurs</p> <code>AssertionError</code> <p>If sample_seeds provided as scalar but is not a positive integer</p> <code>AssertionError</code> <p>If model order not supported (supported: 2, 3)</p> <code>KeyError</code> <p>If model order not provided</p> <code>KeyError</code> <p>If name(s) of coordinates not in columns of neuron properties table</p> <code>Warning</code> <p>If sample_seeds provided as list with duplicates</p> <code>Warning</code> <p>If sample_seeds provided but ignored because subsampling not applicable</p> Notes <p>The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.</p> <p>The 2nd-order and 3rd-order models as defined in [1]_ are supported. See \"See Also\" for details.</p> <p><code>kwargs</code> may contain following settings, most of which are optional:</p> <ul> <li><code>model_order</code> Model order (2 or 3)</li> <li><code>bin_size_um</code> Bin size in um for depth binning (optional; default: 100)</li> <li><code>max_range_um</code> Max. distance range in um to consider (optional; default: full distance range)</li> <li><code>sample_size</code> Size of random subset of neurons to consider (optional; default: no subsampling)</li> <li><code>sample_seeds</code> Integer number of seeds to randomly generate, or list of specific random seeds, for reproducible selection of random subset of neurons (optional)</li> <li><code>meta_seed</code> Meta seed for generating N random seeds, if integer number N of sample_seeds is provided (optional; default: 0)</li> <li><code>coord_names</code> Names of the coordinates (columns in neuron properties table) based on which to compute Euclidean distance (optional; default: [\"x\", \"y\", \"z\"])</li> <li><code>depth_name</code> Name of depth coordinate (column in neuron properties table) to use in 3rd-order (bipolar) model (optional; default: \"depth\")</li> <li><code>N_split</code> Number of data splits (&gt; 1) to sequentially extract data from, to reduce memory consumption (optional; default: no splitting)</li> </ul> See Also <p>conn_prob_2nd_order_model : 2nd-order model building function wrapper for same source/target node population conn_prob_3rd_order_model : 3rd-order model building function wrapper for same source/target node population conn_prob_pathway_model : Generic model building function wrapper for differet source/target node populations</p> References <p>.. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_model(adj, node_properties, **kwargs):\n    \"\"\"Wrapper function for generic probability model building to be used within a processing pipeline, optionally for multiple random subsets of neurons.\n\n    Parameters\n    ----------\n    adj : scipy.sparse\n        Sparse (symmetric) adjacency matrix of the circuit\n    node_properties : pandas.DataFrame\n        Data frame with neuron properties\n    kwargs : dict, optional\n        Additional model building settings; see Notes for details\n\n    Returns\n    -------\n    pandas.DataFrame\n        Data frame with model paramters (columns) for different seeds (rows)\n        (No plotting and data/model/figures saving supported)\n\n    Raises\n    ------\n    AssertionError\n        If the adjacency matrix is not a square matrix matching the length of the neuron properties table\n    AssertionError\n        If invalid arguments given in kwargs which are internally used by this wrapper\n    AssertionError\n        If model fitting error occurs\n    AssertionError\n        If sample_seeds provided as scalar but is not a positive integer\n    AssertionError\n        If model order not supported (supported: 2, 3)\n    KeyError\n        If model order not provided\n    KeyError\n        If name(s) of coordinates not in columns of neuron properties table\n    Warning\n        If sample_seeds provided as list with duplicates\n    Warning\n        If sample_seeds provided but ignored because subsampling not applicable\n\n    Notes\n    -----\n    The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.\n\n    The 2nd-order and 3rd-order models as defined in [1]_ are supported. See \"See Also\" for details.\n\n    `kwargs` may contain following settings, most of which are optional:\n\n    - `model_order` Model order (2 or 3)\n    - `bin_size_um` Bin size in um for depth binning (optional; default: 100)\n    - `max_range_um` Max. distance range in um to consider (optional; default: full distance range)\n    - `sample_size` Size of random subset of neurons to consider (optional; default: no subsampling)\n    - `sample_seeds` Integer number of seeds to randomly generate, or list of specific random seeds, for reproducible selection of random subset of neurons (optional)\n    - `meta_seed` Meta seed for generating N random seeds, if integer number N of sample_seeds is provided (optional; default: 0)\n    - `coord_names` Names of the coordinates (columns in neuron properties table) based on which to compute Euclidean distance (optional; default: [\"x\", \"y\", \"z\"])\n    - `depth_name` Name of depth coordinate (column in neuron properties table) to use in 3rd-order (bipolar) model (optional; default: \"depth\")\n    - `N_split` Number of data splits (&gt; 1) to sequentially extract data from, to reduce memory consumption (optional; default: no splitting)\n\n    See Also\n    --------\n    conn_prob_2nd_order_model : 2nd-order model building function wrapper for same source/target node population\n    conn_prob_3rd_order_model : 3rd-order model building function wrapper for same source/target node population\n    conn_prob_pathway_model : Generic model building function wrapper for differet source/target node populations\n\n    References\n    ----------\n    .. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.\n\n    \"\"\"\n\n    assert adj.shape[0] == adj.shape[1] == node_properties.shape[0], 'ERROR: Data size mismatch!'\n\n    invalid_args = ['model_name', 'sample_seed', 'model_dir', 'data_dir', 'plot_dir', 'do_plot', 'part_idx'] # Not allowed arguments, as they will be set/used internally\n    for arg in invalid_args:\n        assert arg not in kwargs.keys(), f'ERROR: Invalid argument \"{arg}\" in kwargs!'\n    kwargs.update({'model_dir': None, 'data_dir': None, 'plot_dir': None, 'do_plot': False, 'part_idx': None}) # Disable plotting/saving\n    model_name = None\n    model_order = kwargs.pop('model_order')\n\n    sample_size = kwargs.get('sample_size')\n    if sample_size is None or sample_size &lt;= 0 or sample_size &gt;= node_properties.shape[0]:\n        sample_seeds = [None] # No randomization\n        if kwargs.pop('sample_seeds', None) is not None:\n            logging.warning('Using all neurons, ignoring sample seeds!')\n    else:\n        sample_seeds = kwargs.pop('sample_seeds', 1)\n\n        if not isinstance(sample_seeds, list): # sample_seeds corresponds to number of seeds to generate\n            sample_seeds = _generate_seeds(sample_seeds, meta_seed=kwargs.pop('meta_seed', 0))\n        else:\n            num_seeds = len(sample_seeds)\n            sample_seeds = list(np.unique(sample_seeds)) # Assure that unique and sorted\n            if len(sample_seeds) &lt; num_seeds:\n                logging.warning(f'Duplicate seeds provided!')\n\n    model_params = pd.DataFrame()\n    for seed in sample_seeds:\n        kwargs.update({'sample_seed': seed})\n        _, model_dict = run_model_building(adj, node_properties, model_name, model_order, **kwargs)\n        model_params = pd.concat([model_params, pd.DataFrame(model_dict['model_params'], index=pd.Index([seed], name='seed'))])\n\n    return model_params\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_pathway_model","title":"<code>conn_prob_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs)</code>","text":"<p>Wrapper function for generic probability model building to be used within a processing pipeline for pathways with different source and target node populations, optionally for multiple random subsets of neurons.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse</code> <p>Sparse adjacency matrix of the circuit (may be non-symmetric)</p> required <code>node_properties_src</code> <code>DataFrame</code> <p>Data frame with source neuron properties (corresponding to the rows in adj)</p> required <code>node_properties_tgt</code> <code>DataFrame</code> <p>Data frame with target neuron properties (corresponding to the columns in adj)</p> required <code>kwargs</code> <code>dict</code> <p>Additional model building settings; see \"See Also\" for details</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Data frame with model paramters (columns) for different seeds (rows) (No plotting and data/model/figures saving supported)</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the rows/columns of the adjacency matrix are not matching the lengths of the source/target neuron properties tables</p> <code>AssertionError</code> <p>If invalid arguments given in kwargs which are internally used by this wrapper</p> <code>AssertionError</code> <p>If model fitting error occurs</p> <code>AssertionError</code> <p>If sample_seeds provided as scalar but is not a positive integer</p> <code>AssertionError</code> <p>If model order not supported (supported: 2, 3)</p> <code>AssertionError</code> <p>If data splitting selected, which is not supported for pathway model building</p> <code>KeyError</code> <p>If model order not provided</p> <code>KeyError</code> <p>If name(s) of coordinates not in columns of neuron properties table</p> <code>Warning</code> <p>If sample_seeds provided as list with duplicates</p> <code>Warning</code> <p>If sample_seeds provided but ignored because subsampling not applicable</p> Notes <p>The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.</p> <p>The 2nd-order and 3rd-order models as defined in [1]_ are supported. See \"See Also\" for details.</p> See Also <p>conn_prob_model : Special case of generic model building function wrapper for same source/target node population; further details to be found here conn_prob_2nd_order_pathway_model : 2nd-order model building function wrapper for different source/target node population conn_prob_3rd_order_pathway_model : 3rd-order model building function wrapper for different source/target node population</p> References <p>.. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs):\n    \"\"\"Wrapper function for generic probability model building to be used within a processing pipeline for pathways with different source and target node populations, optionally for multiple random subsets of neurons.\n\n    Parameters\n    ----------\n    adj : scipy.sparse\n        Sparse adjacency matrix of the circuit (may be non-symmetric)\n    node_properties_src : pandas.DataFrame\n        Data frame with source neuron properties (corresponding to the rows in adj)\n    node_properties_tgt : pandas.DataFrame\n        Data frame with target neuron properties (corresponding to the columns in adj)\n    kwargs : dict, optional\n        Additional model building settings; see \"See Also\" for details\n\n    Returns\n    -------\n    pandas.DataFrame\n        Data frame with model paramters (columns) for different seeds (rows)\n        (No plotting and data/model/figures saving supported)\n\n    Raises\n    ------\n    AssertionError\n        If the rows/columns of the adjacency matrix are not matching the lengths of the source/target neuron properties tables\n    AssertionError\n        If invalid arguments given in kwargs which are internally used by this wrapper\n    AssertionError\n        If model fitting error occurs\n    AssertionError\n        If sample_seeds provided as scalar but is not a positive integer\n    AssertionError\n        If model order not supported (supported: 2, 3)\n    AssertionError\n        If data splitting selected, which is not supported for pathway model building\n    KeyError\n        If model order not provided\n    KeyError\n        If name(s) of coordinates not in columns of neuron properties table\n    Warning\n        If sample_seeds provided as list with duplicates\n    Warning\n        If sample_seeds provided but ignored because subsampling not applicable\n\n    Notes\n    -----\n    The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.\n\n    The 2nd-order and 3rd-order models as defined in [1]_ are supported. See \"See Also\" for details.\n\n    See Also\n    --------\n    conn_prob_model : Special case of generic model building function wrapper for same source/target node population; further details to be found here\n    conn_prob_2nd_order_pathway_model : 2nd-order model building function wrapper for different source/target node population\n    conn_prob_3rd_order_pathway_model : 3rd-order model building function wrapper for different source/target node population\n\n    References\n    ----------\n    .. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.\n\n    \"\"\"\n\n    assert adj.shape[0] == node_properties_src.shape[0] and adj.shape[1] == node_properties_tgt.shape[0], 'ERROR: Data size mismatch!'\n\n    invalid_args = ['model_name', 'sample_seed', 'model_dir', 'data_dir', 'plot_dir', 'do_plot', 'part_idx'] # Not allowed arguments, as they will be set/used internally\n    for arg in invalid_args:\n        assert arg not in kwargs.keys(), f'ERROR: Invalid argument \"{arg}\" in kwargs!'\n    kwargs.update({'model_dir': None, 'data_dir': None, 'plot_dir': None, 'do_plot': False, 'part_idx': None}) # Disable plotting/saving\n    model_name = None\n    model_order = kwargs.pop('model_order')\n\n    sample_size = kwargs.get('sample_size')\n    if sample_size is None  or sample_size &lt;= 0 or sample_size &gt;= np.maximum(node_properties_src.shape[0], node_properties_tgt.shape[0]):\n        sample_seeds = [None] # No randomization\n        if kwargs.pop('sample_seeds', None) is not None:\n            logging.warning('Using all neurons, ignoring sample seeds!')\n    else:\n        sample_seeds = kwargs.pop('sample_seeds', 1)\n\n        if not isinstance(sample_seeds, list): # sample_seeds corresponds to number of seeds to generate\n            sample_seeds = _generate_seeds(sample_seeds, meta_seed=kwargs.pop('meta_seed', 0))\n        else:\n            num_seeds = len(sample_seeds)\n            sample_seeds = list(np.unique(sample_seeds)) # Assure that unique and sorted\n            if len(sample_seeds) &lt; num_seeds:\n                logging.warning(f'Duplicate seeds provided!')\n\n    model_params = pd.DataFrame()\n    for seed in sample_seeds:\n        kwargs.update({'sample_seed': seed})\n        _, model_dict = run_pathway_model_building(adj, node_properties_src, node_properties_tgt, model_name, model_order, **kwargs)\n        model_params = pd.concat([model_params, pd.DataFrame(model_dict['model_params'], index=pd.Index([seed], name='seed'))])\n\n    return model_params\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.run_batch_model_building","title":"<code>run_batch_model_building(adj_file, nrn_file, cfg_file, N_split=None, part_idx=None)</code>","text":"<p>Main function for data extraction and model building to be used in a batch script on different data splits.</p> <p>Parameters:</p> Name Type Description Default <code>adj_file</code> <code>str</code> <p>File name (.npz format) of scipy.sparse adjacency matrix of the circuit</p> required <code>nrn_file</code> <code>str</code> <p>File name (.h5 or .feather format) of pandas.DataFrame with neuron properties</p> required <code>cfg_file</code> <code>str</code> <p>File name (.json format) of config dict specifying the model building operation; see Notes for details</p> required <code>N_split</code> <code>int</code> <p>Number of data splits to divide data extraction into (to reduce memory consumption)</p> <code>None</code> <code>part_idx</code> <code>int</code> <p>Index of current data split (part) to extract data from Range:  0 .. N_split - 1 Run data extraction of given data split        -1                Merge data splits and build model</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Nothing returned here; Data/model/figures are written to output directories as specified in <code>cfg_file</code></p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If nrn_file is not in .h5 or .feather format</p> <code>AssertionError</code> <p>If the adjacency matrix is not a square matrix matching the length of the neuron properties table</p> <code>AssertionError</code> <p>If model order not supported (supported: 2, 3)</p> <code>AssertionError</code> <p>If model fitting error occurs</p> <code>KeyError</code> <p>If name(s) of coordinates not in columns of neuron properties table</p> Notes <p>The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.</p> <p><code>cfg_file</code> must be a .json file containing a dictionary with following entries, most of which are optional:</p> <ul> <li><code>model_name</code> Name of the model (to be used in file names, ...)</li> <li><code>model_order</code> Model order (2 or 3)</li> <li><code>bin_size_um</code> Bin size in um for depth binning (optional; default: 100)</li> <li><code>max_range_um</code> Max. distance range in um to consider (optional; default: full distance range)</li> <li><code>sample_size</code> Size of random subset of neurons to consider (optional; default: no subsampling)</li> <li><code>sample_seed</code> Seed for reproducible selection of random subset of neurons (optional)</li> <li><code>coord_names</code> Names of the coordinates (columns in neuron properties table) based on which to compute Euclidean distance (optional; default: [\"x\", \"y\", \"z\"])</li> <li><code>depth_name</code> Name of depth coordinate (column in neuron properties table) to use in 3rd-order (bipolar) model (optional; default: \"depth\")</li> <li><code>model_dir</code> Output directory where to save the model (optional; default: no saving)</li> <li><code>data_dir</code> Output directory where to save the extracted data (optional; default: no saving)</li> <li><code>do_plot</code> Enable/disable output plotting (optional; default: no plotting)</li> <li><code>plot_dir</code> Output directory where to save the plots, if plotting enabled (optional; default: no saving)</li> <li><code>N_split</code> Number of data splits (&gt; 1) to sequentially extract data from, to reduce memory consumption (optional; default: no splitting)</li> <li><code>part_idx</code> Part index (from 0 to N_split-1) to run data extraction only on a specific data split; -1 to merge existing splits and build model (optional; default: data extraction and model building for all splits)</li> </ul> See Also <p>run_model_building : Underlying main function for model building</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def run_batch_model_building(adj_file, nrn_file, cfg_file, N_split=None, part_idx=None):\n    \"\"\"Main function for data extraction and model building to be used in a batch script on different data splits.\n\n    Parameters\n    ----------\n    adj_file : str\n        File name (.npz format) of scipy.sparse adjacency matrix of the circuit\n    nrn_file : str\n        File name (.h5 or .feather format) of pandas.DataFrame with neuron properties\n    cfg_file : str\n        File name (.json format) of config dict specifying the model building operation; see Notes for details\n    N_split : int, optional\n        Number of data splits to divide data extraction into (to reduce memory consumption)\n    part_idx : int, optional\n        Index of current data split (part) to extract data from\n        Range:  0 .. N_split - 1 Run data extraction of given data split\n               -1                Merge data splits and build model\n\n    Returns\n    -------\n    None\n        Nothing returned here; Data/model/figures are written to output directories as specified in `cfg_file`\n\n    Raises\n    ------\n    AssertionError\n        If nrn_file is not in .h5 or .feather format\n    AssertionError\n        If the adjacency matrix is not a square matrix matching the length of the neuron properties table\n    AssertionError\n        If model order not supported (supported: 2, 3)\n    AssertionError\n        If model fitting error occurs\n    KeyError\n        If name(s) of coordinates not in columns of neuron properties table\n\n    Notes\n    -----\n    The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.\n\n    `cfg_file` must be a .json file containing a dictionary with following entries, most of which are optional:\n\n    - `model_name` Name of the model (to be used in file names, ...)\n    - `model_order` Model order (2 or 3)\n    - `bin_size_um` Bin size in um for depth binning (optional; default: 100)\n    - `max_range_um` Max. distance range in um to consider (optional; default: full distance range)\n    - `sample_size` Size of random subset of neurons to consider (optional; default: no subsampling)\n    - `sample_seed` Seed for reproducible selection of random subset of neurons (optional)\n    - `coord_names` Names of the coordinates (columns in neuron properties table) based on which to compute Euclidean distance (optional; default: [\"x\", \"y\", \"z\"])\n    - `depth_name` Name of depth coordinate (column in neuron properties table) to use in 3rd-order (bipolar) model (optional; default: \"depth\")\n    - `model_dir` Output directory where to save the model (optional; default: no saving)\n    - `data_dir` Output directory where to save the extracted data (optional; default: no saving)\n    - `do_plot` Enable/disable output plotting (optional; default: no plotting)\n    - `plot_dir` Output directory where to save the plots, if plotting enabled (optional; default: no saving)\n    - `N_split` Number of data splits (&gt; 1) to sequentially extract data from, to reduce memory consumption (optional; default: no splitting)\n    - `part_idx` Part index (from 0 to N_split-1) to run data extraction only on a specific data split; -1 to merge existing splits and build model (optional; default: data extraction and model building for all splits)\n\n    See Also\n    --------\n    run_model_building : Underlying main function for model building\n\n    \"\"\"\n\n    # Load adjacency matrix (.npz) &amp; neuron properties table (.h5 or .feather)\n    adj = sps.load_npz(adj_file)\n    if os.path.splitext(nrn_file)[-1] == '.h5':\n        node_properties = pd.read_hdf(nrn_file)\n    elif os.path.splitext(nrn_file)[-1] == '.feather':\n        node_properties = pd.read_feather(nrn_file)\n    else:\n        assert False, f'ERROR: Neuron table format \"{os.path.splitext(nrn_file)[-1]}\" not supported!'\n\n    assert adj.shape[0] == adj.shape[1] == node_properties.shape[0], 'ERROR: Data size mismatch!'\n    logging.info(f'Loaded connectivity and properties of {node_properties.shape[0]} neurons')\n\n    # Load config file (.json)\n    with open(cfg_file, 'r') as f:\n        config_dict = json.load(f)\n\n    # Set/Overwrite data split options\n    if N_split is not None:\n        config_dict.update({'N_split': int(N_split)})\n    if part_idx is not None:\n        config_dict.update({'part_idx': int(part_idx)})\n\n    # Run model building\n    run_model_building(adj, node_properties, **config_dict)\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.run_model_building","title":"<code>run_model_building(adj, node_properties, model_name, model_order, **kwargs)</code>","text":"<p>Main function for probability model building, consisting of three steps: Data extraction, model fitting, and (optionally) data/model visualization.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse</code> <p>Sparse (symmetric) adjacency matrix of the circuit</p> required <code>node_properties</code> <code>DataFrame</code> <p>Data frame with neuron properties</p> required <code>model_name</code> <code>str</code> <p>Name of the model (to be used in file names, ...)</p> required <code>model_order</code> <code>int</code> <p>Model order (2 or 3)</p> required <code>kwargs</code> <code>dict</code> <p>Additional model building settings; see Notes for details</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>Data dictionary containing extracted data points (connection probabilities) from the \"extract\" step; Data/figures also written to output directories as specified in kwargs</p> <code>dict</code> <p>Model dictionary containing probability model fitted to data points from \"model fitting\" step; Model/figures also written to output directories as specified in kwargs</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the adjacency matrix is not a square matrix matching the length of the neuron properties table</p> <code>AssertionError</code> <p>If model order not supported (supported: 2, 3)</p> <code>AssertionError</code> <p>If model fitting error occurs</p> <code>KeyError</code> <p>If name(s) of coordinates not in columns of neuron properties table</p> Notes <p>The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.</p> <p>The 2nd-order and 3rd-order models as defined in [1]_ are supported. See \"See Also\" for details.</p> <p><code>kwargs</code> may contain following (optional) settings:</p> <ul> <li><code>bin_size_um</code> Bin size in um for depth binning (optional; default: 100)</li> <li><code>max_range_um</code> Max. distance range in um to consider (optional; default: full distance range)</li> <li><code>sample_size</code> Size of random subset of neurons to consider (optional; default: no subsampling)</li> <li><code>sample_seed</code> Seed for reproducible selection of random subset of neurons (optional)</li> <li><code>coord_names</code> Names of the coordinates (columns in neuron properties table) based on which to compute Euclidean distance (optional; default: [\"x\", \"y\", \"z\"])</li> <li><code>depth_name</code> Name of depth coordinate (column in neuron properties table) to use in 3rd-order (bipolar) model (optional; default: \"depth\")</li> <li><code>model_dir</code> Output directory where to save the model (optional; default: no saving)</li> <li><code>data_dir</code> Output directory where to save the extracted data (optional; default: no saving)</li> <li><code>do_plot</code> Enable/disable output plotting (optional; default: no plotting)</li> <li><code>plot_dir</code> Output directory where to save the plots, if plotting enabled (optional; default: no saving)</li> <li><code>N_split</code> Number of data splits (&gt; 1) to sequentially extract data from, to reduce memory consumption (optional; default: no splitting)</li> <li><code>part_idx</code> Part index (from 0 to N_split-1) to run data extraction only on a specific data split; -1 to merge existing splits and build model (optional; default: data extraction and model building for all splits)</li> </ul> See Also <p>run_pathway_model_building : Main model building function for differet source/target node populations conn_prob_2nd_order_model : 2nd-order model building function wrapper for same source/target node population to be used within a processing pipeline conn_prob_3rd_order_model : 3rd-order model building function wrapper for same source/target node population to be used within a processing pipeline</p> References <p>.. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def run_model_building(adj, node_properties, model_name, model_order, **kwargs):\n    \"\"\"Main function for probability model building, consisting of three steps: Data extraction, model fitting, and (optionally) data/model visualization.\n\n    Parameters\n    ----------\n    adj : scipy.sparse\n        Sparse (symmetric) adjacency matrix of the circuit\n    node_properties : pandas.DataFrame\n        Data frame with neuron properties\n    model_name : str\n        Name of the model (to be used in file names, ...)\n    model_order : int\n        Model order (2 or 3)\n    kwargs : dict, optional\n        Additional model building settings; see Notes for details\n\n    Returns\n    -------\n    dict\n        Data dictionary containing extracted data points (connection probabilities) from the \"extract\" step; Data/figures also written to output directories as specified in kwargs\n    dict\n        Model dictionary containing probability model fitted to data points from \"model fitting\" step; Model/figures also written to output directories as specified in kwargs\n\n    Raises\n    ------\n    AssertionError\n        If the adjacency matrix is not a square matrix matching the length of the neuron properties table\n    AssertionError\n        If model order not supported (supported: 2, 3)\n    AssertionError\n        If model fitting error occurs\n    KeyError\n        If name(s) of coordinates not in columns of neuron properties table\n\n    Notes\n    -----\n    The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.\n\n    The 2nd-order and 3rd-order models as defined in [1]_ are supported. See \"See Also\" for details.\n\n    `kwargs` may contain following (optional) settings:\n\n    - `bin_size_um` Bin size in um for depth binning (optional; default: 100)\n    - `max_range_um` Max. distance range in um to consider (optional; default: full distance range)\n    - `sample_size` Size of random subset of neurons to consider (optional; default: no subsampling)\n    - `sample_seed` Seed for reproducible selection of random subset of neurons (optional)\n    - `coord_names` Names of the coordinates (columns in neuron properties table) based on which to compute Euclidean distance (optional; default: [\"x\", \"y\", \"z\"])\n    - `depth_name` Name of depth coordinate (column in neuron properties table) to use in 3rd-order (bipolar) model (optional; default: \"depth\")\n    - `model_dir` Output directory where to save the model (optional; default: no saving)\n    - `data_dir` Output directory where to save the extracted data (optional; default: no saving)\n    - `do_plot` Enable/disable output plotting (optional; default: no plotting)\n    - `plot_dir` Output directory where to save the plots, if plotting enabled (optional; default: no saving)\n    - `N_split` Number of data splits (&gt; 1) to sequentially extract data from, to reduce memory consumption (optional; default: no splitting)\n    - `part_idx` Part index (from 0 to N_split-1) to run data extraction only on a specific data split; -1 to merge existing splits and build model (optional; default: data extraction and model building for all splits)\n\n    See Also\n    --------\n    run_pathway_model_building : Main model building function for differet source/target node populations\n    conn_prob_2nd_order_model : 2nd-order model building function wrapper for same source/target node population to be used within a processing pipeline\n    conn_prob_3rd_order_model : 3rd-order model building function wrapper for same source/target node population to be used within a processing pipeline\n\n    References\n    ----------\n    .. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.\n\n    \"\"\"\n\n    logging.info(f'Running order-{model_order} model building {kwargs}...')\n\n    assert adj.shape[0] == adj.shape[1] == node_properties.shape[0], 'ERROR: Data size mismatch!'\n\n    # Subsampling (optional)\n    sample_size = kwargs.get('sample_size')\n    sample_seed = kwargs.get('sample_seed')\n    if sample_size is not None and sample_size &gt; 0 and sample_size &lt; node_properties.shape[0]:\n        logging.info(f'Subsampling to {sample_size} of {node_properties.shape[0]} neurons (seed={sample_seed})')\n        np.random.seed(sample_seed)\n        sub_sel = np.random.permutation([True] * sample_size + [False] * (node_properties.shape[0] - sample_size))\n        adj = adj.tocsr()[sub_sel, :].tocsc()[:, sub_sel].tocsr()\n        node_properties = node_properties.loc[sub_sel, :]\n\n    # Set modelling functions\n    if model_order == 2: # Distance-dependent\n        fct_extract = _extract_2nd_order\n        fct_fit = _build_2nd_order\n        fct_plot = _plot_2nd_order\n    elif model_order == 3: # Bipolar distance-dependent\n        fct_extract = _extract_3rd_order\n        fct_fit = _build_3rd_order\n        fct_plot = _plot_3rd_order\n    else:\n        assert False, f'ERROR: Order-{model_order} model building not supported!'\n\n    # Data splits (optional)\n    N_split = kwargs.pop('N_split', None)\n    part_idx = kwargs.pop('part_idx', None)\n    if N_split is None:\n        split_indices = None\n    else:\n        assert N_split &gt; 1, 'ERROR: Number of data splits must be larger than 1!'\n        split_indices = np.split(np.arange(node_properties.shape[0]), np.cumsum([np.ceil(node_properties.shape[0] / N_split).astype(int)] * (N_split - 1)))\n\n    if part_idx is None or part_idx == -1: # Run data extraction and model building for all splits\n        extract_only = False\n        data_fn = 'data'\n    else: # Run only data extraction of given part idx\n        assert N_split is not None and 0 &lt;= part_idx &lt; N_split, 'ERROR: Part index out of range!'\n        extract_only = True\n        data_fn = 'data' + _get_data_part_name(N_split, part_idx)\n\n    # Extract connection probability data\n    if part_idx == -1: # Special case: Load and merge results of existing parts\n        assert N_split is not None, 'ERROR: Number of data splits required!'\n        data_dict = _merge_data(kwargs.get('data_dir'), model_name, data_fn, [_get_data_part_name(N_split, p) for p in range(N_split)])\n    else:\n        data_dict = fct_extract(adj, node_properties, split_indices=split_indices, part_idx=part_idx, **kwargs)\n    _save_data(data_dict, kwargs.get('data_dir'), model_name, data_fn)\n\n    if extract_only: # Stop here and return data dict\n        return data_dict, {}\n\n    # Fit model\n    model_dict = fct_fit(**data_dict, **kwargs)\n    _save_data(model_dict, kwargs.get('model_dir'), model_name, 'model')\n\n    # Visualize data/model (optional)\n    if kwargs.get('do_plot'):\n        fct_plot(adj, node_properties, model_name, **data_dict, **model_dict, **kwargs)\n\n    return data_dict, model_dict\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.run_pathway_model_building","title":"<code>run_pathway_model_building(adj, node_properties_src, node_properties_tgt, model_name, model_order, **kwargs)</code>","text":"<p>Main function for probability model building for pathways with different source and target node populations, consisting of three steps: Data extraction, model fitting, and (optionally) data/model visualization.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse</code> <p>Sparse adjacency matrix of the circuit (may be non-symmetric)</p> required <code>node_properties_src</code> <code>DataFrame</code> <p>Data frame with source neuron properties (corresponding to the rows in adj)</p> required <code>node_properties_tgt</code> <code>DataFrame</code> <p>Data frame with target neuron properties (corresponding to the columns in adj)</p> required <code>model_name</code> <code>str</code> <p>Name of the model (to be used in file names, ...)</p> required <code>model_order</code> <code>int</code> <p>Model order (2 or 3)</p> required <code>kwargs</code> <code>dict</code> <p>Additional model building settings; see \"See Also\" for details</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>Data dictionary containing extracted data points (connection probabilities) from the \"extract\" step; Data/figures also written to output directories as specified in kwargs</p> <code>dict</code> <p>Model dictionary containing probability model fitted to data points from \"model fitting\" step; Model/figures also written to output directories as specified in kwargs</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the rows/columns of the adjacency matrix are not matching the lengths of the source/target neuron properties tables</p> <code>AssertionError</code> <p>If model order not supported (supported: 2, 3)</p> <code>AssertionError</code> <p>If model fitting error occurs</p> <code>AssertionError</code> <p>If data splitting selected, which is not supported for pathway model building</p> <code>KeyError</code> <p>If name(s) of coordinates not in columns of neuron properties table</p> Notes <p>The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.</p> <p>The 2nd-order and 3rd-order models as defined in [1]_ are supported. See \"See Also\" for details.</p> See Also <p>run_model_building : Main model building function for same source/target node populations; further details to be found here conn_prob_2nd_order_pathway_model : 2nd-order model building function wrapper for different source/target node populations to be used within a processing pipeline conn_prob_3rd_order_pathway_model : 3rd-order model building function wrapper for different source/target node populations to be used within a processing pipeline</p> References <p>.. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def run_pathway_model_building(adj, node_properties_src, node_properties_tgt, model_name, model_order, **kwargs):\n    \"\"\"Main function for probability model building for pathways with different source and target node populations, consisting of three steps: Data extraction, model fitting, and (optionally) data/model visualization.\n\n    Parameters\n    ----------\n    adj : scipy.sparse\n        Sparse adjacency matrix of the circuit (may be non-symmetric)\n    node_properties_src : pandas.DataFrame\n        Data frame with source neuron properties (corresponding to the rows in adj)\n    node_properties_tgt : pandas.DataFrame\n        Data frame with target neuron properties (corresponding to the columns in adj)\n    model_name : str\n        Name of the model (to be used in file names, ...)\n    model_order : int\n        Model order (2 or 3)\n    kwargs : dict, optional\n        Additional model building settings; see \"See Also\" for details\n\n    Returns\n    -------\n    dict\n        Data dictionary containing extracted data points (connection probabilities) from the \"extract\" step; Data/figures also written to output directories as specified in kwargs\n    dict\n        Model dictionary containing probability model fitted to data points from \"model fitting\" step; Model/figures also written to output directories as specified in kwargs\n\n    Raises\n    ------\n    AssertionError\n        If the rows/columns of the adjacency matrix are not matching the lengths of the source/target neuron properties tables\n    AssertionError\n        If model order not supported (supported: 2, 3)\n    AssertionError\n        If model fitting error occurs\n    AssertionError\n        If data splitting selected, which is not supported for pathway model building\n    KeyError\n        If name(s) of coordinates not in columns of neuron properties table\n\n    Notes\n    -----\n    The adjacency matrix encodes connectivity between source (rows) and taget (columns) neurons.\n\n    The 2nd-order and 3rd-order models as defined in [1]_ are supported. See \"See Also\" for details.\n\n    See Also\n    --------\n    run_model_building : Main model building function for same source/target node populations; further details to be found here\n    conn_prob_2nd_order_pathway_model : 2nd-order model building function wrapper for different source/target node populations to be used within a processing pipeline\n    conn_prob_3rd_order_pathway_model : 3rd-order model building function wrapper for different source/target node populations to be used within a processing pipeline\n\n    References\n    ----------\n    .. [1] Gal E, Perin R, Markram H, London M, Segev I, \"Neuron Geometry Underlies Universal Network Features in Cortical Microcircuits,\" bioRxiv, doi: https://doi.org/10.1101/656058.\n\n    \"\"\"\n\n    logging.info(f'Running order-{model_order} model building {kwargs}...')\n\n    assert adj.shape[0] == node_properties_src.shape[0] and adj.shape[1] == node_properties_tgt.shape[0], 'ERROR: Data size mismatch!'\n\n    # Subsampling (optional)\n    sample_size = kwargs.get('sample_size')\n    sample_seed = kwargs.get('sample_seed')\n    if sample_size is not None and sample_size &gt; 0 and sample_size &lt; np.maximum(node_properties_src.shape[0], node_properties_tgt.shape[0]):\n        logging.info(f'Subsampling to {sample_size} of {node_properties_src.shape[0]}x{node_properties_tgt.shape[0]} neurons (seed={sample_seed})')\n        np.random.seed(sample_seed)\n        if sample_size &lt; node_properties_src.shape[0]:\n            sub_sel_src = np.random.permutation([True] * sample_size + [False] * (node_properties_src.shape[0] - sample_size))\n        else:\n            sub_sel_src = np.full(node_properties_src.shape[0], True)\n\n        if sample_size &lt; node_properties_tgt.shape[0]:\n            sub_sel_tgt = np.random.permutation([True] * sample_size + [False] * (node_properties_tgt.shape[0] - sample_size))\n        else:\n            sub_sel_tgt = np.full(node_properties_tgt.shape[0], True)\n\n        adj = adj.tocsr()[sub_sel_src, :].tocsc()[:, sub_sel_tgt].tocsr()\n        # adj = adj[sub_sel_src, :][:, sub_sel_tgt]\n        node_properties_src = node_properties_src.loc[sub_sel_src, :]\n        node_properties_tgt = node_properties_tgt.loc[sub_sel_tgt, :]\n\n    # Set modelling functions\n    if model_order == 2: # Distance-dependent\n        fct_extract = _extract_2nd_order_pathway\n        fct_fit = _build_2nd_order\n        fct_plot = _plot_2nd_order\n    elif model_order == 3: # Bipolar distance-dependent\n        fct_extract = _extract_3rd_order_pathway\n        fct_fit = _build_3rd_order\n        fct_plot = _plot_3rd_order\n    else:\n        assert False, f'ERROR: Order-{model_order} model building not supported!'\n\n    # Data splits (optional)\n    N_split = kwargs.pop('N_split', None)\n    part_idx = kwargs.pop('part_idx', None)\n    assert N_split is None and part_idx is None, 'ERROR: Data splitting not supported!'\n    data_fn = 'data'\n\n    # Extract connection probability data\n    data_dict = fct_extract(adj, node_properties_src, node_properties_tgt, split_indices=None, part_idx=None, **kwargs)\n    _save_data(data_dict, kwargs.get('data_dir'), model_name, data_fn)\n\n    # Fit model\n    model_dict = fct_fit(**data_dict, **kwargs)\n    _save_data(model_dict, kwargs.get('model_dir'), model_name, 'model')\n\n    # Visualize data/model (optional)\n    if kwargs.get('do_plot'):\n        fct_plot(adj, [node_properties_src, node_properties_tgt], model_name, **data_dict, **model_dict, **kwargs)\n\n    return data_dict, model_dict\n</code></pre>"},{"location":"network/","title":"Network","text":""},{"location":"network/#this-page-describes-functions-contained-in-the-network-module-these-are-arranged-by-motivation-either","title":"This page describes functions contained in the <code>network</code> module.  These are arranged by motivation, either:","text":"<ul> <li>Topology: Network analyses that are topologically motivated.</li> <li>Classic: Network analyses based on classic graph theoretic metrics.</li> <li>Local: Utility functions to restrict analyses to neighborhoods.</li> <li>Stats: Utility functions to compute simple statistics across simplices and neighborhoods.</li> </ul>"},{"location":"network_classic/","title":"Classic","text":""},{"location":"network_classic/#these-functions-are-classic-graph-theoretic-metrics","title":"These functions are classic graph theoretic metrics.","text":""},{"location":"network_classic/#src.connalysis.network.classic.ccc","title":"<code>ccc(matrix)</code>","text":"<p>Computes the classical clustering coefficient of a directed graph</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>numpy array</code> <p>the adjaceny matrix of an unweighted graph</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The index is the node index in <code>matrix</code> and the value is the clustering coefficient of the node. </p> References <p>The formula is taken from the following paper.</p> <p>[1] G. Fagiolo, \"Clustering in complex directed networks\", 2006;         DOI: 10.1103/PhysRevE.76.026107.</p> <p>[2] Concei\u00e7\u00e3o, Pedro, et al. \"An application of neighbourhoods in digraphs to the classification of binary dynamics.\", 2022         DOI: 10.1162/netn_a_00228.</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def ccc(matrix):\n    \"\"\"Computes the classical clustering coefficient of a directed graph\n\n    Parameters\n    ----------\n    matrix : numpy array\n        the adjaceny matrix of an unweighted graph\n\n    Returns\n    -------\n    Series\n        The index is the node index in ``matrix`` and the value is the clustering coefficient of the node. \n\n    References\n    ----------\n    The formula is taken from the following paper.\n\n    [1] G. Fagiolo, \"Clustering in complex directed networks\", 2006;\n            [DOI: 10.1103/PhysRevE.76.026107]().\n\n    [2] Concei\u00e7\u00e3o, Pedro, et al. \"An application of neighbourhoods in digraphs to the classification of binary dynamics.\", 2022\n            [DOI: 10.1162/netn_a_00228](). \n\n    \"\"\"\n    # We only analyze the udnerlying connectivity not the weights\n    matrix=matrix.astype(bool).astype(int)\n    # Numerator \n    if sp.issparse(matrix):\n        dense_matrix = matrix + matrix.transpose()\n        numerator = 0.5*np.diag((dense_matrix ** 3).toarray())\n    else:\n        numerator = 0.5*np.diag(np.linalg.matrix_power(matrix + np.transpose(matrix), 3))\n    # Denominator\n    deg = node_degree(matrix) \n    denominator = (deg*(deg-1)-2*rc_submatrix(matrix).toarray().sum(axis=0))\n    return numerator/denominator\n</code></pre>"},{"location":"network_classic/#src.connalysis.network.classic.closeness_connected_components","title":"<code>closeness_connected_components(adj, directed=False, return_sum=True)</code>","text":"<p>Compute the closeness of each connected component of more than 1 vertex</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>array_like</code> <p>Adjacency matrix of the graph</p> required <code>directed</code> <code>bool</code> <p>If <code>True</code>, will be computed using strongly connected components and directed closeness.</p> <code>False</code> <code>return_sum</code> <code>bool</code> <p>If <code>True</code>, only one list will be returned, by summing over all the connected components.</p> <code>True</code> <p>Returns:</p> Type Description <code>array_like</code> <p>A single array( if <code>return_sum=True</code>) or a list of arrays of shape <code>n</code>, containting closeness of vertices in that component, or 0 if the vertex is not in the component. Closeness cannot be zero otherwise.</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def closeness_connected_components(adj, directed=False, return_sum=True):\n    \"\"\"Compute the closeness of each connected component of more than 1 vertex\n\n    Parameters\n    ----------\n    adj : array_like\n        Adjacency matrix of the graph\n    directed : bool\n        If `True`, will be computed using strongly connected components and directed closeness.\n    return_sum : bool\n        If `True`, only one list will be returned, by summing over all the connected components.\n\n\n    Returns\n    -------\n    array_like\n        A single array( if `return_sum=True`) or a list of arrays of shape `n`, containting closeness of vertices in that component, or 0 if the vertex is not in the component. Closeness cannot be zero otherwise.\n\n    \"\"\"\n    from sknetwork.ranking import Closeness\n    from scipy.sparse.csgraph import connected_components\n\n    matrix = sp.csr_matrix(adj)\n    if directed:\n        n_comp, comp = connected_components(matrix, directed=True, connection=\"strong\")\n    else:\n        n_comp, comp = connected_components(matrix, directed=False)\n        matrix = matrix + matrix.T  # we need to make the matrix symmetric\n\n    closeness = Closeness()  # if matrix is not symmetric automatically use directed\n    n = matrix.shape[0]\n    all_c = []\n    for i in range(n_comp):\n        c = np.zeros(n)\n        idx = np.where(comp == i)[0]\n        sub_mat = matrix[np.ix_(idx, idx)]\n        if sub_mat.getnnz() &gt; 0:\n            c[idx] = closeness.fit_predict(sub_mat)\n            all_c.append(c)\n    if return_sum:\n        all_c = np.array(all_c)\n        return np.sum(all_c, axis=0)\n    else:\n        return all_c\n</code></pre>"},{"location":"network_classic/#src.connalysis.network.classic.connected_components","title":"<code>connected_components(adj, directed=True, connection='weak', return_labels=False)</code>","text":"<p>Returns a list of the size of the connected components of the graph</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>array_like or sparse matrix</code> <p>Adjacency matrix of the graph</p> required <code>directed</code> <code>bool</code> <p>If <code>True</code>, will be compute connected components of the directed graph</p> <code>True</code> <code>connection</code> <code>str {'weak', 'strong'}</code> <p>If <code>weak</code>, it will compute the connected components of the underlying undirected graph.  If <code>strong</code>, it will compute strongly connected components of the directed graph.</p> <code>'weak'</code> <code>return_labels</code> <code>bool</code> <p>If <code>True</code>, will return the labels of the connected components</p> <code>False</code> <p>Returns:</p> Type Description <code>array_like</code> <p>A list of the size of the connected components of the graph.  If return_labels == True, it also returns the list of labels of the connected components.</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def connected_components(adj,directed=True, connection='weak', return_labels=False):\n    \"\"\"Returns a list of the size of the connected components of the graph\n\n    Parameters\n    ----------\n    adj : array_like or sparse matrix\n        Adjacency matrix of the graph\n    directed : bool\n        If `True`, will be compute connected components of the directed graph\n    connection : str {'weak', 'strong'}\n        If `weak`, it will compute the connected components of the underlying undirected graph. \n        If `strong`, it will compute strongly connected components of the directed graph.\n    return_labels : bool\n        If `True`, will return the labels of the connected components\n\n    Returns\n    -------\n    array_like\n        A list of the size of the connected components of the graph. \n        If return_labels == True, it also returns the list of labels of the connected components.\n    \"\"\"\n\n    matrix = sp.csr_matrix(adj)\n\n    comps=sp.csgraph.connected_components(matrix , directed=directed, \n                                          connection=connection, return_labels=True)\n    comps_size=np.unique(comps[1], return_counts=True)[1]\n    if return_labels:\n        return comps_size, comps[1]\n    else:\n        return comps_size\n</code></pre>"},{"location":"network_classic/#src.connalysis.network.classic.connection_probability_within","title":"<code>connection_probability_within(m, v, cols=['x', 'y'], max_dist=100, type='directed', skip_symmetry_check=False)</code>","text":"<p>Returns the average density of submatrices of nodes within distance <code>max_dist</code> of each node in <code>m</code>.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>array or sparse matrix</code> <p>Adjacency matrix of the graph</p> required <code>v</code> <code>DataFrame or tuple</code> <p>DataFrame with the coordinates of the nodes if m is square and has the same pre and post nodes.  If a tuple is passed, it should contain two DataFrames, one for the source nodes and one for the target nodes.</p> required <code>cols</code> <code>list</code> <p>Columns of the DataFrame containing the coordinates of the nodes.</p> <code>['x', 'y']</code> <code>max_dist</code> <code>float</code> <p>Maximum distance between nodes to be considered connected.</p> <code>100</code> <code>type</code> <code>str {'directed', 'undirected', 'reciprocal'}</code> <p>The type of the graph considered for the computation. If 'directed', the density as a directed graph is computed. If 'undirected', the density of the underlying undirected graph is computed. Only possible if the matrix is square. If 'reciprocal', the density of the underlying reciprocal graph is computed. Only possible if the matrix is square.</p> <code>'directed'</code> <code>skip_symmetry_check</code> <code>bool</code> <p>If <code>True</code>, it will skip the check for symmetry of the matrix.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The average density of the submatrices.</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def connection_probability_within(m, v, cols=[\"x\", \"y\"], max_dist=100, type='directed', skip_symmetry_check= False):\n    \"\"\"Returns the average density of submatrices of nodes within distance `max_dist` of each node in `m`.\n\n        Parameters\n        ----------\n        m : array or sparse matrix\n            Adjacency matrix of the graph\n        v : DataFrame or tuple\n            DataFrame with the coordinates of the nodes if m is square and has the same pre and post nodes. \n            If a tuple is passed, it should contain two DataFrames, one for the source nodes and one for the target nodes.\n        cols : list\n            Columns of the DataFrame containing the coordinates of the nodes.\n        max_dist : float\n            Maximum distance between nodes to be considered connected.        \n        type : str {'directed', 'undirected', 'reciprocal'}\n            The type of the graph considered for the computation.\n            If 'directed', the density as a directed graph is computed.\n            If 'undirected', the density of the underlying undirected graph is computed. Only possible if the matrix is square.\n            If 'reciprocal', the density of the underlying reciprocal graph is computed. Only possible if the matrix is square.\n        skip_symmetry_check : bool\n            If `True`, it will skip the check for symmetry of the matrix.\n\n        Returns\n        -------\n        float\n            The average density of the submatrices.\n    \"\"\"\n    # Get coordinates of nodes and their nearest neighbors\n    pairs_mat=get_pairs_within(m, v, cols=cols, max_dist=max_dist)\n    if pairs_mat is np.nan:\n        return np.nan\n    else:\n        if type=='reciprocal':\n            assert m.shape[0]==m.shape[1], \"The matrix must be square to compute the reciprocal connectivity\"\n            m=rc_submatrix(m).tocsc()\n        elif type=='undirected':\n            assert m.shape[0]==m.shape[1], \"The matrix must be square to compute the undirected connectivity\"\n            if not skip_symmetry_check:\n                if (m!=m.T).nnz &gt;0:\n                    print(\"The graph is directed. Taking the underlying undirected graph\")\n                    m=underlying_undirected_matrix(m)\n            m=sp.csc_matrix(m)\n        elif type=='directed':\n            m=sp.csc_matrix(m)\n        else:\n            print(type)\n            raise ValueError(\"Type must be 'directed', 'undirected' or 'reciprocal'\")\n        return m[pairs_mat].astype(bool).mean() \n</code></pre>"},{"location":"network_classic/#src.connalysis.network.classic.core_number","title":"<code>core_number(adj)</code>","text":"<p>Returns the core number for each node.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>array_like or sparse matrix</code> <p>Adjacency matrix of the graph</p> required <code>directed</code> <code>bool</code> <p>If <code>True</code>, will be compute connected components of the directed graph</p> required <code>connection</code> <code>str {'weak', 'strong'}</code> <p>If <code>weak</code>, it will compute the connected components of the underlying undirected graph.  If <code>strong</code>, it will compute strongly connected components of the directed graph.</p> required <code>return_labels</code> <code>bool</code> <p>If <code>True</code>, will return the labels of the connected components</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with keys the indices of the nodes of adj and values their corresponding core number.</p> Notes <p>The k-core of a graph is the maximal subgraph that contains nodes of degree k or more in the induced subgraph. The core number of a node is the largest value k of a k-core containing that node. For directed graphs the total node degree is use, i.e., the sum of in-degree + out-degree.</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def core_number(adj):\n    \"\"\"Returns the core number for each node.\n\n        Parameters\n        ----------\n        adj : array_like or sparse matrix\n            Adjacency matrix of the graph\n        directed : bool\n            If `True`, will be compute connected components of the directed graph\n        connection : str {'weak', 'strong'}\n            If `weak`, it will compute the connected components of the underlying undirected graph. \n            If `strong`, it will compute strongly connected components of the directed graph.\n        return_labels : bool\n            If `True`, will return the labels of the connected components\n\n        Returns\n        -------\n        dict\n            A dictionary with keys the indices of the nodes of adj and values their corresponding core number.\n\n        Notes\n        -----\n        The k-core of a graph is the maximal subgraph that contains nodes of degree k or more in the induced subgraph.\n        The core number of a node is the largest value k of a k-core containing that node.\n        For directed graphs the total node degree is use, i.e., the sum of in-degree + out-degree.\n    \"\"\"\n    # TODO: Implement directed (k,l) core and k-core of underlying undirected graph (very similar to this)\n    # TODO: Filtered simplex counts with different weights on vertices (coreness, intersection)\n    #  or on edges (strength of connection).    \n    adj=sp.csr_matrix(adj)\n    G = nx.from_scipy_sparse_array(adj)\n    # Very inefficient (returns a dictionary!). TODO: Look for different implementation\n    return nx.algorithms.core.core_number(G)\n</code></pre>"},{"location":"network_classic/#src.connalysis.network.classic.density","title":"<code>density(adj, type='directed', skip_symmetry_check=False)</code>","text":"<p>Returns the density of a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>array_like or sparse matrix</code> <p>Adjacency matrix of the graph</p> required <code>type</code> <code>str {'directed', 'undirected', 'reciprocal'}</code> <p>The type of the graph considered for the computation. If 'directed', the density as a directed graph is computed. If 'undirected', the density of the underlying undirected graph is computed. If 'reciprocal', the density of the underlying reciprocal graph is computed.</p> <code>'directed'</code> <code>skip_symmetry_check</code> <code>bool</code> <p>If <code>True</code>, it will skip the check for symmetry of the matrix.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The density of the graph.</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def density(adj, type=\"directed\", skip_symmetry_check=False):\n    \"\"\"Returns the density of a matrix.\n\n        Parameters\n        ----------\n        adj : array_like or sparse matrix\n            Adjacency matrix of the graph\n        type : str {'directed', 'undirected', 'reciprocal'}\n            The type of the graph considered for the computation.\n            If 'directed', the density as a directed graph is computed.\n            If 'undirected', the density of the underlying undirected graph is computed.\n            If 'reciprocal', the density of the underlying reciprocal graph is computed.\n        skip_symmetry_check : bool\n            If `True`, it will skip the check for symmetry of the matrix.\n\n        Returns\n        -------\n        float\n            The density of the graph.\n    \"\"\"\n    assert adj.shape[0] == adj.shape[1], \"The matrix must be square!\"\n    n=adj.shape[0]\n    adj=sp.csr_matrix(adj).astype('bool')\n    if type==\"undirected\":\n        if not skip_symmetry_check:\n            if (adj!=adj.T).nnz &gt;0:\n                print(\"The graph is directed. Taking the underlying undirected graph\")\n                adj=underlying_undirected_matrix(adj)\n    if type==\"reciprocal\":\n        adj=rc_submatrix(adj)\n    return adj.sum() /  (n*(n-1))\n</code></pre>"},{"location":"network_classic/#src.connalysis.network.classic.efficient_rich_club_curve","title":"<code>efficient_rich_club_curve(M, direction='TOTAL', pre_calculated_filtration=None, sparse_bin_set=False)</code>","text":"<p>Fast computation of the rich-club curve of the matrix M with respect to a degree filtration and possibly any filtration chosen by the user.</p> <p>Parameters:</p> Name Type Description Default <code>M</code> <code>array_like or sparse matrix</code> <p>Adjacency matrix of the graph.</p> required <code>direction</code> <code>str {'OUT', 'IN', 'TOTAL'}</code> <p>'OUT' : Compute the rich-club curve for the out-degree.</p> <p>'IN' : Compute the rich-club curve for the in-degree.</p> <p>'TOTAL' or None : Compute the rich-club curve for the total degree i.e., in-degree + out-degree.</p> <code>'TOTAL'</code> <code>pre_calculated_filtration</code> <code>pandas series </code> <p>To provide user defined filtration values other than degree. The index are the nodes of M and values the filtration values.</p> <code>None</code> <code>sparse_bin_set</code> <code>bool</code> <p>If False, all integer values between 0 and the maximum degree/filtration value will be used to generate the bins.  If True, unique values of the degrees/filtration will be used.  This is useful when the degrees/filtration values are sparse over the whole range.</p> <code>False</code> <p>Returns:</p> Type Description <code>Pandas series</code> <p>With index the center of the binned degrees/filtration and values the rich club coefficient at that degree.</p> Notes <p>The rich-club coefficient is a measure of the tendency of high-degree nodes (nodes with a high filtration value) to form tightly interconnected communities.</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def efficient_rich_club_curve(M, direction=\"TOTAL\", pre_calculated_filtration=None, sparse_bin_set=False):\n    \"\"\"Fast computation of the rich-club curve of the matrix M with respect to a degree filtration and possibly any filtration chosen by the user.\n\n    Parameters\n    ----------\n    M : array_like or sparse matrix\n        Adjacency matrix of the graph.\n    direction : str {'OUT', 'IN', 'TOTAL'}\n        'OUT' : Compute the rich-club curve for the out-degree.\n\n        'IN' : Compute the rich-club curve for the in-degree.\n\n        'TOTAL' or None : Compute the rich-club curve for the total degree i.e., in-degree + out-degree.\n    pre_calculated_filtration : pandas series \n        To provide user defined filtration values other than degree.  \n        The index are the nodes of M and values the filtration values.\n    sparse_bin_set : bool\n        If False, all integer values between 0 and the maximum degree/filtration value will be used to generate the bins. \n        If True, unique values of the degrees/filtration will be used. \n        This is useful when the degrees/filtration values are sparse over the whole range.\n    Returns\n    -------\n    Pandas series\n        With index the center of the binned degrees/filtration and values the rich club coefficient at that degree.\n    Notes\n    -----\n    The rich-club coefficient is a measure of the tendency of high-degree nodes (nodes with a high filtration value) to form tightly interconnected communities.\n    \"\"\"\n    #TODO: Maybe expand the notes explaining this concept.\n    if pre_calculated_filtration is not None:\n        deg = pre_calculated_filtration\n    else:\n        if direction==\"TOTAL\": direction = None \n        deg=node_degree(M,direction=direction)\n    M = M.tocoo()\n    shape = M.shape\n    M = pd.DataFrame.from_dict({\"row\": M.row, \"col\": M.col})\n\n\n\n    if sparse_bin_set == False:\n        degree_bins = np.arange(deg.max() + 2)\n    elif sparse_bin_set == True:\n        degree_bins = np.unique(np.append(deg, [0, deg.max() + 1]))\n    degree_bins_rv = degree_bins[-2::-1]\n    nrn_degree_distribution = np.histogram(deg.values, bins=degree_bins)[0]\n    nrn_cum_degrees = np.cumsum(nrn_degree_distribution[-1::-1])\n    nrn_cum_pairs = nrn_cum_degrees * (nrn_cum_degrees - 1)\n\n    deg_arr = np.zeros(shape[0], dtype=int)\n    deg_arr[deg.index.values] = deg.values\n\n    deg = None\n\n    con_degree = np.minimum(deg_arr[M[\"row\"].values], deg_arr[M[\"col\"].values])\n    M = None\n    con_degree = np.histogram(con_degree, bins=degree_bins)[0]\n\n    cum_degrees = np.cumsum(con_degree[-1::-1])\n\n    return pd.Series(cum_degrees / nrn_cum_pairs, degree_bins_rv)[::-1]\n</code></pre>"},{"location":"network_classic/#src.connalysis.network.classic.get_pairs_within","title":"<code>get_pairs_within(m, v, cols=['x', 'y'], max_dist=100)</code>","text":"<p>Returns a matrix of the paris of nodes within distance <code>max_dist</code> of each other.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>array or sparse matrix</code> <p>Adjacency matrix of the graph</p> required <code>v</code> <code>DataFrame or tuple</code> <p>DataFrame with the coordinates of the nodes if m is square and has the same source and target nodes.  If a tuple is passed, it should contain two DataFrames, one for the source nodes and one for the target nodes.</p> required <code>cols</code> <code>list</code> <p>Columns of the DataFrame containing the coordinates of the nodes.</p> <code>['x', 'y']</code> <code>max_dist</code> <code>float</code> <p>Maximum distance between nodes to be considered connected.</p> <code>100</code> <p>Returns:</p> Type Description <code>sparse matrix</code> <p>Boolean matrix with 1 indicating the pairs of nodes within <code>max_dist</code> of each other,  excluding the diagonal for square matrices.</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def get_pairs_within(m, v, cols=[\"x\", \"y\"], max_dist=100):\n    \"\"\"Returns a matrix of the paris of nodes within distance `max_dist` of each other.\n\n        Parameters\n        ----------\n        m : array or sparse matrix\n            Adjacency matrix of the graph\n        v : DataFrame or tuple\n            DataFrame with the coordinates of the nodes if m is square and has the same source and target nodes. \n            If a tuple is passed, it should contain two DataFrames, one for the source nodes and one for the target nodes.        \n        cols : list\n            Columns of the DataFrame containing the coordinates of the nodes.\n        max_dist : float\n            Maximum distance between nodes to be considered connected.        \n\n        Returns\n        -------\n        sparse matrix\n            Boolean matrix with 1 indicating the pairs of nodes within `max_dist` of each other, \n            excluding the diagonal for square matrices.\n    \"\"\"\n    if isinstance(v, tuple):\n        vpre, vpost = v\n    else:\n        vpre = v; vpost = v\n    tree = KDTree(vpre[cols])\n    pairs = tree.query_ball_point(vpost[cols], max_dist)\n    indptr = np.cumsum([0] + [len(_x) for _x in pairs])\n    pairs_mat = sp.csc_matrix((np.ones(indptr[-1], dtype=bool),\n                                   np.hstack(pairs), indptr),\n                                   shape=m.shape)\n    if not isinstance(v,tuple):\n        pairs_mat.setdiag(0) # Don't consider edges from i to i when pre and post subpopulations are the same\n    if indptr[-1] == 0:\n        return np.nan\n    return pairs_mat.astype(bool)\n</code></pre>"},{"location":"network_classic/#src.connalysis.network.classic.largest_strongly_connected_component","title":"<code>largest_strongly_connected_component(adjacency_matrix)</code>","text":"<p>Computes the largest strongly connected component of the graph with adjacency matrix adjacency_matrix,     and returns the adjacency matrix of said component</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrix</code> <code>numpy array</code> <p>the adjaceny matrix of the DiGraph as a numpy array</p> required <p>Returns:</p> Type Description <code>numpy array</code> <p>The adjacency matrix of the largest strongly connected component</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def largest_strongly_connected_component(adjacency_matrix):\n    \"\"\"Computes the largest strongly connected component of the graph with adjacency matrix adjacency_matrix,\n        and returns the adjacency matrix of said component\n\n    Parameters\n    ----------\n    adjacency_matrix : numpy array\n        the adjaceny matrix of the DiGraph as a numpy array\n\n\n    Returns\n    -------\n    numpy array\n        The adjacency matrix of the largest strongly connected component\n\n    \"\"\"\n    if sp.issparse(adjacency_matrix):\n        adjacency_matrix = adjacency_matrix.toarray()\n    current_tribe_nx = np_to_nx(adjacency_matrix)\n    largest_comp = max(nx.strongly_connected_components(current_tribe_nx), key=len)\n    current_tribe_strong_nx = current_tribe_nx.subgraph(largest_comp)\n    current_tribe_strong = nx_to_np(current_tribe_strong_nx)\n    return current_tribe_strong\n</code></pre>"},{"location":"network_classic/#src.connalysis.network.classic.np_to_nx","title":"<code>np_to_nx(adjacency_matrix)</code>","text":"<p>Converts numpy array of an adjacency matrix to a networkx digraph</p> <p>Parameters:</p> Name Type Description Default <code>adjacency_matrix</code> <code>numpy array</code> <p>the adjaceny matrix of the DiGraph as a numpy array</p> required <p>Returns:</p> Type Description <code>networkx DiGraph</code> <p>a directed graph</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def np_to_nx(adjacency_matrix):\n    \"\"\"Converts numpy array of an adjacency matrix to a networkx digraph\n\n    Parameters\n    ----------\n    adjacency_matrix : numpy array\n        the adjaceny matrix of the DiGraph as a numpy array\n\n\n    Returns\n    -------\n    networkx DiGraph\n            a directed graph\n\n    \"\"\"\n    return nx.from_numpy_array(adjacency_matrix,create_using=nx.DiGraph)\n</code></pre>"},{"location":"network_classic/#src.connalysis.network.classic.nx_to_np","title":"<code>nx_to_np(directed_graph)</code>","text":"<p>Converts networkx digraph to numpy array of the adjacency matrix </p> <p>Parameters:</p> Name Type Description Default <code>directed_graph</code> <code>networkx DiGraph</code> <p>a directed graph</p> required <p>Returns:</p> Type Description <code>numpy array</code> <p>the adjaceny matrix of the DiGraph as a numpy array</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def nx_to_np(directed_graph):\n    \"\"\"Converts networkx digraph to numpy array of the adjacency matrix \n\n    Parameters\n    ----------\n    directed_graph : networkx DiGraph\n        a directed graph\n\n    Returns\n    -------\n    numpy array\n        the adjaceny matrix of the DiGraph as a numpy array\n    \"\"\"\n    return nx.to_numpy_array(directed_graph,dtype=int)\n</code></pre>"},{"location":"network_classic/#src.connalysis.network.classic.rich_club_curve","title":"<code>rich_club_curve(m, direction='TOTAL')</code>","text":"<p>Compute the rich-club curve of the matrix m.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>array_like or sparse matrix</code> <p>Adjacency matrix of the graph.</p> required <code>direction</code> <code>str {'OUT', 'IN', 'TOTAL'}</code> <p>'OUT' : Compute the rich-club curve for the out-degree.</p> <p>'IN' : Compute the rich-club curve for the in-degree.</p> <p>'TOTAL' : Compute the rich-club curve for the total degree i.e., in-degree + out-degree.</p> <code>'TOTAL'</code> <p>Returns:</p> Type Description <code>Pandas series</code> <p>With index the center of the binned degrees and values the rich club coefficient at that degree.</p> Notes <p>The rich-club coefficient is a measure of the tendency of high-degree nodes to form tightly interconnected communities.</p> Source code in <code>src/connalysis/network/classic.py</code> <pre><code>def rich_club_curve(m, direction='TOTAL'):\n    \"\"\"Compute the rich-club curve of the matrix m.\n\n    Parameters\n    ----------\n    m : array_like or sparse matrix\n        Adjacency matrix of the graph.\n    direction : str {'OUT', 'IN', 'TOTAL'}\n        'OUT' : Compute the rich-club curve for the out-degree.\n\n        'IN' : Compute the rich-club curve for the in-degree.\n\n        'TOTAL' : Compute the rich-club curve for the total degree i.e., in-degree + out-degree.\n\n    Returns\n    -------\n    Pandas series\n        With index the center of the binned degrees and values the rich club coefficient at that degree.\n    Notes\n    -----\n    The rich-club coefficient is a measure of the tendency of high-degree nodes to form tightly interconnected communities.\n    \"\"\"\n    m = sp.csc_matrix(m)\n    if direction == 'TOTAL': direction = None \n    degrees = node_degree(m, direction=direction)\n    if m.dtype == bool:\n        udegrees = np.arange(1, degrees.max() + 1)\n        ret_x = udegrees\n    else:\n        ret_x, udegrees, degrees = _bin_degrees(degrees)\n\n    edge_counter = lambda i: (degrees &gt;= i).sum() * ((degrees &gt;= i).sum() - 1)  # number of pot. edges\n    mat_counter = lambda i: m[np.ix_(degrees &gt;= i, degrees &gt;= i)].sum()  # number of actual edges\n    ret = (np.array([mat_counter(i) for i in udegrees]).astype(float)\n           / np.array([edge_counter(i) for i in udegrees]))\n    return pd.Series(ret, index=pd.Index(ret_x, name=\"degree\"))\n</code></pre>"},{"location":"network_local/","title":"Local","text":""},{"location":"network_local/#these-are-utility-functions-used-to-restrict-analyses-to-neighborhoods","title":"These are utility functions used to restrict analyses to neighborhoods.","text":""},{"location":"network_local/#src.connalysis.network.local.neighborhood","title":"<code>neighborhood(adj, v, pre=True, post=True, include_center=True, return_neighbors=False)</code>","text":"<p>Gets the neighborhood of v in adj</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse matrix or 2d array</code> <p>The adjacency matrix of the graph</p> required <code>pre</code> <code>bool</code> <p>If <code>True</code> compute the submatrix on the nodes mapping to v (the in-neighbors of v)</p> <code>True</code> <code>post</code> <code>bool</code> <p>If <code>True</code> compute the submatrix on the nodes mapping to v (the out-neighbors of v)</p> <code>True</code> <code>include_center</code> <code>bool</code> <p>If <code>True</code> include v in the neighborhood</p> <code>True</code> <code>return_neighbors</code> <code>bool</code> <p>If <code>True</code> also return the indices in adj of the neighbors of v</p> <code>False</code> <p>Returns:</p> Type Description <code>matrix (sparse if adj is sparse)</code> <p>If pre = post = <code>True</code> it returns the full neighbohood of v</p> <p>If pre = <code>True</code> and post = <code>False</code>it returns the in-neighborhood of v</p> <p>If pre = <code>False</code> and post = <code>True</code>it returns the out-neighborhood of v</p> <p>If include_center = <code>True</code>, then v is the first indexed node in the matrix, else it is excluded</p> Source code in <code>src/connalysis/network/local.py</code> <pre><code>def neighborhood(adj, v, pre=True, post=True, include_center=True, return_neighbors=False):\n    \"\"\"Gets the neighborhood of v in adj\n            Parameters\n            ----------\n            adj : sparse matrix or 2d array\n                The adjacency matrix of the graph\n            pre : bool\n                If ``True`` compute the submatrix on the nodes mapping to v (the in-neighbors of v)\n            post : bool\n                If ``True`` compute the submatrix on the nodes mapping to v (the out-neighbors of v)\n            include_center : bool\n                If ``True`` include v in the neighborhood\n            return_neighbors : bool\n                If ``True`` also return the indices in adj of the neighbors of v\n\n            Returns\n            -------\n            matrix (sparse if adj is sparse)\n                If pre = post = ``True`` it returns the full neighbohood of v\n\n                If pre = ``True`` and post = ``False``it returns the in-neighborhood of v\n\n                If pre = ``False`` and post = ``True``it returns the out-neighborhood of v\n\n                If include_center = ``True``, then v is the first indexed node in the matrix,\n                else it is excluded\n        \"\"\"\n    nb_df=neighborhood_indices(adj, pre=pre, post=post, all_nodes=False, centers=np.array([v]))\n    if sp.issparse(adj): adj=adj.tocsr()\n    nb_ind = nb_df.loc[v]\n    if include_center:\n        nb_ind = np.append(v, nb_ind)\n    if return_neighbors:\n        return submat_at_ind(adj, nb_ind), nb_ind\n    else:\n        return submat_at_ind(adj, nb_ind)\n</code></pre>"},{"location":"network_local/#src.connalysis.network.local.neighborhood_indices","title":"<code>neighborhood_indices(M, pre=True, post=True, all_nodes=True, centers=None)</code>","text":"<p>Computes the indices of the neighbors of the nodes listed in centers</p> <p>Parameters:</p> Name Type Description Default <code>M</code> <code>sparse matrix or 2d array</code> <p>The adjacency matrix of the graph</p> required <code>pre</code> <code>bool</code> <p>If <code>True</code> compute the nodes mapping to the nodes in centers (the in-neighbors of the centers)</p> <code>True</code> <code>post</code> <code>bool</code> <p>If <code>True</code> compute the nodes that the centers map to (the out-neighbors of the centers)</p> <code>True</code> <code>all_nodes</code> <code>bool</code> <p>If <code>True</code> compute the neighbors of all nodes in M, if <code>False</code> compute only the neighbors of the nodes listed in centers</p> <code>True</code> <code>centers</code> <code>1d-array</code> <p>The indices of the nodes for which the neighbors need to be computed.  This entry is ignored if all_nodes is <code>True</code> and required if all_nodes is <code>False</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>data frame</code> <p>indices: range from 0 to <code>M.shape[0]</code> if all_nodes is set to <code>True</code>, otherwise centers.</p> <p>values: the neighbors of each center in the indices.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the matrix M is not square</p> <code>AssertionError</code> <p>If both pre and post are <code>False</code></p> <code>AssertionError</code> <p>If all_nodes is <code>False</code> but centers are not provided</p> Source code in <code>src/connalysis/network/local.py</code> <pre><code>def neighborhood_indices(M, pre=True, post=True, all_nodes=True, centers=None):\n    \"\"\"Computes the indices of the neighbors of the nodes listed in centers\n\n        Parameters\n        ----------\n        M : sparse matrix or 2d array\n            The adjacency matrix of the graph\n        pre : bool\n            If ``True`` compute the nodes mapping to the nodes in centers (the in-neighbors of the centers)\n        post : bool\n            If ``True`` compute the nodes that the centers map to (the out-neighbors of the centers)\n        all_nodes : bool\n            If ``True`` compute the neighbors of all nodes in M, if ``False`` compute only the neighbors of the nodes\n            listed in centers\n        centers : 1d-array\n            The indices of the nodes for which the neighbors need to be computed.  This entry is ignored if\n            all_nodes is ``True`` and required if all_nodes is ``False``.\n\n        Returns\n        -------\n        data frame\n            indices: range from 0 to ``M.shape[0]`` if all_nodes is set to ``True``, otherwise centers.\n\n            values: the neighbors of each center in the indices.\n\n        Raises\n        ------\n        AssertionError\n            If the matrix M is not square\n        AssertionError\n            If both pre and post are ``False``\n        AssertionError\n            If all_nodes is ``False`` but centers are not provided\n\n    \"\"\"\n    assert M.shape[0] == M.shape[1], \"The matrix is not square\"\n    assert np.logical_or(pre, post), \"At least one of the pre/post parameters must be True\"\n    if all_nodes: centers = np.arange(M.shape[0])\n    assert centers is not None, \"If all_nodes == False and array of centers must be provided\"\n\n    M = M.tocoo() if sp.issparse(M) else sp.coo_matrix(M)\n\n    base_df = pd.DataFrame({\"row\": M.row, \"col\": M.col})\n    idxx = pd.Index(centers, name=\"center\")\n    nb_df = pd.Series([[]] * centers.shape[0], index=idxx, name=\"neighbors\")\n    # Restriction to requested centers\n    if not all_nodes: res_df = base_df.apply(lambda _x: np.isin(_x, centers))\n\n    if post:\n        df = base_df[res_df['row']] if not all_nodes else base_df\n        new_df = df.rename(columns={\"row\": \"center\", \"col\": \"neighbors\"}).groupby(\"center\")[\"neighbors\"].apply(list)\n        nb_df = nb_df.combine(new_df, np.union1d, fill_value=[])\n    if pre:\n        df = base_df[res_df['col']] if not all_nodes else base_df\n        new_df = df.rename(columns={\"col\": \"center\", \"row\": \"neighbors\"}).groupby(\"center\")[\"neighbors\"].apply(list)\n        nb_df = nb_df.combine(new_df, np.union1d, fill_value=[])\n    return nb_df.apply(lambda _x: _x.astype(int))\n</code></pre>"},{"location":"network_local/#src.connalysis.network.local.neighborhood_of_set","title":"<code>neighborhood_of_set(M, node_set, pre=True, post=True, include_centers=True, return_neighbors=False)</code>","text":"<p>Gets the neighborhood of the nodes in node_set</p> <p>Parameters:</p> Name Type Description Default <code>M</code> <code>sparse matrix or 2d array</code> <p>The adjacency matrix of the graph</p> required <code>node_set</code> <code>array</code> <p>The indices of the nodes of which the neighborhood will be computed</p> required <code>pre</code> <code>bool</code> <p>If <code>True</code> compute the submatrix on the nodes mapping to node_set (the in-neighbors of node_set)</p> <code>True</code> <code>post</code> <code>bool</code> <p>If <code>True</code> compute the submatrix on the nodes that node_set maps to (the out-neighbors of node_set)</p> <code>True</code> <code>include_center</code> <code>bool</code> <p>If <code>True</code> include node_set in the graph</p> required <code>return_neighbors</code> <code>bool</code> <p>If <code>True</code> also return the indices in M of the neighbors of node_set</p> <code>False</code> <p>Returns:</p> Type Description <code>matrix (sparse if M is sparse)</code> <p>If pre = post = <code>True</code> it returns the full neighbohood of node_set</p> <p>If pre = <code>True</code> and post = <code>False</code>it returns the in-neighborhood of node_set</p> <p>If pre = <code>False</code> and post = <code>True</code>it returns the out-neighborhood of node_set</p> Source code in <code>src/connalysis/network/local.py</code> <pre><code>def neighborhood_of_set(M, node_set, pre=True, post=True, include_centers=True, return_neighbors=False):\n    \"\"\"Gets the neighborhood of the nodes in node_set\n            Parameters\n            ----------\n            M : sparse matrix or 2d array\n                The adjacency matrix of the graph\n            node_set : array\n                The indices of the nodes of which the neighborhood will be computed\n            pre : bool\n                If ``True`` compute the submatrix on the nodes mapping to node_set (the in-neighbors of node_set)\n            post : bool\n                If ``True`` compute the submatrix on the nodes that node_set maps to (the out-neighbors of node_set)\n            include_center : bool\n                If ``True`` include node_set in the graph\n            return_neighbors : bool\n                If ``True`` also return the indices in M of the neighbors of node_set\n\n            Returns\n            -------\n            matrix (sparse if M is sparse)\n                If pre = post = ``True`` it returns the full neighbohood of node_set\n\n                If pre = ``True`` and post = ``False``it returns the in-neighborhood of node_set\n\n                If pre = ``False`` and post = ``True``it returns the out-neighborhood of node_set\n    \"\"\"\n    nodes=neighborhood_of_set_indices(M, node_set, pre, post)\n    if include_centers:\n        nodes=np.unique(np.concatenate([node_set, nodesA]))\n    if isinstance(M, sp.coo_matrix): M=M.tocsr()\n    # Slicing in different ways depending on matrix type\n    if isinstance(M, np.ndarray): nbd= M[np.ix_(nodes, nodes)]\n    if isinstance(M, sp.csr_matrix): nbd= M[nodes].tocsc()[:, nodes]\n    if isinstance(M, sp.csc_matrix): nbd= M[:,nodes].tocsr()[nodes]\n    if return_neighbors: return nbd, nodes\n    else: return nbd\n</code></pre>"},{"location":"network_local/#src.connalysis.network.local.neighborhood_of_set_indices","title":"<code>neighborhood_of_set_indices(M, node_set, pre=True, post=True)</code>","text":"<p>Computes the indices of the neighbors of the nodes in node_set</p> <p>Parameters:</p> Name Type Description Default <code>M</code> <code>sparse matrix or 2d array</code> <p>The adjacency matrix of the graph</p> required <code>pre</code> <code>bool</code> <p>If <code>True</code> compute the nodes mapping to the nodes in node_set (the in-neighbors of the node_set)</p> <code>True</code> <code>post</code> <code>bool</code> <p>If <code>True</code> compute the nodes that the centers map to node_set (the out-neighbors of the node_set)</p> <code>True</code> <code>node_set</code> <code>1d-array</code> <p>The indices of the nodes for which the neighbors need to be computed.</p> required <p>Returns:</p> Type Description <code>array</code> <p>indices of the neighbhors of node_set</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If both pre and post are <code>False</code></p> Source code in <code>src/connalysis/network/local.py</code> <pre><code>def neighborhood_of_set_indices(M, node_set, pre=True, post=True):\n    \"\"\"Computes the indices of the neighbors of the nodes in node_set\n\n            Parameters\n            ----------\n            M : sparse matrix or 2d array\n                The adjacency matrix of the graph\n            pre : bool\n                If ``True`` compute the nodes mapping to the nodes in node_set (the in-neighbors of the node_set)\n            post : bool\n                If ``True`` compute the nodes that the centers map to node_set (the out-neighbors of the node_set)\n            node_set : 1d-array\n                The indices of the nodes for which the neighbors need to be computed.\n\n            Returns\n            -------\n            array\n                indices of the neighbhors of node_set\n\n            Raises\n            ------\n            AssertionError\n                If both pre and post are ``False``\n    \"\"\"\n    assert np.logical_or(pre, post), \"At least one of the pre/post parameters must be True\"\n    return np.unique(np.concatenate(neighborhood_indices(M,pre=pre,post=post,all_nodes=False,centers=node_set).values))\n</code></pre>"},{"location":"network_local/#src.connalysis.network.local.neighbourhood","title":"<code>neighbourhood(v, matrix)</code>","text":"<p>Computes the matrix induced by the neighbours of v in graph with adjacency matrix matrix</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>int</code> <p>the index of the vertex</p> required <code>matrix</code> <code>matrix</code> <p>the adjacency matrix of the graph</p> required <p>Returns:</p> Type Description <code>matrix</code> <p>the adjaceny matrix of the neighbourhood of v in matrix</p> Source code in <code>src/connalysis/network/local.py</code> <pre><code>def neighbourhood(v, matrix): #Previous name # def tribe(v, matrix):\n    # TODO: Needs to be deleted\n    \"\"\"Computes the matrix induced by the neighbours of v in graph with adjacency matrix matrix\n\n    Parameters\n    ----------\n    v : int\n        the index of the vertex\n    matrix : matrix\n        the adjacency matrix of the graph\n\n    Returns\n    -------\n    matrix\n        the adjaceny matrix of the neighbourhood of v in matrix\n    \"\"\"\n    #print(matrix.getformat())\n    nhbd = neighbours(v, matrix)\n    return matrix[np.ix_(nhbd,nhbd)]\n</code></pre>"},{"location":"network_local/#src.connalysis.network.local.neighbours","title":"<code>neighbours(v, matrix)</code>","text":"<p>Computes the neighbours of v in graph with adjacency matrix matrix</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>int</code> <p>the index of the vertex</p> required <code>matrix</code> <code>matrix</code> <p>the adjacency matrix of the graph</p> required <p>Returns:</p> Type Description <code>list</code> <p>the list of neighbours of v in matrix</p> Source code in <code>src/connalysis/network/local.py</code> <pre><code>def neighbours(v, matrix):\n    #TODO: Delete redundant\n    \"\"\"Computes the neighbours of v in graph with adjacency matrix matrix\n\n    Parameters\n    ----------\n    v : int\n        the index of the vertex\n    matrix : matrix\n        the adjacency matrix of the graph\n\n    Returns\n    -------\n    list\n        the list of neighbours of v in matrix\n    \"\"\"\n    neighbours = np.unique(np.concatenate((np.nonzero(matrix[v])[0],np.nonzero(np.transpose(matrix)[v])[0])))\n    neighbours.sort(kind='mergesort')\n    return np.concatenate((np.array([v]),neighbours))\n</code></pre>"},{"location":"network_local/#src.connalysis.network.local.properties_at_neighborhoods","title":"<code>properties_at_neighborhoods(adj, func_config, pre=True, post=True, include_center=True, all_nodes=True, centers=None)</code>","text":"<p>Computes the properties in func_config on the neighborhoods of the centers within adj</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse matrix or 2d array</code> <p>The adjacency matrix of the graph</p> required <code>func_config</code> <code>dict</code> <p>Configuration dictionary of functions to be computed on neihgborhoods</p> required <code>pre</code> <code>bool</code> <p>If <code>True</code> include the nodes mapping to the nodes in centers</p> <code>True</code> <code>post</code> <code>bool</code> <p>If <code>True</code> include the nodes that the centers map to</p> <code>True</code> <code>include_center</code> <code>bool</code> <p>If <code>True</code> include the centers</p> <code>True</code> <code>all_nodes</code> <code>bool</code> <p>If <code>True</code> compute func on the neighborhoods of all nodes in adj,</p> <p>If <code>False</code> compute it only the neighborhoods of the nodes listed in centers</p> <code>True</code> <code>centers</code> <code>1d-array</code> <p>The indices of the nodes to consider.  This entry is ignored if all_nodes is <code>True</code> and required if all_nodes is <code>False</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>keys: keys of func_config</p> <p>values: dict with</p> <pre><code>keys: range from 0 to ``M.shape[0]`` if all_nodes is set to ``True``, otherwise centers\n\nvalues: the output of ``func`` in each neighborhood of the nodes in centers\n</code></pre> Source code in <code>src/connalysis/network/local.py</code> <pre><code>def properties_at_neighborhoods(adj, func_config, pre=True, post=True, include_center=True,\n                                all_nodes=True, centers=None):\n    \"\"\"Computes the properties in func_config on the neighborhoods of the centers within adj\n\n            Parameters\n            ----------\n            adj : sparse matrix or 2d array\n                The adjacency matrix of the graph\n            func_config : dict\n                Configuration dictionary of functions to be computed on neihgborhoods\n            pre : bool\n                If ``True`` include the nodes mapping to the nodes in centers\n            post : bool\n                If ``True`` include the nodes that the centers map to\n            include_center : bool\n                If ``True`` include the centers\n            all_nodes : bool\n                If ``True`` compute func on the neighborhoods of all nodes in adj,\n\n                If ``False`` compute it only the neighborhoods of the nodes listed in centers\n            centers : 1d-array\n                The indices of the nodes to consider.  This entry is ignored if\n                all_nodes is ``True`` and required if all_nodes is ``False``.\n\n            Returns\n            -------\n            dict\n                keys: keys of func_config\n\n                values: dict with\n\n                    keys: range from 0 to ``M.shape[0]`` if all_nodes is set to ``True``, otherwise centers\n\n                    values: the output of ``func`` in each neighborhood of the nodes in centers\n    \"\"\"\n    nb_df = neighborhood_indices(adj, pre=pre, post=post, all_nodes=all_nodes, centers=centers)\n    if sp.issparse(adj): adj = adj.tocsr()  # TODO: Add warning of format?\n    nbhd_values = {key: {} for key in func_config.keys()}\n    for center in nb_df.index:\n        nb_ind = nb_df.loc[center]\n        if include_center: nb_ind = np.append(center, nb_ind)\n        nbhd = submat_at_ind(adj, nb_ind)\n        for key in func_config.keys():\n            nbhd_values[key][center] = func_config[key]['function'](nbhd, **func_config[key]['kwargs'])\n    return nbhd_values\n</code></pre>"},{"location":"network_local/#src.connalysis.network.local.property_at_neighborhoods","title":"<code>property_at_neighborhoods(adj, func, pre=True, post=True, include_center=True, all_nodes=True, centers=None, **kwargs)</code>","text":"<p>Computes the property func on the neighborhoods of the centers within adj</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse matrix or 2d array</code> <p>The adjacency matrix of the graph</p> required <code>func</code> <code>function</code> <p>Function computing a network theoretic property e.g., degree or simplex counts</p> required <code>pre</code> <code>bool</code> <p>If <code>True</code> include the nodes mapping to the nodes in centers</p> <code>True</code> <code>post</code> <code>bool</code> <p>If <code>True</code> include the nodes that the centers map to</p> <code>True</code> <code>include_center</code> <code>bool</code> <p>If <code>True</code> include the centers</p> <code>True</code> <code>all_nodes</code> <code>bool</code> <p>If <code>True</code> compute func on the neighborhoods of all nodes in adj,</p> <p>If <code>False</code> compute it only the neighborhoods of the nodes listed in centers</p> <code>True</code> <code>centers</code> <code>1d-array</code> <p>The indices of the nodes to consider.  This entry is ignored if all_nodes is <code>True</code> and required if all_nodes is <code>False</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>keys: range from 0 to <code>M.shape[0]</code> if all_nodes is set to <code>True</code>, otherwise centers</p> <p>values: the output of <code>func</code> in each neighborhood of the nodes in centers</p> Source code in <code>src/connalysis/network/local.py</code> <pre><code>def property_at_neighborhoods(adj, func, pre=True, post=True,include_center=True,\n                             all_nodes=True, centers=None, **kwargs):\n    \"\"\"Computes the property func on the neighborhoods of the centers within adj\n\n            Parameters\n            ----------\n            adj : sparse matrix or 2d array\n                The adjacency matrix of the graph\n            func : function\n                Function computing a network theoretic property e.g., degree or simplex counts\n            pre : bool\n                If ``True`` include the nodes mapping to the nodes in centers\n            post : bool\n                If ``True`` include the nodes that the centers map to\n            include_center : bool\n                If ``True`` include the centers\n            all_nodes : bool\n                If ``True`` compute func on the neighborhoods of all nodes in adj,\n\n                If ``False`` compute it only the neighborhoods of the nodes listed in centers\n            centers : 1d-array\n                The indices of the nodes to consider.  This entry is ignored if\n                all_nodes is ``True`` and required if all_nodes is ``False``.\n\n            Returns\n            -------\n            dict\n                keys: range from 0 to ``M.shape[0]`` if all_nodes is set to ``True``, otherwise centers\n\n                values: the output of ``func`` in each neighborhood of the nodes in centers\n    \"\"\"\n    nb_df=neighborhood_indices(adj, pre=pre, post=post, all_nodes=all_nodes, centers=centers)\n    if sp.issparse(adj): adj=adj.tocsr()# TODO: Add warning of format!\n    nbhd_values={}\n    for center in nb_df.index:\n        nb_ind = nb_df.loc[center]\n        if include_center: nb_ind=np.append(center, nb_ind)\n        nbhd=submat_at_ind(adj, nb_ind)\n        #nbhd_values.loc[center]=func(nbhd, kwargs)\n        nbhd_values[center]=func(nbhd, **kwargs)\n    return nbhd_values\n</code></pre>"},{"location":"network_local/#src.connalysis.network.local.submat_at_ind","title":"<code>submat_at_ind(M, ind)</code>","text":"<p>Computes the submatrix of M on the nondes indexed by ind</p> <p>Parameters:</p> Name Type Description Default <code>M</code> <code>matrix</code> <p>the adjacency matrix of the graph</p> required <code>ind</code> <code>1d-array</code> <p>the indices on which to slice the matrix M</p> required <p>Returns:</p> Type Description <code>matrix</code> <p>the adjaceny matrix of the submatrix of M on the nodes in ind</p> Source code in <code>src/connalysis/network/local.py</code> <pre><code>def submat_at_ind(M, ind):\n    \"\"\"Computes the submatrix of M on the nondes indexed by ind\n\n    Parameters\n    ----------\n    M : matrix\n        the adjacency matrix of the graph\n    ind : 1d-array\n        the indices on which to slice the matrix M\n\n    Returns\n    -------\n    matrix\n        the adjaceny matrix of the submatrix of M on the nodes in ind\n    \"\"\"\n    if sp.issparse(M): M=M.tocsr()\n    return  M[ind][:, ind]\n</code></pre>"},{"location":"network_stats/","title":"Stats","text":""},{"location":"network_stats/#these-are-utility-functions-used-to-compute-simple-statistics-across-simplices-and-neighborhoods","title":"These are utility functions used to compute simple statistics across simplices and neighborhoods.","text":"<p>Functions to average (functional or structural) metrics across simplices or neighborhoods. Author(s): Daniela Egas Santander, Last update: 11.2023</p>"},{"location":"network_stats/#src.connalysis.network.stats.edge_stats_participation","title":"<code>edge_stats_participation(participation, vals, condition=operator.eq, dims=None)</code>","text":"<p>Get statistics of the values in vals across edges filtered using edge participation</p> <p>Parameters:</p> Name Type Description Default <code>participation</code> <code>DataFrame</code> <p>DataFrame of edge participation with index the edges of an NxN matrix to consider, columns are dimensions and values are edge participation.</p> required <code>values</code> <code>Series</code> <p>pandas Series with index the edges of the NxN matrix of which edge participation has been computed and vals the values on that edge to be averaged.</p> required <code>condition</code> <code>operator</code> <p>operator with which to filter the nodes. The default <code>operator.eq</code> filters nodes such that their maximal dimension of edge participation is a given value. Alternatively, <code>operator.ge</code> filters edges such that their maximal dimension of node participation is at least a given value.</p> <code>eq</code> <code>dims</code> <code>iterable</code> <p>dimensions for which to run the analysis, if <code>None</code> all the columns of participation will be analyzed</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>with index, the dimensions for which the analysis have been run and columns the statistics of the values in vals where the nodes have been grouped according to the condition given.</p> Source code in <code>src/connalysis/network/stats.py</code> <pre><code>def edge_stats_participation(participation, vals, condition=operator.eq, dims=None):\n    \"\"\" Get statistics of the values in vals across edges filtered using edge participation\n\n    Parameters\n    ----------\n    participation : DataFrame\n        DataFrame of edge participation with index the edges of an NxN matrix to consider,\n        columns are dimensions and values are edge participation.\n    values : Series\n        pandas Series with index the edges of the NxN matrix of which edge participation has been computed\n        and vals the values on that edge to be averaged.\n    condition : operator\n        operator with which to filter the nodes. The default ``operator.eq`` filters nodes such that their maximal\n        dimension of edge participation is a given value.\n        Alternatively, ``operator.ge`` filters edges such that their maximal dimension of node participation is at least a given\n        value.\n    dims : iterable\n        dimensions for which to run the analysis, if ``None`` all the columns of participation will be analyzed\n\n    Returns\n    -------\n    DataFrame\n        with index, the dimensions for which the analysis have been run and columns the statistics of the values in vals\n        where the nodes have been grouped according to the condition given.\n    \"\"\"\n    par_df = participation.copy()\n    if dims is None:\n        dims = par_df.columns\n    vals = vals.rename(\"values\")\n    par_df[\"max_dim\"] = (par_df &gt; 0).sum(axis=1)  # maximal dimension an edge is part of. Note that edge participation in dimension 0 is 0\n    stats_vals = {}\n    for dim in dims:\n        mask = condition(par_df.max_dim, dim)\n        c = pd.DataFrame(vals.loc[par_df[mask].index], columns=[\"values\"])\n        c[\"weight\"] = par_df[mask][dim]\n        w_mean = c.apply(np.product, axis=1).sum() / (c[\"weight\"].sum())\n        stats_vals[dim] = (c.shape[0],  # Number of nodes fulfilling the condition\n                           np.nanmean(c[\"values\"]),\n                           np.nanstd(c[\"values\"]),\n                           stats.sem(c[\"values\"], nan_policy=\"omit\"),\n                           w_mean  # mean weighted by participation\n                           )\n    stats_vals = pd.DataFrame.from_dict(stats_vals, orient=\"index\",\n                                        columns=[\"counts\", \"mean\", \"std\", \"sem\", \"weighted_mean\"])\n    stats_vals.index.name = \"dim\"\n    return stats_vals.drop(0)\n</code></pre>"},{"location":"network_stats/#src.connalysis.network.stats.node_stats_neighborhood","title":"<code>node_stats_neighborhood(values, adj=None, pre=True, post=True, all_nodes=True, centers=None, include_center=True, precomputed=False, neighborhoods=None)</code>","text":"<p>Get basic statistics of the property values on the neighbhood of the nodes in centers in the graph described by adj.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>pandas Series with index the nodes of the NxN matrix of which the simplices are listed, and values the values on that node to be averaged.</p> required <code>adj</code> <code>sparse matrix or 2d array</code> <p>The adjacency matrix of the graph</p> <code>None</code> <code>pre</code> <code>bool</code> <p>If <code>True</code> compute the nodes mapping to the nodes in centers (the in-neighbors of the centers)</p> <code>True</code> <code>post</code> <code>bool</code> <p>If <code>True</code> compute the nodes that the centers map to (the out-neighbors of the centers)</p> <code>True</code> <code>all_nodes</code> <code>bool</code> <p>If <code>True</code> compute the neighbors of all nodes in adj, if <code>False</code> compute only the neighbors of the nodes listed in centers</p> <code>True</code> <code>centers</code> <code>1d-array</code> <p>The indices of the nodes for which the neighbors need to be computed.  This entry is ignored if all_nodes is <code>True</code> and required if all_nodes is <code>False</code></p> <code>None</code> <code>include_center</code> <code>bool</code> <p>If <code>True</code> it includes the center in the computation otherwise it ignores it</p> <code>True</code> <code>precomputed</code> <code>bool</code> <p>If <code>False</code> it precomputes the neighbhorhoods in adj, if <code>False</code> it skips the computation and reads it fromt the input</p> <code>False</code> <code>neighborhoods</code> <code>DataFrame</code> <p>DataFrame of neighbhoord indices. Required if precomputed is <code>True</code></p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>with index, centers to be considered and columns the sum, mean, standard deviation and standard error of the mean of the values in that neighborhood.</p> See Also <p>[neighborhood_indices] (network_local.md#src.connalysis.network.local.neighborhood_indices): Function to precompute the neighborhood_indices that can be used if precomputed is set <code>True</code>. Precomputing the neighborhoods would increase efficiency if multiple properties are averaged across neighborhoods.</p> Source code in <code>src/connalysis/network/stats.py</code> <pre><code>def node_stats_neighborhood(values, adj=None, pre=True, post=True, all_nodes=True, centers=None,\n                            include_center=True, precomputed=False, neighborhoods=None):\n    \"\"\" Get basic statistics of the property values on the neighbhood of the nodes in centers in the\n    graph described by adj.\n    Parameters\n    ----------\n    values : Series\n        pandas Series with index the nodes of the NxN matrix of which the simplices are listed,\n        and values the values on that node to be averaged.\n    adj : sparse matrix or 2d array\n        The adjacency matrix of the graph\n    pre : bool\n        If ``True`` compute the nodes mapping to the nodes in centers (the in-neighbors of the centers)\n    post : bool\n        If ``True`` compute the nodes that the centers map to (the out-neighbors of the centers)\n    all_nodes : bool\n        If ``True`` compute the neighbors of all nodes in adj, if ``False`` compute only the neighbors of the nodes\n        listed in centers\n    centers : 1d-array\n        The indices of the nodes for which the neighbors need to be computed.  This entry is ignored if\n        all_nodes is ``True`` and required if all_nodes is ``False``\n    include_center : bool\n        If ``True`` it includes the center in the computation otherwise it ignores it\n    precomputed : bool\n        If ``False`` it precomputes the neighbhorhoods in adj,\n        if ``False`` it skips the computation and reads it fromt the input\n    neighborhoods : DataFrame\n        DataFrame of neighbhoord indices. Required if precomputed is ``True``\n\n    Returns\n    -------\n    DataFrame\n        with index, centers to be considered and columns the sum, mean, standard deviation and\n        standard error of the mean of the values in that neighborhood.\n\n    See Also\n    --------\n    [neighborhood_indices] (network_local.md#src.connalysis.network.local.neighborhood_indices):\n    Function to precompute the neighborhood_indices that can be used if precomputed is set ``True``.\n    Precomputing the neighborhoods would increase efficiency if multiple properties are averaged across neighborhoods.\n    \"\"\"\n\n    # Single value functions for DataFrames\n    def append_center(x):\n        # To include center in the computation\n        return np.append(x[\"center\"], x[\"neighbors\"])\n\n    def mean_nbd(nbd_indices, v):\n        df = v[nbd_indices]\n        return [np.nansum(df), np.nanmean(df), np.nanstd(df), stats.sem(df, nan_policy=\"omit\")]\n\n    # Get neighborhoods\n    if precomputed:\n        assert isinstance(neighborhoods,\n                          pd.Series), \"If precomputed a Series of neighbhoords indexed by their center must be provided\"\n    else:\n        assert (adj is not None), \"If not precomputed and adjancecy matrix must be provided\"\n        neighborhoods = neighborhood_indices(adj, pre=pre, post=post, all_nodes=all_nodes, centers=centers)\n    centers = neighborhoods.index\n    if include_center:\n        neighborhoods = neighborhoods.reset_index().apply(append_center, axis=1)\n    else:\n        neighborhoods = neighborhoods.reset_index(drop=True)\n    stat_vals = pd.DataFrame.from_records(neighborhoods.map(lambda x: mean_nbd(x, values)),\n                                          columns=[\"sum\", \"mean\", \"std\", \"sem\"])\n    stat_vals[\"center\"] = centers\n    return stat_vals.set_index(\"center\")\n</code></pre>"},{"location":"network_stats/#src.connalysis.network.stats.node_stats_participation","title":"<code>node_stats_participation(participation, vals, condition=operator.eq, dims=None)</code>","text":"<p>Get statistics of the values in vals across nodes filtered using node participation</p> <p>Parameters:</p> Name Type Description Default <code>participation</code> <code>DataFrame</code> <p>DataFrame of node participation with index the nodes in nodes of an NxN matrix to consider, columns are dimensions and values are node participation computed with node_participation.</p> required <code>values</code> <code>Series</code> <p>pandas Series with index the nodes of the NxN matrix of where node participation has been computed and vals the values on that node to be averaged.</p> required <code>condition</code> <code>operator</code> <p>operator with which to filter the nodes. The default <code>operator.eq</code> filters nodes such that their maximal dimension of node participation is a given value. Alternatively, <code>operator.ge</code> filters nodes such that their maximal dimension of node participation is at least a given value.</p> <code>eq</code> <code>dims</code> <code>iterable</code> <p>dimensions for which to run the analysis, if <code>None</code> all the columns of participation will be analyzed</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>with index, the dimensions for which the analysis have been run and columns the statistics of the values in vals where the nodes have been grouped according to the condition given.</p> See Also <p>node_stats_per_position_single: A similar function where the position of the nodes in the simplex are taken into account.  Note in particular that if condition = <code>operator.ge</code> the weighted_mean of this analyisis is equivalent than the value given by this function for position <code>all</code>. However the computation using node_participation is more efficient.</p> Source code in <code>src/connalysis/network/stats.py</code> <pre><code>def node_stats_participation(participation, vals, condition=operator.eq, dims=None):\n    \"\"\" Get statistics of the values in vals across nodes filtered using node participation\n    Parameters\n    ----------\n    participation : DataFrame\n        DataFrame of node participation with index the nodes in nodes of an NxN matrix to consider,\n        columns are dimensions and values are node participation computed with\n        [node_participation](network_topology.md#src.connalysis.network.topology.node_participation).\n    values : Series\n        pandas Series with index the nodes of the NxN matrix of where node participation has been computed\n        and vals the values on that node to be averaged.\n    condition : operator\n        operator with which to filter the nodes. The default ``operator.eq`` filters nodes such that their maximal\n        dimension of node participation is a given value.\n        Alternatively, ``operator.ge`` filters nodes such that their maximal dimension of node participation is at least a given\n        value.\n    dims : iterable\n        dimensions for which to run the analysis, if ``None`` all the columns of participation will be analyzed\n\n    Returns\n    -------\n    DataFrame\n        with index, the dimensions for which the analysis have been run and columns the statistics of the values in vals\n        where the nodes have been grouped according to the condition given.\n\n    See Also\n    --------\n    [node_stats_per_position_single](network_stats.md#src.connalysis.network.stats.node_stats_per_position_single):\n    A similar function where the position of the nodes in the simplex are taken into account.  Note in particular that\n    if condition = ``operator.ge`` the weighted_mean of this analyisis is equivalent than the value given by this function for position ``all``.\n    However the computation using\n    [node_participation](network_topology.md#src.connalysis.network.topology.node_participation)\n    is more efficient.\n    \"\"\"\n    par_df = participation.copy()\n    if dims is None:\n        dims = par_df.columns\n    vals = vals.rename(\"values\")\n    par_df[\"max_dim\"] = (par_df &gt; 0).sum(axis=1) - 1  # maximal dimension a node is part of\n    stats_vals = {}\n    for dim in dims:\n        mask = condition(par_df.max_dim, dim)\n        c = pd.DataFrame(vals.loc[par_df[mask].index], columns=[\"values\"])\n        c[\"weight\"] = par_df[mask][dim]\n        w_mean = c.apply(np.product, axis=1).sum() / (c[\"weight\"].sum())\n        stats_vals[dim] = (c.shape[0],  # Number of nodes fulfilling the condition\n                           np.nanmean(c[\"values\"]),\n                           np.nanstd(c[\"values\"]),\n                           stats.sem(c[\"values\"], nan_policy=\"omit\"),\n                           w_mean  # mean weighted by participation\n                           )\n    stats_vals = pd.DataFrame.from_dict(stats_vals, orient=\"index\",\n                                        columns=[\"counts\", \"mean\", \"std\", \"sem\", \"weighted_mean\"])\n    stats_vals.index.name = \"dim\"\n    return stats_vals\n</code></pre>"},{"location":"network_stats/#src.connalysis.network.stats.node_stats_per_position","title":"<code>node_stats_per_position(simplex_lists, values, dims=None, with_multiplicity=True)</code>","text":"<p>Get across dimensions mean, standard deviation and standard error of the mean averaged across simplex lists and filtered per position</p> <p>Parameters:</p> Name Type Description Default <code>simplex_lists</code> <code>dict</code> <p>keys : are int values representing dimensions values : for key <code>k</code> array of dimension (no. of simplices, <code>k</code>) listing simplices to be considered. Each row corresponds to a list of nodes on a simplex indexed by the order of the nodes in an NxN matrix. All entries must be an index in values</p> required <code>values</code> <code>Series</code> <p>pandas Series with index the nodes of the NxN matrix of which the simplices are listed, and values the values on that node to be averaged.</p> required <code>with_multiplicity</code> <code>bool</code> <p>if <code>True</code> the values are averaged with multiplicity i.e., they are weighted by the number of times a node participates in a simplex in a given position if <code>False</code> repetitions of a node in a given position are ignored.</p> <code>True</code> <code>dims</code> <code>iterable</code> <p>dimensions for which to run the analysis, if <code>None</code> all the keys of simplex lists will be analyzed</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>keys the dimensions anlayzed and values for key <code>k</code> a DataFrame with index, the possible positions of o node in a <code>k</code>-simplex and columns the mean, standard deviation and standard error of the mean for that position.</p> Source code in <code>src/connalysis/network/stats.py</code> <pre><code>def node_stats_per_position(simplex_lists, values, dims=None, with_multiplicity=True):\n    \"\"\" Get across dimensions mean, standard deviation and standard error of the mean averaged across simplex lists\n    and filtered per position\n    Parameters\n    ----------\n    simplex_lists : dict\n        keys : are int values representing dimensions\n        values : for key ``k`` array of dimension (no. of simplices, ``k``) listing simplices to be considered.\n        Each row corresponds to a list of nodes on a simplex indexed by the order of the nodes in an NxN matrix.\n        All entries must be an index in values\n    values : Series\n        pandas Series with index the nodes of the NxN matrix of which the simplices are listed,\n        and values the values on that node to be averaged.\n    with_multiplicity : bool\n        if ``True`` the values are averaged with multiplicity i.e., they are weighted by the number of times a node\n        participates in a simplex in a given position\n        if ``False`` repetitions of a node in a given position are ignored.\n    dims : iterable\n        dimensions for which to run the analysis, if ``None`` all the keys of simplex lists will be analyzed\n\n    Returns\n    -------\n    dict\n        keys the dimensions anlayzed and values for key ``k`` a DataFrame\n        with index, the possible positions of o node in a ``k``-simplex and columns the mean, standard deviation and\n        standard error of the mean for that position.\n    \"\"\"\n    if dims is None:\n        dims = simplex_lists.index\n    stats_dict = {}\n    for dim in tqdm(dims):\n        sl = simplex_lists.loc[dim]\n        stats_dict[dim] = node_stats_per_position_single(sl, values, with_multiplicity=with_multiplicity)\n    return stats_dict\n</code></pre>"},{"location":"network_stats/#src.connalysis.network.stats.node_stats_per_position_single","title":"<code>node_stats_per_position_single(simplex_list, values, with_multiplicity=True)</code>","text":"<p>Get mean, standard deviation and standard error of the mean averaged across simplex lists and filtered per position</p> <p>Parameters:</p> Name Type Description Default <code>simplex_list</code> <code>2d-array</code> <p>Array of dimension (no. of simplices, dimension) listing simplices to be considered. Each row corresponds to a list of nodes on a simplex indexed by the order of the nodes in an NxN matrix. All entries must be an index in values</p> required <code>values</code> <code>Series</code> <p>pandas Series with index the nodes of the NxN matrix of which the simplices are listed, and values the values on that node to be averaged.</p> required <code>with_multiplicity</code> <code>bool</code> <p>if <code>True</code> the values are averaged with multiplicity i.e., they are weighted by the number of times a node participates in a simplex in a given position if <code>False</code> repetitions of a node in a given position are ignored.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>with index, the possible positions of o node in a <code>k</code>-simplex and columns the mean, standard deviation and standard error of the mean for that position</p> Source code in <code>src/connalysis/network/stats.py</code> <pre><code>def node_stats_per_position_single(simplex_list, values, with_multiplicity=True):\n    \"\"\" Get mean, standard deviation and standard error of the mean averaged across simplex lists and filtered per position\n    Parameters\n    ----------\n    simplex_list : 2d-array\n        Array of dimension (no. of simplices, dimension) listing simplices to be considered.\n        Each row corresponds to a list of nodes on a simplex indexed by the order of the nodes in an NxN matrix.\n        All entries must be an index in values\n    values : Series\n        pandas Series with index the nodes of the NxN matrix of which the simplices are listed,\n        and values the values on that node to be averaged.\n    with_multiplicity : bool\n        if ``True`` the values are averaged with multiplicity i.e., they are weighted by the number of times a node\n        participates in a simplex in a given position\n        if ``False`` repetitions of a node in a given position are ignored.\n\n    Returns\n    -------\n    DataFrame\n        with index, the possible positions of o node in a ``k``-simplex and columns the mean, standard deviation and\n        standard error of the mean for that position\n    \"\"\"\n    # Filter values\n    if with_multiplicity:\n        vals_sl = values.loc[simplex_list.flatten()].to_numpy().reshape(simplex_list.shape)\n    else:\n        vals_sl = pd.concat([values.loc[np.unique(simplex_list[:, pos])] for pos in range(simplex_list.shape[1])],\n                            axis=1, keys=range(simplex_list.shape[1]))\n    # Compute stats\n    stats_vals = pd.DataFrame(index=pd.Index(range(simplex_list.shape[1]), name=\"position\"))\n    # Stats per position\n    stats_vals[\"mean\"] = np.nanmean(vals_sl, axis=0)\n    stats_vals[\"std\"] = np.nanstd(vals_sl, axis=0)\n    stats_vals[\"sem\"] = stats.sem(vals_sl, axis=0, nan_policy=\"omit\")\n    # Stats in any position\n    stats_vals.loc[\"all\", \"mean\"] = np.nanmean(vals_sl)\n    stats_vals.loc[\"all\", \"std\"] = np.nanstd(vals_sl)\n    stats_vals.loc[\"all\", \"sem\"] = stats.sem(vals_sl, axis=None, nan_policy=\"omit\")\n    return stats_vals\n</code></pre>"},{"location":"network_topology/","title":"Topology","text":""},{"location":"network_topology/#these-functions-describe-topologically-motivated-network-metrics","title":"These functions describe topologically motivated network metrics.","text":""},{"location":"network_topology/#src.connalysis.network.topology._binary2simplex","title":"<code>_binary2simplex(address, test=None, verbosity=1000000)</code>","text":"<p>...Not used --- keeping it here as it is of interest to understanmd how simplices are represented on the disc by Flagser.</p>"},{"location":"network_topology/#src.connalysis.network.topology._binary2simplex--input-address-of-binary-file-storing-simplices","title":"INPUT: Address of binary file storing simplices","text":""},{"location":"network_topology/#src.connalysis.network.topology._binary2simplex--output-a-list-if-lists-l-where-li-contains-the-vertex-ids-of-the-ith-simplex","title":"OUTPUT: A list if lists L where L[i] contains the vertex ids of the i'th simplex,","text":""},{"location":"network_topology/#src.connalysis.network.topology._binary2simplex--note-the-simplices-appear-in-no-particular-order","title":"note the simplices appear in no particular order","text":"Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def _binary2simplex(address, test=None, verbosity=1000000):\n    \"\"\"...Not used --- keeping it here as it is of interest to understanmd\n    how simplices are represented on the disc by Flagser.\n    #INPUT: Address of binary file storing simplices\n    #OUTPUT: A list if lists L where L[i] contains the vertex ids of the i'th simplex,\n    #          note the simplices appear in no particular order\n\n    \"\"\"\n    LOG.info(\"Load binary simplex info from %s\", address)\n    simplex_info = pd.Series(np.fromfile(address, dtype=np.uint64))\n    LOG.info(\"Done loading binary simplex info.\")\n\n    if test:\n        simplex_info = simplex_info.iloc[0:test]\n\n    mask64 = np.uint(1) &lt;&lt; np.uint(63)\n    mask21 = np.uint64(1 &lt;&lt; 21) - np.uint64(1)\n    mask42 = (np.uint64(1 &lt;&lt; 42) - np.uint64(1)) ^ mask21\n    mask63 = ((np.uint64(1 &lt;&lt; 63) - np.uint64(1)) ^ mask42) ^ mask21\n    end = np.uint64(2 ** 21 - 1)\n\n    def decode_vertices(integer):\n        decode_vertices.ncalls += 1\n        if decode_vertices.ncalls % verbosity == 0:\n            mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n            LOG.info(\"\\t progress %s / %s memory %s\",\n                     decode_vertices.ncalls , len(simplex_info), mem_used)\n        integer = np.uint64(integer)\n        start = not((integer &amp; mask64) &gt;&gt; np.uint64(63))\n        v0 = integer &amp; mask21\n        v1 = (integer &amp; mask42) &gt;&gt; np.uint64(21)\n        v2 = (integer &amp; mask63) &gt;&gt; np.uint64(42)\n        vertices = [v for v in [v0, v1, v2] if v != end]\n        return pd.Series([start, vertices], index=[\"start\", \"vertices\"])\n    #    vertices = [start, v0, v1, v2]\n    #    return pd.Series(vertices, index=[\"start\", 0, 1, 2])\n    decode_vertices.ncalls = 0\n\n    LOG.info(\"Decode the simplices into simplex vertices\")\n    vertices = simplex_info.apply(decode_vertices)\n    LOG.info(\"Done decoding to simplex vertices\")\n\n    vertices = (vertices.assign(sid=np.cumsum(vertices.start))\n                .reset_index(drop=True))\n\n    simplices = (vertices.set_index(\"sid\").vertices\n                 .groupby(\"sid\").apply(np.hstack))\n\n    if not test:\n        return simplices\n\n    return (vertices.vertices, simplices)\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology._convex_hull","title":"<code>_convex_hull(adj, node_properties)</code>","text":"<p>Return the convex hull of the sub gids in the 3D space using x,y,z position for gids</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def _convex_hull(adj, node_properties):# --&gt; topology\n    \"\"\"Return the convex hull of the sub gids in the 3D space using x,y,z position for gids\"\"\"\n    pass\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology._flagser_counts","title":"<code>_flagser_counts(adjacency, max_simplices=False, count_node_participation=False, list_simplices=False, threads=8, max_dim=-1, edge_containment=False, vertices_todo='')</code>","text":"<p>Call package <code>pyflagsercount's flagser_count</code> method that can be used to compute some analyses, getting counts of quantities such as simplices, or node-participation (a.k.a. <code>containment</code>)</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def _flagser_counts(adjacency,\n                    max_simplices=False,\n                    count_node_participation=False,\n                    list_simplices=False,\n                    threads=8,max_dim=-1, edge_containment=False, vertices_todo=''):\n    \"\"\"Call package `pyflagsercount's flagser_count` method that can be used to compute\n    some analyses, getting counts of quantities such as simplices,\n    or node-participation (a.k.a. `containment`)\n    \"\"\"\n    import pyflagsercount\n    adjacency = sp.csr_matrix(adjacency.astype(bool).astype(int))\n    if np.count_nonzero(adjacency.diagonal()) != 0:\n        logging.warning('The diagonal is non-zero!  Non-zero entries in the diagonal will be ignored.')\n\n\n    flagser_counts = pyflagsercount.flagser_count(adjacency,\n                                                  max_simplices=max_simplices,\n                                                  containment=count_node_participation,\n                                                  return_simplices=list_simplices,\n                                                  threads=threads,max_dim=max_dim,\n                                                  edge_containment=edge_containment,\n                                                  vertices_todo=vertices_todo)\n\n    counts =  {\"euler\": flagser_counts.pop(\"euler\"),\n               \"simplex_counts\": _series_by_dim(flagser_counts.pop(\"cell_counts\"),\n                                                name=\"simplex_count\", name_index=\"dim\"),\n               \"max_simplex_counts\": _series_by_dim(flagser_counts.pop(\"max_cell_counts\", None),\n                                                    name=\"max_simplex_count\", name_index=\"dim\"),\n               \"simplices\": flagser_counts.pop(\"simplices\", None)}\n    if counts[\"max_simplex_counts\"] is None:\n        max_dim_participation=counts[\"simplex_counts\"].shape[0]\n    else:\n        max_dim_participation=counts[\"max_simplex_counts\"].shape[0]\n    counts[\"node_participation\"]= _frame_by_dim(flagser_counts.pop(\"contain_counts\", None),max_dim_participation,\n                                                name=\"node_participation\", index=\"node\")\n    counts.update(flagser_counts)\n    return counts\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology._frame_by_dim","title":"<code>_frame_by_dim(from_array, no_columns, name, index)</code>","text":"<p>A dataframe of counts, like node participation: one count for a node and simplex dimension.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def _frame_by_dim(from_array, no_columns, name, index):\n    \"\"\"A dataframe of counts, like node participation:\n    one count for a node and simplex dimension.\n    \"\"\"\n    if from_array is None:\n        return None\n    #Todo add method for when no_columns is not given\n    columns = pd.Index(range(no_columns), name=index)\n    return pd.DataFrame(from_array, columns=columns).fillna(0).astype(int)\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology._get_triad_id","title":"<code>_get_triad_id(edges)</code>","text":"<p>Given a the edges on a graph on nodes {0,1,2} return a list of edges indexed from 0 to 8 as in label_edges.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>tuple of pairs</code> <p>Each pair is of the form \\((i,j)\\) where \\(i, j \\in \\{0,1,2}\\).</p> required <p>Returns:</p> Type Description <code>list of integers between 0 and 8 indexing the edges as in label_edges</code> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def _get_triad_id(edges):\n    \"\"\"Given a the edges on a graph on nodes {0,1,2}\n    return a list of edges indexed from 0 to 8 as in label_edges.\n\n    Parameters\n    ----------\n    edges : tuple of pairs\n        Each pair is of the form $(i,j)$ where $i, j \\\\in \\\\{0,1,2}$.\n\n    Returns\n    -------\n    list of integers between 0 and 8 indexing the edges as in label_edges\n    \"\"\"\n    row, col = tuple(zip(*edges))\n    return tuple(np.sort(label_edges[row, col]))\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology._series_by_dim","title":"<code>_series_by_dim(from_array, name_index=None, index=None, name=None)</code>","text":"<p>A series of counts, like simplex counts: one count for a given value of simplex dimension.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def _series_by_dim(from_array, name_index=None, index=None, name=None):\n    \"\"\"A series of counts, like simplex counts:\n    one count for a given value of simplex dimension.\n    \"\"\"\n    if from_array is None:\n        return None\n    if index is None:\n        index = pd.Index(range(len(from_array)), name=name_index)\n    else:\n        assert len(index)==len(from_array), \"array and index are not the same length\"\n        index = pd.Index(index, name=name_index)\n    return pd.Series(from_array, index=index, name=name)\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.bedge_counts","title":"<code>bedge_counts(adjacency, simplices=None, max_simplices=False, max_dim=-1, simplex_type='directed', **kwargs)</code>","text":"<p>Counts the total number of edges in each position on the subgraphs defined by the nodes of the simplices listed in simplices.  If a simplex list is not passed, simplices are computed on the adjacency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency</code> <code>(N,N)-array or sparse matrix</code> <p>Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>simplices</code> <code>series</code> <p>Series  of 2d-arrays indexed by dimension. Each array is of dimension (no. of simplices, dimension). Each row corresponds to a list of nodes on a simplex.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <code>simplex_type</code> <p>See simplex_counts</p> <code>'directed'</code> <p>Returns:</p> Type Description <code>series</code> <p>pandas series with index dimensions values (dim+1, dim+1) arrays.  The (i,j) entry counts the number of edges from node i to node j on all the subgraphs of adjacency on the nodes of the simplices listed.  See notes.</p> Notes <p>Every directed \\(k\\)-simplex \\([v_o, v_1, \\ldots, v_k]\\) defines as subgraph of the adjacency matrix, with edges \\(v_i \\to v_j\\) whenever \\(i\\leq j\\), but also possibly with ''reverse'' edges.  One can represent this structure with a non-symmetric \\((k+1, k+1)\\)-matrix with <code>1</code>'s for every edge in the subgraph.  The output of this function gives for each dimension the sum of all these matrices over all the simplices provided in <code>simplices</code> or over all the simplices in the adjacency matrix if none is provided.  The lower triangular part of these matrices is therefore a metric of recurrence within simplices, or \"higher dimensional recurrence\". In particular, in dimension 1 it is the number of reciprocal edges in the network.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def bedge_counts(adjacency, simplices=None,\n                 max_simplices = False, max_dim = -1, simplex_type = 'directed', ** kwargs):\n    r\"\"\"Counts the total number of edges in each position on the subgraphs defined by the nodes\n    of the simplices listed in simplices.  If a simplex list is not passed, simplices are computed on\n    the adjacency matrix.\n\n    Parameters\n    ----------\n    adjacency : (N,N)-array or sparse matrix\n        Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    simplices : series\n        Series  of 2d-arrays indexed by dimension.\n        Each array is of dimension (no. of simplices, dimension).\n        Each row corresponds to a list of nodes on a simplex.\n    max_simplices : bool\n        If False counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n    simplex_type: str\n        See [simplex_counts](network_topology.md#src.connalysis.network.topology.simplex_counts)\n\n    Returns\n    -------\n    series\n        pandas series with index dimensions values (dim+1, dim+1) arrays.  The (i,j) entry counts the number of edges\n        from node i to node j on all the subgraphs of adjacency on the nodes of the simplices listed.  See notes.\n\n    Notes\n    -------\n    Every directed $k$-simplex $[v_o, v_1, \\ldots, v_k]$ defines as subgraph of the adjacency matrix, with edges\n    $v_i \\to v_j$ whenever $i\\leq j$, but also possibly with ''reverse'' edges.  One can represent this structure\n    with a non-symmetric $(k+1, k+1)$-matrix with `1`'s for every edge in the subgraph.  The output of this function\n    gives for each dimension the sum of all these matrices over all the simplices provided in `simplices` or over\n    all the simplices in the adjacency matrix if none is provided.  The lower triangular part of these matrices is\n    therefore a metric of recurrence within simplices, or \"higher dimensional recurrence\".\n    In particular, in dimension 1 it is the number of reciprocal edges in the network.\n    \"\"\"\n\n    adj = adjacency\n\n    if simplices is None:\n        LOG.info(\"COMPUTE `bedge_counts(...)`: No argued simplices.\")\n        return bedge_counts(adj,\n                            list_simplices_by_dimension(adj, max_simplices = max_simplices,\n                                                        max_dim = max_dim, simplex_type = simplex_type, ** kwargs))\n    else:\n        LOG.info(\"COMPUTE `bedge_counts(...): for simplices: %s \", simplices.shape)\n\n    dense = np.array(adjacency.toarray(), dtype=int)\n\n    def subset_adj(simplex):\n        return dense[simplex].T[simplex]\n\n    def count_bedges(simplices_given_dim):\n        \"\"\"...\"\"\"\n        try:\n            d_simplices = simplices_given_dim.get_value()\n        except AttributeError:\n            d_simplices = simplices_given_dim\n\n        if d_simplices is None or d_simplices.shape[1] == 1:\n            return np.nan\n\n        return (pd.DataFrame(d_simplices, columns=range(d_simplices.shape[1]))\n                .apply(subset_adj, axis=1)\n                .agg(\"sum\"))\n\n    return simplices.apply(count_bedges)\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.betti_counts","title":"<code>betti_counts(adj, node_properties=None, min_dim=0, max_dim=[], simplex_type='directed', approximation=None, **kwargs)</code>","text":"<p>Count betti counts of flag complex of adj.  Type of flag complex is given by simplex_type.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d (N,N)-array or sparse matrix</code> <p>Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.  Matrix will be cast to 0,1 entries so weights will be ignored.</p> required <code>node_properties</code> <code> data frame</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>min_dim</code> <code>int</code> <p>Minimal dimension from which betti counts are computed. The default min_dim = 0 (counting number of connected components).</p> <code>0</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = [] counts betti numbers up to the maximal dimension of the complex.</p> <code>[]</code> <code>simplex_type</code> <code>string</code> <p>Type of flag complex to consider, given by the type of simplices it is built on. Possible types are:</p> <p>\u2019directed\u2019 - directed simplices (directed flag complex)</p> <p>\u2019undirected\u2019 - simplices in the underlying undirected graph (clique complex of the underlying undirected graph)</p> <p>\u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections (clique complex of the undirected graph of reciprocal connections.)</p> <code>'directed'</code> <code>approximation</code> <code>list of integers  or None</code> <p>Approximation parameter for the computation of the betti numbers.  Useful for large networks. If None all betti numbers are computed exactly. Otherwise, min_dim must be 0 and approximation but be a list of positive integers or -1. The list approximation is either extended by -1 entries on the right or sliced from [0:max_dim+1] to obtain a list of length max_dim.  Each entry of the list denotes the approximation value for the betti computation of that dimension if -1 approximation in that dimension is set to None.</p> <p>If the approximation value at a given dimension is <code>a</code> flagser skips cells creating columns in the reduction matrix with more than <code>a</code> entries.  This is useful for hard problems.  For large, sparse networks a good value if often <code>100,00</code>.  If set to <code>1</code> that dimension will be virtually ignored.  See [1]_</p> <code>None</code> <p>Returns:</p> Type Description <code>series</code> <p>Betti counts indexed per dimension from min_dim to max_dim.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj has non-zero entries in the diagonal which can produce errors.</p> <code>AssertionError</code> <p>If adj is not square.</p> <code>AssertionError</code> <p>If approximation != None and min_dim != 0.</p> See Also <p>simplex_counts : A function that counts the simplices forming the complex from which bettis are count. Simplex types are described there in detail.</p> References <p>For details about the approximation algorithm see</p> <p>..[1] D. Luetgehetmann, \"Documentation of the C++ flagser library\";        GitHub: luetge/flagser.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def betti_counts(adj, node_properties=None,\n                 min_dim=0, max_dim=[], simplex_type='directed', approximation=None,\n                 **kwargs):\n    \"\"\"Count betti counts of flag complex of adj.  Type of flag complex is given by simplex_type.\n\n    Parameters\n    ----------\n    adj : 2d (N,N)-array or sparse matrix\n        Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.  Matrix will be cast to 0,1 entries so weights\n        will be ignored.\n    node_properties :  data frame\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    min_dim : int\n        Minimal dimension from which betti counts are computed.\n        The default min_dim = 0 (counting number of connected components).\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = [] counts betti numbers up to the maximal dimension of the complex.\n    simplex_type : string\n        Type of flag complex to consider, given by the type of simplices it is built on.\n        Possible types are:\n\n        \u2019directed\u2019 - directed simplices (directed flag complex)\n\n        \u2019undirected\u2019 - simplices in the underlying undirected graph (clique complex of the underlying undirected graph)\n\n        \u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections (clique complex of the\n        undirected graph of reciprocal connections.)\n    approximation : list of integers  or None\n        Approximation parameter for the computation of the betti numbers.  Useful for large networks.\n        If None all betti numbers are computed exactly.\n        Otherwise, min_dim must be 0 and approximation but be a list of positive integers or -1.\n        The list approximation is either extended by -1 entries on the right or sliced from [0:max_dim+1] to obtain\n        a list of length max_dim.  Each entry of the list denotes the approximation value for the betti computation\n        of that dimension if -1 approximation in that dimension is set to None.\n\n        If the approximation value at a given dimension is `a` flagser skips cells creating columns in the reduction\n        matrix with more than `a` entries.  This is useful for hard problems.  For large, sparse networks a good value\n        if often `100,00`.  If set to `1` that dimension will be virtually ignored.  See [1]_\n\n    Returns\n    -------\n    series\n        Betti counts indexed per dimension from min_dim to max_dim.\n\n    Raises\n    ------\n    AssertionError\n        If adj has non-zero entries in the diagonal which can produce errors.\n    AssertionError\n        If adj is not square.\n    AssertionError\n        If approximation != None and min_dim != 0.\n\n    See Also\n    --------\n    [simplex_counts](network_topology.md#src.connalysis.network.topology.simplex_counts) :\n    A function that counts the simplices forming the complex from which bettis are count.\n    Simplex types are described there in detail.\n\n    References\n    ----------\n    For details about the approximation algorithm see\n\n    ..[1] D. Luetgehetmann, \"Documentation of the C++ flagser library\";\n           [GitHub: luetge/flagser](https://github.com/luetge/flagser/blob/master/docs/documentation_flagser.pdf).\n\n    \"\"\"\n    LOG.info(\"Compute betti counts for %s-type adjacency matrix and %s-type node properties\",\n             type(adj), type(node_properties))\n\n    from pyflagser import flagser_unweighted\n\n    #Checking matrix\n    adj = sp.csr_matrix(adj).astype(bool).astype('int')\n    assert np.count_nonzero(adj.diagonal()) == 0, 'The diagonal of the matrix is non-zero and this may lead to errors!'\n    N, M = adj.shape\n    assert N == M, 'Dimension mismatch. The matrix must be square.'\n    assert not((not approximation is None) and (min_dim!=0)), \\\n        'For approximation != None, min_dim must be set to 0.  \\nLower dimensions can be ignored by setting approximation to 1 on those dimensions'\n\n    # Symmetrize matrix if simplex_type is not 'directed'\n    if simplex_type == 'undirected':\n        adj = sp.triu(underlying_undirected_matrix(adj))  # symmtrize and keep upper triangular only\n    elif simplex_type == \"reciprocal\":\n        adj = sp.triu(rc_submatrix(adj))  # symmtrize and keep upper triangular only\n    #Computing bettis\n    if max_dim==[]:\n        max_dim=np.inf\n\n    if approximation==None:\n        LOG.info(\"Run without approximation\")\n        bettis = flagser_unweighted(adj, min_dimension=min_dim, max_dimension=max_dim,\n                                    directed=True, coeff=2,\n                                    approximation=None)['betti']\n    else:\n        assert (all([isinstance(item,int) for item in approximation])) # assert it's a list of integers\n        approximation=np.array(approximation)\n        bettis=[]\n\n        #Make approximation vector to be of size max_dim\n        if max_dim!=np.inf:\n            if approximation.size-1 &lt; max_dim:#Vector too short so pad with -1's\n                approximation=np.pad(approximation,\n                                     (0,max_dim-(approximation.size-1)),\n                                     'constant',constant_values=-1)\n            if approximation.size-1&gt;max_dim:#Vector too long, select relevant slice\n                approximation=approximation[0:max_dim+1]\n            #Sanity check\n            LOG.info(\"Correct dimensions for approximation: %s\", approximation.size==max_dim+1)\n\n        #Split approximation into sub-vectors of same value to speed up computation\n        diff=approximation[1:]-approximation[:-1]\n        slice_indx=np.array(np.where(diff!=0)[0])+1\n\n        #Compute betti counts\n        for dims_range in  np.split(np.arange(approximation.size),slice_indx):\n            n=dims_range[0] #min dim for computation\n            N=dims_range[-1] #max dim for computation\n            a=approximation[n]\n            if a==-1:\n                a=None\n            LOG.info(\"Run betti for dim range %s-%s with approximation %s\", n,N,a)\n            bettis=bettis+flagser_unweighted(adj, min_dimension=n, max_dimension=N,\n                                             directed=True, coeff=2,\n                                             approximation=a)['betti']\n\n        if max_dim==np.inf:\n            n=approximation.size #min dim for computation\n            N=np.inf #max dim for computation\n            a=None\n            LOG.info(\"Run betti for dim range %s-%s with approximation %s\",n,N,a)\n            bettis=bettis+flagser_unweighted(adj, min_dimension=n, max_dimension=N,\n                                             directed=True, coeff=2,\n                                             approximation=a)['betti']\n\n    return pd.Series(bettis, name=\"betti_count\",\n                     index=pd.Index(np.arange(min_dim, len(bettis)+min_dim), name=\"dim\"))\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.count_rc_edges_skeleta","title":"<code>count_rc_edges_skeleta(adj=None, max_dim=-1, max_simplices=False, N=None, simplex_list=None, edge_par=None, simplex_type='directed', position='all', return_mats=False, threads=8, **kwargs)</code>","text":"<p>Count the edges and reciprocal edges in the k-<code>skeleta</code> of the directed flag complex of adj for all k&lt;= max_dim. If simplex list are provided, it will compute the skeleta directly from these and not use adj.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>(N,N)-array or sparse matrix</code> <p>Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <code>simplex</code> <p>Series 2d-arrays indexed by dimension. Each array is of dimension (no. of simplices, dimension). Each row corresponds to a list of nodes on a simplex. If provided adj will be ignored but N will be required.</p> required <code>edge_par</code> <p>DataFrame with edge participation values formatted as the output of <code>edge_participation</code> i.e., indexed by the edges in adj and with columns de dimension for which edge participation is counted</p> <code>None</code> <code>N</code> <p>Number of nodes in original graph.</p> <code>None</code> <code>simple_type</code> <p>See simplex_counts</p> required <code>position</code> <p>Position of the edges to extract</p> <p>'all': all edges of the simplex</p> <p>'spine': edges along the spine of the simplex (only makes sense if simplices are directed)</p> <code>'all'</code> <code>return_mats</code> <code>bool</code> <p>If True return the matrices of the underlying graphs of the k-skeleta as in get_k_skeleta_graph</p> <code>False</code> <code>threads</code> <p>Number of threads into which to parallelize the computation</p> <code>8</code> <p>Returns:</p> Type Description <code>data frame, (dict)</code> <p>data frame with index dimensions and columns number of (rc) edges in the corresponding skeleta if return_mats==True, also return the graphs of the k-skeleta as in get_k_skeleta_graph.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If neither adj nor precomputed simplex_list or edge_par values are provided</p> <code>AssertionError</code> <p>If N &lt;= than an entry in the simplex list</p> See Also <p>edge_participation : A function that counts the number of times an edge is part of a simplex.</p> <p>simplex_counts : A function that counts the simplices forming the complex from which bettis are count, where simplex types are described in detail.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def count_rc_edges_skeleta(adj=None, max_dim=-1, max_simplices=False,\n                            N=None, simplex_list=None, edge_par=None,\n                           simplex_type='directed',position=\"all\",\n                           return_mats=False,threads=8, **kwargs):\n    \"\"\"Count the edges and reciprocal edges in the k-``skeleta`` of the directed flag complex of adj for all\n    k&lt;= max_dim. If simplex list are provided, it will compute the skeleta directly from these and not use adj.\n\n    Parameters\n    ----------\n    adj : (N,N)-array or sparse matrix\n        Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    max_simplices : bool\n        If False counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n    simplex list: series\n        Series 2d-arrays indexed by dimension.\n        Each array is of dimension (no. of simplices, dimension).\n        Each row corresponds to a list of nodes on a simplex.\n        If provided adj will be ignored but N will be required.\n    edge_par: DataFrame\n        DataFrame with edge participation values formatted as the output of ``edge_participation`` i.e.,\n        indexed by the edges in adj and with columns de dimension for which edge participation is counted\n    N: int\n        Number of nodes in original graph.\n    simple_type: str\n        See [simplex_counts](network_topology.md#src.connalysis.network.topology.simplex_counts)\n    position: str\n        Position of the edges to extract\n\n        'all': all edges of the simplex\n\n        'spine': edges along the spine of the simplex\n        (only makes sense if simplices are directed)\n    return_mats : bool\n        If True return the matrices of the underlying graphs of the k-skeleta as in\n        get_k_skeleta_graph\n    threads: int\n        Number of threads into which to parallelize the computation\n\n    Returns\n    -------\n    data frame, (dict)\n        data frame with index dimensions and columns number of (rc) edges in the corresponding skeleta\n        if return_mats==True, also return the graphs of the k-skeleta as in get_k_skeleta_graph.\n\n    Raises\n    ------\n    AssertionError\n        If neither adj nor precomputed simplex_list or edge_par values are provided\n    AssertionError\n        If N &lt;= than an entry in the simplex list\n\n    See Also\n    --------\n    [edge_participation](network_topology.md#src.connalysis.network.topology.edge_participation) :\n    A function that counts the number of times an edge is part of a simplex.\n\n    [simplex_counts](network_topology.md#src.connalysis.network.topology.simplex_counts) :\n    A function that counts the simplices forming the complex from which bettis are count,\n    where simplex types are described in detail.\n    \"\"\"\n\n    # Check if enough inputs are provided\n    if (position == 'spine') and (simplex_list is not None):\n        not_precomputed = False\n    elif (position == 'all') and (edge_par is not None):\n        not_precomputed = False\n    else:\n        not_precomputed = True\n\n    assert not (adj is None and not_precomputed), \"Either adj or simplex_list/edge_participation need to be provided\"\n\n    if not_precomputed:\n        N = adj.shape[0]\n        if position == \"spine\":  # Compute simplex since they are not provided\n            simplex_list = list_simplices_by_dimension(adj, node_properties=None,\n                                                       max_simplices=max_simplices, max_dim=max_dim,\n                                                       simplex_type='directed',\n                                                       nodes=None, verbose=False, **kwargs)\n        elif position == \"all\":  # More efficient than going from simplex lists if the position is not important\n            edge_par, simplex_counts = edge_participation(adj, node_properties=None, max_simplices=max_simplices,\n                                                          threads=threads, max_dim=max_dim, simplex_type=simplex_type,\n                                                          return_simplex_counts=True)\n\n    else:\n        if position == \"spine\":\n            assert N &gt; np.nanmax(simplex_list.explode().explode()), \\\n                \"N must be larger than all the entries in the simplex list\"\n\n    # Extract 'k'-skeleton and count (rc-)edges\n    if position == \"spine\":\n        dims = simplex_list.index[simplex_list.index != 0]  # Doesn't make sense to look at the 0-skeleton\n    elif position == \"all\":\n        dims = pd.Index(edge_par.drop(0, axis=1, errors=\"ignore\").columns, name=\"dim\")\n    edge_counts = pd.DataFrame(index=dims, columns=[\"number_of_edges\", \"number_of_rc_edges\", \"rc/edges_percent\"])\n    if return_mats == True:\n        skeleton_mats = {f'dimension_{dim}': None for dim in dims}\n\n    if position == \"spine\":\n        for dim in dims:\n            if simplex_list[dim].size &gt; 0:\n                assert N &gt; np.max(simplex_list[dim]), \\\n                    \"N must be larger than all the entries in the simplex list\"\n                mat = extract_submatrix_from_simplex_list(simplex_list[dim], N, position=position)\n                edges = mat.sum();\n                rc_edges = rc_submatrix(mat).sum()\n                edge_counts[\"number_of_edges\"].loc[dim] = edges\n                edge_counts[\"number_of_rc_edges\"].loc[dim] = rc_edges\n                edge_counts[\"rc/edges_percent\"].loc[dim] = (rc_edges * 100) / edges\n            else:\n                edge_counts[\"number_of_edges\"].loc[dim] = 0\n            if return_mats == True:\n                skeleton_mats[f'dimension_{dim}'] = mat\n    elif position == \"all\":\n        skeleton_mats = extract_submatrices_from_edge_participation(edge_par, N, dims=dims, thresh=1)\n        for dim in dims:\n            edges = skeleton_mats[f'dimension_{dim}'].sum()\n            edge_counts[\"number_of_edges\"].loc[dim] = edges\n            if edges &gt; 0:\n                rc_edges = rc_submatrix(skeleton_mats[f'dimension_{dim}']).sum()\n                edge_counts[\"number_of_rc_edges\"].loc[dim] = rc_edges\n                edge_counts[\"rc/edges_percent\"].loc[dim] = (rc_edges * 100) / edges\n            else:\n                edge_counts[\"number_of_rc_edges\"].loc[dim] = 0\n                edge_counts[\"rc/edges_percent\"].loc[dim] = 0\n\n    if return_mats == True:\n        return edge_counts, skeleton_mats\n    else:\n        return edge_counts\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.count_triads_fully_connected","title":"<code>count_triads_fully_connected(adj, max_num_sampled=5000000, return_normalized=False)</code>","text":"<p>Counts the numbers of each triadic motif in the matrix adj.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d-array</code> <p>Adjacency matrix of a directed network.</p> required <code>max_num_sampled</code> <code>int</code> <p>The maximal number of connected triads classified. If the number of connected triads is higher than that, only the specified number is sampled at random and classified. The final counts are extrapolated as (actual_num_triads/ max_num_sampled) * counts.</p> <code>5000000</code> <code>return_normalized</code> <code>bool</code> <p>If True return the triad counts divided by the size of each isomorphism class.  That is, the total counts divided by the following array:</p> <p>\\([6, 3, 3, 6, 6, 6, 2, 3, 6, 3, 3, 6, 1].\\)</p> <code>False</code> <p>Returns:</p> Type Description <code>1d-array</code> <p>The counts of the various triadic motifs in adj as ordered in Figure 5 [1]_.</p> Notes <p>Only connectected motifs are counted, i.e. motifs with less than 2 connections or only a single bidirectional connection are not counted. The connected motifs are ordered as in Figure 5 [1]_.</p> References <p>..[1] Gal, Eyal, et al. \"Rich cell-type-specific network topology in neocortical microcircuitry.\" Nature neuroscience 20.7 (2017): 1004-1013.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def count_triads_fully_connected(adj, max_num_sampled=5000000, return_normalized=False):\n    \"\"\"Counts the numbers of each triadic motif in the matrix adj.\n\n    Parameters\n    ----------\n    adj : 2d-array\n        Adjacency matrix of a directed network.\n    max_num_sampled : int\n        The maximal number of connected triads classified. If the number of\n        connected triads is higher than that, only the specified number is sampled at random and\n        classified. The final counts are extrapolated as (actual_num_triads/ max_num_sampled) * counts.\n    return_normalized : bool\n        If True return the triad counts divided by the size of each isomorphism class.  That is, the total counts\n        divided by the following array:\n\n        $[6, 3, 3, 6, 6, 6, 2, 3, 6, 3, 3, 6, 1].$\n\n    Returns\n    -------\n    1d-array\n        The counts of the various triadic motifs in adj as ordered in Figure 5 [1]_.\n\n    Notes\n    ------\n    Only connectected motifs are counted, i.e. motifs with less than 2 connections or only a single bidirectional\n    connection are not counted. The connected motifs are ordered as in Figure 5 [1]_.\n\n    References\n    -------\n\n    ..[1] Gal, Eyal, et al.\n    [\"Rich cell-type-specific network topology in neocortical microcircuitry.\"](https://www.nature.com/articles/nn.4576)\n    Nature neuroscience 20.7 (2017): 1004-1013.\n\n    \"\"\"\n\n    # Functions to indetify triads\n    def canonical_sort(M):\n        \"\"\"Sorts row/columns of the matrix adj using the lexicographical order of the\n        tuple (out_degree, in_degree).\n\n        Parameters\n        ----------\n        M : 2d-array\n            Adjacency matrix of a directed network.\n\n        Returns\n        -------\n        2d-array\n            the matrix adj with rows/columns sorted\n        \"\"\"\n        in_degree = np.sum(M, axis=0)\n        out_degree = np.sum(M, axis=1)\n        idx = np.argsort(-in_degree - 10 * out_degree)\n        return M[:, idx][idx]\n\n    def identify_motif(M):\n        \"\"\"\n        Identifies the connected directed digraph on three nodes M as on in the full classification\n        list given in the dictionary triad_dict.\n\n        Parameters\n        ----------\n        M : array\n            A (3,3) array describing a directed connected digraph on three nodes.\n\n        Returns\n        -------\n        The index of the motif as indexed in the dictiroanry triad_dict which follows the\n        ordering of Gal et al., 2017\n        \"\"\"\n        triad_code = tuple(np.nonzero(canonical_sort(M).flatten())[0])\n        return triad_dict[triad_code]\n\n    # Finding and counting triads\n    import time\n    adj = adj.toarray()  # Casting to array makes finding triads an order of magnitude faster\n    t0 = time.time()\n    undirected_adj = underlying_undirected_matrix(adj).toarray()\n    # Matrix with i,j entries number of undirected paths between i and j in adj\n    path_counts = np.triu(undirected_adj @ undirected_adj, 1)\n    connected_pairs = np.nonzero(path_counts)\n    triads = set()\n    print(\"Testing {0} potential triadic pairs\".format(len(connected_pairs[0])))\n    for x, y in zip(*connected_pairs):\n        # zs = np.nonzero((undirected_adj.getrow(x).multiply(undirected_adj.getrow(y))).toarray()[0])[0]\n        zs = np.nonzero(undirected_adj[x] &amp; undirected_adj[y])[0]\n        for z in zs:\n            triads.add(tuple(sorted([x, y, z])))\n    triads = list(triads)\n    print(\"Time spent finding triads: {0}\".format(time.time() - t0))\n    print(\"Found {0} connected triads\".format(len(triads)))\n    t0 = time.time()\n    counts = np.zeros(np.max(list(triad_dict.values())) + 1)\n    sample_idx = np.random.choice(len(triads),\n                                  np.minimum(max_num_sampled, len(triads)),\n                                  replace=False)\n    for idx in sample_idx:\n        triad = triads[idx]\n        motif_id = identify_motif(adj[:, triad][triad, :])\n        counts[motif_id] += 1\n    print(\"Time spent classifying triads: {0}\".format(time.time() - t0))\n    if return_normalized:\n        return (((len(triads) / len(sample_idx)) * counts).astype(int)) / triad_combinations\n    else:\n        return (((len(triads) / len(sample_idx)) * counts).astype(int))\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.cross_col_k_in_degree","title":"<code>cross_col_k_in_degree(adj_cross, adj_source, max_simplices=False, threads=8, max_dim=-1, **kwargs)</code>","text":"<p>Compute generalized in-degree of nodes in adj_target from nodes in adj_source. The k-in-degree of a node v is the number of k-simplices in adj_source with all its nodes mapping to v through edges in adj_cross.</p> <p>Parameters:</p> Name Type Description Default <code>adj_cross</code> <code>(n,m) array or sparse matrix</code> <p>Matrix of connections from the nodes in adj_n to the target population. n is the number of nodes in adj_source and m is the number of nodes in adj_target. A non-zero entry adj_cross[i,j] implies there is an edge from i-th node of adj_source to the j-th node of adj_target.</p> required <code>adj_source</code> <code>(n, n)-array or sparse matrix</code> <p>Adjacency matrix of the source network where n is the number of nodes in the source network. A non-zero entry adj_source[i,j] implies there is an edge from node i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices. If True counts only maximal simplices.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions. Particularly useful for large or dense graphs.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Data frame</code> <p>Table of cross-k-in-degrees indexed by the m nodes in the target population.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj_source has non-zero entries in the diagonal which can produce errors.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def cross_col_k_in_degree(adj_cross, adj_source, max_simplices=False,\n                          threads=8,max_dim=-1,**kwargs):\n    #TODO: DO THE OUTDEGREE VERSION\n    #TODO: Get participation directly from flagsercount via vertices to do?\n    \"\"\"Compute generalized in-degree of nodes in adj_target from nodes in adj_source.\n    The k-in-degree of a node v is the number of k-simplices in adj_source with all its nodes mapping to v\n    through edges in adj_cross.\n    Parameters\n    ----------\n    adj_cross : (n,m) array or sparse matrix\n        Matrix of connections from the nodes in adj_n to the target population.\n        n is the number of nodes in adj_source and m is the number of nodes in adj_target.\n        A non-zero entry adj_cross[i,j] implies there is an edge from i-th node of adj_source\n        to the j-th node of adj_target.\n    adj_source : (n, n)-array or sparse matrix\n        Adjacency matrix of the source network where n is the number of nodes in the source network.\n        A non-zero entry adj_source[i,j] implies there is an edge from node i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    max_simplices : bool\n        If False counts all simplices.\n        If True counts only maximal simplices.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.\n        Particularly useful for large or dense graphs.\n\n    Returns\n    -------\n    Data frame\n        Table of cross-k-in-degrees indexed by the m nodes in the target population.\n\n    Raises\n    ------\n    AssertionError\n        If adj_source has non-zero entries in the diagonal which can produce errors.\n    \"\"\"\n    adj_source=sp.csr_matrix(adj_source).astype('bool')\n    adj_cross=sp.csr_matrix(adj_cross).astype('bool')\n    assert np.count_nonzero(adj_source.diagonal()) == 0, \\\n    'The diagonal of the source matrix is non-zero and this may lead to errors!'\n    assert adj_source.shape[0] == adj_source.shape[1], \\\n    'Dimension mismatch. The source matrix must be square.'\n    assert adj_source.shape[0] == adj_cross.shape[0], \\\n    'Dimension mismatch. The source matrix and cross matrix must have the same number of rows.'\n\n    n_source = adj_source.shape[0] #Size of the source population\n    n_target = adj_cross.shape[1] #Size of the target population\n    # Building a square matrix [[adj_source, adj_cross], [0,0]]\n    adj=sp.bmat([[adj_source, adj_cross],\n                 [sp.csr_matrix((n_target, n_source), dtype='bool'),\n                  sp.csr_matrix((n_target, n_target), dtype='bool')]])\n    # Transposing to restrict computation to ``source nodes'' in adj_target in flagsercount\n    adj=adj.T\n    nodes=np.arange(n_source, n_source+n_target) #nodes on target population\n    slist=list_simplices_by_dimension(adj, max_simplices=max_simplices, max_dim=max_dim,nodes=nodes,\n                                      simplex_type='directed',verbose=False,threads=threads,**kwargs)\n\n    #Count participation as a source in transposed matrix i.e. participation as sink in the original\n    cross_col_deg=pd.DataFrame(columns=slist.index[1:], index=nodes)\n    for dim in slist.index[1:]:\n        index,deg=np.unique(slist[dim][:,0],return_counts=True)\n        cross_col_deg[dim].loc[index]=deg\n    cross_col_deg=cross_col_deg.fillna(0)\n    return cross_col_deg\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.dc","title":"<code>dc(matrix, center=0, coeff_index=2, nhbd=True)</code>","text":"<p>Computes the density coefficient of the graph induced by the neighbourhood of center <code>center</code> in the matrix</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>2d-array</code> <p>Adjacency matrix of a directed network.</p> required <code>center</code> <code>int</code> <p>The index of the vertex whose neighbourhood is to be considered, default=0</p> <code>0</code> <code>coeff_index</code> <code>int</code> <p>The dimension to be computed, default=2</p> <code>2</code> <code>nhbd</code> <code>bool</code> <p>If true then is assumed that matrix is the neighbourhood of center.  If false the neighbourhood of center is computed and used, default= True.</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>The density coefficient of the neighbourhood of center</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def dc(matrix, center=0, coeff_index=2, nhbd=True):\n    \"\"\"Computes the density coefficient of the graph induced by\n        the neighbourhood of center ``center`` in the matrix\n\n        Parameters\n        ----------\n\n        matrix : 2d-array\n            Adjacency matrix of a directed network.\n        center : int\n            The index of the vertex whose neighbourhood is to be considered, default=0\n        coeff_index : int\n             The dimension to be computed, default=2\n        nhbd : bool\n             If true then is assumed that matrix is the neighbourhood of center.\n             If false the neighbourhood of center is computed and used, default= True.\n\n        Returns\n        -------\n        float\n            The density coefficient of the neighbourhood of center\n\n    \"\"\"\n    assert coeff_index &gt;= 2, 'Assertion error: Density coefficient must be at least 2'\n    if not nhbd:\n        matrix = neighbourhood(center, matrix)\n        center = 0\n\n    flagser_output = node_participation(matrix,max_dim=coeff_index).iloc[center]\n    if len(flagser_output) &lt;= coeff_index:\n        density_coeff = 0\n    elif flagser_output[coeff_index] == 0:\n        density_coeff = 0\n    else:\n        numerator = coeff_index*flagser_output[coeff_index]\n        denominator = (coeff_index+1)*(len(matrix)-coeff_index)*flagser_output[coeff_index-1]\n        if denominator == 0:\n            density_coeff = 0\n        else:\n            density_coeff = numerator/denominator\n    return density_coeff\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.edge_participation","title":"<code>edge_participation(adj, node_properties=None, max_simplices=False, threads=8, max_dim=-1, simplex_type='directed', return_simplex_counts=False, verbose=False, **kwargs)</code>","text":"<p>Compute the number of simplex motifs in the network adj each edge is part of. See simplex_counts for details.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>node_properties</code> <code>dataframe</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False (default) counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <code>simplex_type</code> <code>string</code> <p>Type of simplex to consider:</p> <p>\u2019directed\u2019 - directed simplices</p> <p>\u2019undirected\u2019 - simplices in the underlying undirected graph</p> <p>\u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections</p> <code>'directed'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>Indexed by the edges in adj and with columns de dimension for which node participation is counted</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj has non-zero entries in the diagonal which can produce errors.</p> <code>AssertionError</code> <p>If adj is not square.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def edge_participation(adj, node_properties=None, max_simplices=False,\n                       threads=8,max_dim=-1,simplex_type='directed', return_simplex_counts=False, verbose=False, **kwargs):\n    \"\"\"Compute the number of simplex motifs in the network adj each edge is part of.\n    See simplex_counts for details.\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    node_properties : dataframe\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    max_simplices : bool\n        If False (default) counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n    simplex_type : string\n        Type of simplex to consider:\n\n        \u2019directed\u2019 - directed simplices\n\n        \u2019undirected\u2019 - simplices in the underlying undirected graph\n\n        \u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections\n\n    Returns\n    -------\n    data frame\n        Indexed by the edges in adj and with columns de dimension for which node participation is counted\n\n    Raises\n    -------\n    AssertionError\n        If adj has non-zero entries in the diagonal which can produce errors.\n    AssertionError\n        If adj is not square.\n    \"\"\"\n\n    adj=sp.csr_matrix(adj).astype('bool')\n    assert np.count_nonzero(adj.diagonal()) == 0, 'The diagonal of the matrix is non-zero and this may lead to errors!'\n    N, M = adj.shape\n    assert N == M, 'Dimension mismatch. The matrix must be square.'\n\n\n    #Symmetrize matrix if simplex_type is not 'directed'\n    if simplex_type=='undirected':\n        adj=sp.triu(underlying_undirected_matrix(adj)) #symmtrize and keep upper triangular only\n    elif simplex_type==\"reciprocal\":\n        adj=sp.triu(rc_submatrix(adj)) #symmtrize and keep upper triangular only\n\n    flagser_out = pyflagsercount.flagser_count(adj, edge_containment=True, threads=threads,\n                                     max_simplices=max_simplices, max_dim=max_dim)\n    logging.info(\"Done running flagser\")\n    e_contain = pd.DataFrame.from_dict(flagser_out['edge_contain_counts'], orient=\"index\").fillna(0).astype(int)\n    if return_simplex_counts:\n        return e_contain, flagser_out[\"cell_counts\"]\n    else:\n        return e_contain\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.euler_characteristic","title":"<code>euler_characteristic(matrix)</code>","text":"<p>Computes the Euler characteristic of the flag complex of the graph with adjacency matrix matrix</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>2d-array</code> <p>Adjacency matrix of a directed network.</p> required <p>Returns:</p> Type Description <code>integer</code> <p>The Euler characteristic of the flag complex of matrix</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def euler_characteristic(matrix):\n    \"\"\"Computes the Euler characteristic of the flag complex of the graph with adjacency matrix matrix\n\n        Parameters\n        ----------\n        matrix : 2d-array\n            Adjacency matrix of a directed network.\n\n        Returns\n        -------\n        integer\n            The Euler characteristic of the flag complex of matrix\n\n    \"\"\"\n    return pyflagsercount.flagser_count(M)['euler']\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.extract_submatrices_from_edge_participation","title":"<code>extract_submatrices_from_edge_participation(edge_par, N, dims=None, thresh=1)</code>","text":"<p>Generate binary submatrix of an NxN matrix of edges with edge participation greater than thresh.</p> <p>Parameters:</p> Name Type Description Default <code>edge_par</code> <p>DataFrame with edge participation values formatted as the output of <code>edge_participation</code> i.e., indexed by the edges in adj and with columns de dimension for which edge participation is counted</p> required <code>dims</code> <p>dimensions of the simplices to consider, if <code>None</code> all positive dimensions are considered</p> <code>None</code> <code>N</code> <p>Number of nodes in original graph defining the NxN matrix. The indices of edge_par must be smaller than N</p> required <code>thresh</code> <p>Threshold value at which to consider an edge.  If thresh=1 all edges that elong to at least one simplex are considered.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict</code> <p>keys: dimensions values: sparse binary matrices in coo format of shape (N,N) with entries <code>True</code> corresponding to edges that belong to at least thresh simplices of the dimension in its corresponding key.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def extract_submatrices_from_edge_participation(edge_par, N, dims=None, thresh=1):\n    \"\"\"Generate binary submatrix of an NxN matrix of edges with edge participation greater than thresh.\n\n    Parameters\n    ----------\n    edge_par: DataFrame\n        DataFrame with edge participation values formatted as the output of ``edge_participation`` i.e.,\n        indexed by the edges in adj and with columns de dimension for which edge participation is counted\n    dims: list of ints\n        dimensions of the simplices to consider, if ``None`` all positive dimensions are considered\n    N: int\n        Number of nodes in original graph defining the NxN matrix.\n        The indices of edge_par must be smaller than N\n    thresh: int\n        Threshold value at which to consider an edge.  If thresh=1 all edges that\n        elong to at least one simplex are considered.\n\n    Returns\n    -------\n    dict\n        keys: dimensions\n        values: sparse binary matrices in coo format of shape (N,N) with entries `True` corresponding to edges\n        that belong to at least thresh simplices of the dimension in its corresponding key.\n    \"\"\"\n\n    if dims is None: dims = edge_par.columns\n    dims = np.array(dims)\n    assert np.isin(dims,\n                   edge_par.columns).all(), \"The array dims shoulds be a subset of the columns of edge participation\"\n    mats = {}\n    # Reformat edge_participation (Maybe already do this in the output of edge_participation?)\n    df = edge_par.set_index(keys=pd.MultiIndex.from_tuples(edge_par.index)).reset_index(names=[\"row\", \"col\"])\n    for dim in dims:\n        edges = df[df[dim] &gt;= thresh]\n        mats[f'dimension_{dim}'] = sp.coo_matrix((np.ones(len(edges)), (edges.row, edges.col)), shape=(N, N)).astype(\n            bool)\n    return mats\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.extract_submatrix_from_simplex_list","title":"<code>extract_submatrix_from_simplex_list(simplex_list, N, position='spine')</code>","text":"<p>Generate binary submatrix of NxN matrix of edges in simplex list.</p> <p>Parameters:</p> Name Type Description Default <code>simplex</code> <p>Array of dimension (no. of simplices, dimension). Each row corresponds to a list of nodes on a simplex indexed by the order of the nodes in an NxN matrix.</p> required <code>N</code> <p>Number of nodes in original graph defining the NxN matrix.</p> required <code>position</code> <p>Position of the edges to extract</p> <p>'all': all edges of the simplex (it is more efficient to do this with <code>extract_submatrix_from_edge_par</code>)</p> <p>'spine': edges along the spine of the simplex (only makes sense for directed simplices)</p> <code>'spine'</code> <p>Returns:</p> Type Description <code>coo bool matrix</code> <p>Matrix with of shape (N,N) with entries <code>True</code> corresponding to edges in simplices.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def extract_submatrix_from_simplex_list(simplex_list, N, position=\"spine\"):\n    \"\"\"Generate binary submatrix of NxN matrix of edges in simplex list.\n\n    Parameters\n    ----------\n    simplex list: 2d-array\n        Array of dimension (no. of simplices, dimension).\n        Each row corresponds to a list of nodes on a simplex\n        indexed by the order of the nodes in an NxN matrix.\n    N: int\n        Number of nodes in original graph defining the NxN matrix.\n    position: str\n        Position of the edges to extract\n\n        'all': all edges of the simplex (it is more efficient to do this with ``extract_submatrix_from_edge_par``)\n\n        'spine': edges along the spine of the simplex\n        (only makes sense for directed simplices)\n\n    Returns\n    -------\n    coo bool matrix\n        Matrix with of shape (N,N) with entries `True` corresponding to edges in simplices.\n    \"\"\"\n    if position==\"all\":\n        logging.warning(\"For edges in any position it is more efficient to use extract_submatrices_from_edge_participation\")\n    if simplex_list.shape[0] == 0:\n        return sp.csr_matrix((N, N), dtype=bool)  # no simplices in this dimension\n    else:\n        dim = simplex_list.shape[1] - 1\n        edges_abstract = _generate_abstract_edges_in_simplices(dim,\n                                                               position=position)  # abstract list of edges to extract from each simplex\n        edges = np.unique(np.concatenate([simplex_list[:, edge] for edge in edges_abstract]), axis=0)\n        return (sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(N, N))).astype(bool)\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.get_all_simplices_from_max","title":"<code>get_all_simplices_from_max(max_simplices)</code>","text":"<p>Takes the list of maximal simplices are returns the list of all simplices.</p> <p>Parameters:</p> Name Type Description Default <code>max_simplices</code> <code>list</code> <p>A list of lists of tuples. Where max_simplices[k] is a list of the 0 dimensional maximal simplices, where each simplex is a tuple of the vertices of the simplex</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of lists of tuples. Of the same format as the inputted list but now contains all simplices.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def get_all_simplices_from_max(max_simplices):\n    \"\"\"Takes the list of maximal simplices are returns the list of all simplices.\n\n        Parameters\n        ----------\n        max_simplices : list\n            A list of lists of tuples. Where max_simplices[k] is a list of the 0 dimensional maximal simplices,\n            where each simplex is a tuple of the vertices of the simplex\n\n        Returns\n        -------\n        list\n            A list of lists of tuples. Of the same format as the inputted list but now contains all simplices.\n        \"\"\"\n    simplices = list(max_simplices)\n    for k in range(len(max_simplices)-1,0,-1):\n        print(max_simplices[k])\n        for simplex in simplices[k]:\n            for s in range(k,-1,-1):\n                x = tuple(simplex[:s]+simplex[s+1:])\n                if x not in simplices[k-1]:\n                    simplices[k-1].append(x)\n\n    return simplices\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.get_k_skeleta_graph","title":"<code>get_k_skeleta_graph(adj=None, dimensions=None, max_simplices=False, N=None, simplex_list=None, edge_par=None, simplex_type='directed', position='all', threads=8, **kwargs)</code>","text":"<p>Return the edges of the (maximal) k-skeleton of the flag complex of adj for all k&lt;= max_dim in the position determined by position. If simplex list are provided, it will compute the edges directly from these and not use adj, in which case N (the number of rows and columns of adj) is required. If simplex lists are not provided they will be calculated with for the flag complex whose type is determined by simplex_type as for simplex_counts.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>(N,N)-array or sparse matrix</code> <p>Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>dimensions</code> <code>list of ints</code> <p>Dimensions <code>k</code> for which the <code>k</code>-skeleta is computed, if None all dimensions are computed.</p> <code>None</code> <code>simplex_type</code> <code>string</code> <p>Type of simplex to consider if computed from adj:</p> <p>\u2019directed\u2019 - directed simplices</p> <p>\u2019undirected\u2019 - simplices in the underlying undirected graph</p> <p>\u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections</p> <code>'directed'</code> <code>simplex</code> <p>Series 2d-arrays indexed by dimension. Each array is of dimension (no. of simplices, dimension). Each row corresponds to a list of nodes on a simplex. If provided adj will be ignored but N will be required.</p> required <code>N</code> <p>Number of nodes in original graph.</p> <code>None</code> <code>position</code> <p>Position of the edges to extract</p> <p>'all': all edges of the simplex</p> <p>'spine': edges along the spine of the simplex (only makes sense if simplices are directed)</p> <code>'all'</code> <code>threads</code> <p>Number of threads into which the computation should be parallelized</p> <code>8</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with keys dimensions and values boolean (N,N) matrices with entries <code>True</code> corresponding to edges in (maximal) simplices of that dimension.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If neither adj nor simplex_list are provided</p> <code>AssertionError</code> <p>If N &lt;= than an entry in the simplex list</p> <code>AssertionError</code> <p>If a dimension is required that is not an index in the simplex list</p> Notes <p>In order to list k-simplices and thus the k-skeleton, flagsercount needs to list all lower dimensional simplices anyhow.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def get_k_skeleta_graph(adj=None, dimensions=None, max_simplices=False,\n                        N=None, simplex_list=None, edge_par=None,\n                        simplex_type='directed', position=\"all\",\n                        threads=8, **kwargs):\n    \"\"\"Return the edges of the (maximal) k-skeleton of the flag complex of adj for all k&lt;= max_dim in the position determined\n    by position.\n    If simplex list are provided, it will compute the edges directly from these and not use adj,\n    in which case N (the number of rows and columns of adj) is required.\n    If simplex lists are not provided they will be calculated with for the flag complex whose type is determined by\n    simplex_type as for simplex_counts.\n\n    Parameters\n    ----------\n    adj : (N,N)-array or sparse matrix\n        Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    max_simplices : bool\n        If False counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    dimensions : list of ints\n        Dimensions `k` for which the `k`-skeleta is computed, if None all dimensions are computed.\n    simplex_type : string\n        Type of simplex to consider if computed from adj:\n\n        \u2019directed\u2019 - directed simplices\n\n        \u2019undirected\u2019 - simplices in the underlying undirected graph\n\n        \u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections\n    simplex list: series\n        Series 2d-arrays indexed by dimension.\n        Each array is of dimension (no. of simplices, dimension).\n        Each row corresponds to a list of nodes on a simplex.\n        If provided adj will be ignored but N will be required.\n    N: int\n        Number of nodes in original graph.\n    position: str\n        Position of the edges to extract\n\n        'all': all edges of the simplex\n\n        'spine': edges along the spine of the simplex\n        (only makes sense if simplices are directed)\n    threads: int\n        Number of threads into which the computation should be parallelized\n\n    Returns\n    -------\n    dict\n        Dictionary with keys dimensions and values boolean (N,N) matrices with entries `True`\n        corresponding to edges in (maximal) simplices of that dimension.\n\n    Raises\n    ------\n    AssertionError\n        If neither adj nor simplex_list are provided\n    AssertionError\n        If N &lt;= than an entry in the simplex list\n    AssertionError\n        If a dimension is required that is not an index in the simplex list\n\n    Notes\n    ------\n    In order to list k-simplices and thus the k-skeleton, flagsercount needs to list all lower\n    dimensional simplices anyhow.\n\n    \"\"\"\n\n    # Check if enough inputs are provided\n    if (position == 'spine') and (simplex_list is not None):\n        not_precomputed = False\n    elif (position == 'all') and (edge_par is not None):\n        not_precomputed = False\n    else:\n        not_precomputed = True\n    assert not (adj is None and not_precomputed), \"Either adj or simplex_list/edge_participation need to be provided\"\n\n    # Determine dimensions\n    if dimensions == None:\n        max_dim = -1\n    else:\n        max_dim = np.max(np.array(dimensions))\n    # Compute simplex list or edge particiption if not precomputed\n    if not_precomputed:\n        N = adj.shape[0]\n        if position == \"spine\":  # Compute simplex since they are not provided\n            simplex_list = list_simplices_by_dimension(adj, node_properties=None,\n                                                       max_simplices=max_simplices, max_dim=max_dim,\n                                                       simplex_type='directed',\n                                                       nodes=None, verbose=False, **kwargs)\n        elif position == \"all\":  # More efficient than going from simplex lists if the position is not important\n            edge_par, simplex_counts = edge_participation(adj, node_properties=None, max_simplices=max_simplices,\n                                                          threads=threads, max_dim=max_dim, simplex_type=simplex_type,\n                                                          return_simplex_counts=True)\n    else:\n        if position == \"spine\":\n            assert N &gt; np.nanmax(simplex_list.explode().explode()), \\\n                \"N must be larger than all the entries in the simplex list\"\n    # Extract 'k'-skeleton\n    if position == \"spine\":\n        dims = simplex_list.index[simplex_list.index != 0]  # Doesn't make sense to look at the 0-skeleton\n    elif position == \"all\":\n        dims = pd.Index(edge_par.drop(0, axis=1, errors=\"ignore\").columns, name=\"dim\")\n    if dimensions != None:\n        dims = dims[np.isin(dims, dimensions)]\n    skeleton_mats = {f'dimension_{dim}': None for dim in dims}\n    if position == \"spine\":\n        for dim in dims:\n            if simplex_list[dim].size &gt; 0:\n                assert N &gt; np.max(simplex_list[dim]), \\\n                    \"N must be larger than all the entries in the simplex list\"\n                skeleton_mats[f'dimension_{dim}'] = extract_submatrix_from_simplex_list(simplex_list[dim], N,\n                                                                                        position=position)\n    elif position == \"all\":\n        skeleton_mats = extract_submatrices_from_edge_participation(edge_par, N, dims=dims, thresh=1)\n    return skeleton_mats\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.in_degree_from_pop","title":"<code>in_degree_from_pop(adj, source_pop, max_simplices=False, threads=8, max_dim=-1, **kwargs)</code>","text":"<p>Compute generalized in-degree of nodes source_pop onto the rest of the nodes in adj.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <p>Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>source_pop</code> required <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices. If True counts only maximal simplices.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions. Particularly useful for large or dense graphs.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Data frame</code> <p>Table of k-in-degrees from source_pop indexed by the target population.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj restricted to source_pop has non-zero entries in the diagonal which can produce errors.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def in_degree_from_pop(adj, source_pop, max_simplices=False,threads=8, max_dim=-1, ** kwargs):\n    # TODO: DO THE OUTDEGREE VERSION\n    # TODO: Get participation directly from flagsercount via vertices to do?\n    \"\"\"Compute generalized in-degree of nodes source_pop onto the rest of the nodes in adj.\n    Parameters\n    ----------\n    adj: 2d (N,N)-array or sparse matrix\n        Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    source_pop: list of indices of the source population, must be a subset of ``np.arange(0, adj.shape[0])``\n    max_simplices : bool\n        If False counts all simplices.\n        If True counts only maximal simplices.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.\n        Particularly useful for large or dense graphs.\n\n    Returns\n    -------\n    Data frame\n        Table of k-in-degrees from source_pop indexed by the target population.\n\n    Raises\n    ------\n    AssertionError\n        If adj restricted to source_pop has non-zero entries in the diagonal which can produce errors.\n    \"\"\"\n    adj=adj.tocsr()\n    source_pop = np.sort(source_pop)\n    target_pop = np.setdiff1d(np.arange(adj.shape[0]), source_pop)\n    adj_source = adj[np.ix_(source_pop, source_pop)]\n    adj_cross = adj[np.ix_(source_pop, target_pop)]\n    degs=cross_col_k_in_degree(adj_cross, adj_source,\n                                 max_simplices=max_simplices,threads=threads, max_dim=max_dim, **kwargs)\n    degs.index=target_pop\n    return degs\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.list_simplices_by_dimension","title":"<code>list_simplices_by_dimension(adj, node_properties=None, max_simplices=False, max_dim=-1, nodes=None, verbose=False, simplex_type='directed', **kwargs)</code>","text":"<p>List all simplex motifs in the network adj.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d (N,N)-array or sparse matrix</code> <p>Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>node_properties</code> <code> data frame</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <code>simplex_type</code> <code>string</code> <p>Type of simplex to consider:</p> <p>\u2019directed\u2019 - directed simplices</p> <p>\u2019undirected\u2019 - simplices in the underlying undirected graph</p> <p>\u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections</p> <code>'directed'</code> <code>nodes</code> <code>1d array or None(default)</code> <p>Restrict to list only the simplices whose source node is in nodes.  If None list all simplices</p> <code>None</code> <p>Returns:</p> Type Description <code>series</code> <p>Simplex lists indexed per dimension.  The dimension k entry is a (no. of k-simplices, k+1)-array is given, where each row denotes a simplex.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj has non-zero entries in the diagonal which can produce errors.</p> <code>AssertionError</code> <p>If adj is not square.</p> <code>AssertionError</code> <p>If nodes is not a subarray of np.arange(N)</p> See Also <p>simplex_counts : A function that counts the simplices instead of listing them and has descriptions of the simplex types.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def list_simplices_by_dimension(adj, node_properties=None, max_simplices=False,max_dim=-1,nodes=None,\n                                verbose=False, simplex_type='directed', **kwargs):\n    \"\"\"List all simplex motifs in the network adj.\n    Parameters\n    ----------\n    adj : 2d (N,N)-array or sparse matrix\n        Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    node_properties :  data frame\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    max_simplices : bool\n        If False counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n    simplex_type : string\n        Type of simplex to consider:\n\n        \u2019directed\u2019 - directed simplices\n\n        \u2019undirected\u2019 - simplices in the underlying undirected graph\n\n        \u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections\n    nodes : 1d array or None(default)\n        Restrict to list only the simplices whose source node is in nodes.  If None list all simplices\n\n    Returns\n    -------\n    series\n        Simplex lists indexed per dimension.  The dimension k entry is a (no. of k-simplices, k+1)-array\n        is given, where each row denotes a simplex.\n\n    Raises\n    ------\n    AssertionError\n        If adj has non-zero entries in the diagonal which can produce errors.\n    AssertionError\n        If adj is not square.\n    AssertionError\n        If nodes is not a subarray of np.arange(N)\n\n    See Also\n    --------\n    simplex_counts : A function that counts the simplices instead of listing them and has descriptions of the\n    simplex types.\n    \"\"\"\n    LOG.info(\"COMPUTE list of %ssimplices by dimension\", \"max-\" if max_simplices else \"\")\n\n    import pyflagsercount\n\n    adj=sp.csr_matrix(adj)\n    assert np.count_nonzero(adj.diagonal()) == 0, 'The diagonal of the matrix is non-zero and this may lead to errors!'\n    N, M = adj.shape\n    assert N == M, 'Dimension mismatch. The matrix must be square.'\n    if not nodes is None:\n        assert np.isin(nodes,np.arange(N)).all(), \"nodes must be a subarray of the nodes of the matrix\"\n\n    #Symmetrize matrix if simplex_type is not 'directed'\n    if simplex_type=='undirected':\n        adj=sp.triu(underlying_undirected_matrix(adj)) #symmtrize and keep upper triangular only\n    elif simplex_type==\"reciprocal\":\n        adj=sp.triu(rc_submatrix(adj)) #symmtrize and keep upper triangular only\n\n    n_threads = kwargs.get(\"threads\", kwargs.get(\"n_threads\", 1))\n\n\n    # Only the simplices that have sources stored in this temporary file will be considered\n    if not nodes is None:\n        import tempfile\n        import os\n        tmp_file = tempfile.NamedTemporaryFile(delete=False)\n        vertices_todo = tmp_file.name + \".npy\"\n        np.save(vertices_todo, nodes, allow_pickle=False)\n    else:\n        vertices_todo=''\n\n    #Generate simplex_list\n    original=pyflagsercount.flagser_count(adj, max_simplices=max_simplices,threads=n_threads,max_dim=max_dim,\n                                      vertices_todo=vertices_todo, return_simplices=True)['simplices']\n\n    #Remove temporary file\n    if not nodes is None:\n        os.remove(vertices_todo)\n\n    #Format output\n    max_dim = len(original)\n    dims = pd.Index(np.arange(max_dim), name=\"dim\")\n    simplices = pd.Series(original, name=\"simplices\", index=dims).apply(np.array)\n    #When counting all simplices flagser doesn't list dim 0 and 1 because they correspond to vertices and edges\n    if not max_simplices:\n        if nodes is None:\n            nodes=np.arange(0, N)\n        coom = adj.tocoo()\n        simplices[0] = np.reshape(nodes, (nodes.size, 1))\n        mask=np.isin(coom.row,nodes)\n        simplices[1] = np.stack([coom.row[mask], coom.col[mask]]).T\n    return simplices\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.nbc","title":"<code>nbc(matrix)</code>","text":"<p>Computes the normalised Betti coefficient of the graph with adjacency matrix matrix</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>2d-array</code> <p>Adjacency matrix of a directed network.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The normalised Betti coefficient of the graph with adjacency matrix matrix</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def nbc(matrix):\n    \"\"\"Computes the normalised Betti coefficient of the graph with adjacency matrix matrix\n\n        Parameters\n        ----------\n        matrix : 2d-array\n            Adjacency matrix of a directed network.\n\n        Returns\n        -------\n        float\n            The normalised Betti coefficient of the graph with adjacency matrix matrix\n\n    \"\"\"\n    flagser_output = pyflagser.flagser_unweighted(matrix, directed=True)\n    cells = flagser_output['cell_count']\n    bettis = flagser_output['betti']\n    while (cells[-1] == 0) and (len(cells) &gt; 1):\n        cells = cells[:-1]\n    while (bettis[-1] == 0) and (len(bettis) &gt; 1):\n        bettis = bettis[:-1]\n    normalized_betti_list = [(i+1)*bettis[i]/cells[i] for i in range(min(len(bettis),len(cells)))]\n    return sum(normalized_betti_list)\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.node_degree","title":"<code>node_degree(adj, node_properties=None, direction=None, weighted=False, **kwargs)</code>","text":"<p>Compute degree of nodes in network adj</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j of weight adj[i,j].</p> required <code>node_properties</code> <code>data frame</code> <p>Data frame of neuron properties in adj. Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>direction</code> <code>string or tuple of strings</code> <p>Direction for which to compute the degree</p> <p>'IN' - In degree</p> <p>'OUT'- Out degree</p> <p>None or ('IN', 'OUT') - Total degree i.e. IN+OUT</p> <code>None</code> <p>Returns:</p> Type Description <code>series or data frame</code> <p>Raises:</p> Type Description <code>Warning</code> <p>If adj has non-zero entries in the diagonal</p> <code>AssertionError</code> <p>If direction is invalid</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def node_degree(adj, node_properties=None, direction=None, weighted=False, **kwargs):\n    \"\"\"Compute degree of nodes in network adj\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j\n        of weight adj[i,j].\n    node_properties : data frame\n        Data frame of neuron properties in adj. Only necessary if used in conjunction with TAP or connectome utilities.\n    direction : string or tuple of strings\n        Direction for which to compute the degree\n\n        'IN' - In degree\n\n        'OUT'- Out degree\n\n        None or ('IN', 'OUT') - Total degree i.e. IN+OUT\n\n    Returns\n    -------\n    series or data frame\n\n    Raises\n    ------\n    Warning\n        If adj has non-zero entries in the diagonal\n    AssertionError\n        If direction is invalid\n    \"\"\"\n    assert not direction or direction in (\"IN\", \"OUT\") or tuple(direction) == (\"IN\", \"OUT\"),\\\n        f\"Invalid `direction`: {direction}\"\n\n    if not isinstance(adj, np. ndarray):\n        matrix = adj.toarray()\n    else:\n        matrix=adj.copy()\n    if not weighted:\n        matrix=matrix.astype('bool')\n    if np.count_nonzero(np.diag(matrix)) != 0:\n        logging.warning('The diagonal is non-zero!  This may cause errors in the analysis')\n    index = pd.Series(range(matrix.shape[0]), name=\"node\")\n    series = lambda array: pd.Series(array, index)\n    in_degree = lambda: series(matrix.sum(axis=0))\n    out_degree = lambda: series(matrix.sum(axis=1))\n\n    if not direction:\n        return in_degree() + out_degree()\n\n    if tuple(direction) == (\"IN\", \"OUT\"):\n        return pd.DataFrame({\"IN\": in_degree(), \"OUT\": out_degree()})\n\n    if tuple(direction) == (\"OUT\", \"IN\"):\n        return pd.DataFrame({\"OUT\": out_degree(), \"IN\": in_degree()})\n\n    return in_degree() if direction == \"IN\" else out_degree()\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.node_k_degree","title":"<code>node_k_degree(adj, node_properties=None, direction=('IN', 'OUT'), max_dim=-1, **kwargs)</code>","text":"<p>Compute generalized degree of nodes in network adj.  The k-(in/out)-degree of a node v is the number of k-simplices with all its nodes mapping to/from the node v.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>node_properties</code> <code>dataframe</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>direction</code> <code>string</code> <p>Direction for which to compute the degree</p> <p>'IN' - In degree</p> <p>'OUT'- Out degree</p> <p>(\u2019IN\u2019, \u2019OUT\u2019) - both</p> <code>('IN', 'OUT')</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension for which to compute the degree max_dim &gt;=2 or -1 in which case it computes all dimensions.</p> <code>-1</code> <p>Returns:</p> Type Description <code>data frame</code> <p>Table of of k-(in/out)-degrees</p> <p>Raises:</p> Type Description <code>Warning</code> <p>If adj has non-zero entries in the diagonal which are ignored in the analysis</p> <code>AssertionError</code> <p>If direction is invalid</p> <code>AssertionError</code> <p>If not max_dim &gt;1</p> Notes <p>Note that the k-in-degree of a node v is the number of (k+1) simplices the node v is a sink of. Dually, the k-out-degree of a node v is the number of (k+1) simplices the node v is a source of.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def node_k_degree(adj, node_properties=None, direction=(\"IN\", \"OUT\"), max_dim=-1, **kwargs):\n    #TODO: Generalize from one population to another\n    \"\"\"Compute generalized degree of nodes in network adj.  The k-(in/out)-degree of a node v is the number of\n    k-simplices with all its nodes mapping to/from the node v.\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j\n        of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.\n    node_properties : dataframe\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    direction : string\n        Direction for which to compute the degree\n\n        'IN' - In degree\n\n        'OUT'- Out degree\n\n        (\u2019IN\u2019, \u2019OUT\u2019) - both\n    max_dim : int\n        Maximal dimension for which to compute the degree max_dim &gt;=2 or -1 in\n        which case it computes all dimensions.\n\n    Returns\n    -------\n    data frame\n        Table of of k-(in/out)-degrees\n\n    Raises\n    ------\n    Warning\n        If adj has non-zero entries in the diagonal which are ignored in the analysis\n    AssertionError\n        If direction is invalid\n    AssertionError\n        If not max_dim &gt;1\n\n    Notes\n    -----\n    Note that the k-in-degree of a node v is the number of (k+1) simplices the node v is a sink of.\n    Dually, the k-out-degree of a node v is the number of (k+1) simplices the node v is a source of.\n    \"\"\"\n    matrix = sp.csr_matrix(adj)\n    assert (max_dim &gt; 1) or (max_dim==-1), \"max_dim should be &gt;=2\"\n    assert direction in (\"IN\", \"OUT\") or tuple(direction) == (\"IN\", \"OUT\"), \\\n        f\"Invalid `direction`: {direction}\"\n    if np.count_nonzero(matrix.diagonal()) != 0:\n        logging.warning('The diagonal is non-zero!  Non-zero entries in the diagonal will be ignored.')\n    import pyflagsercount\n    flagser_out = pyflagsercount.flagser_count(matrix, return_simplices=True, max_dim=max_dim)\n    max_dim_possible = len(flagser_out['cell_counts']) - 1\n    if max_dim==-1:\n        max_dim = max_dim_possible\n    elif max_dim &gt; max_dim_possible:\n        logging.warning(\"The maximum dimension selected is not attained\")\n        max_dim = max_dim_possible\n    if (max_dim &lt;= 1) and (max_dim!=-1):\n        print(\"There are no simplices of dimension 2 or higher\")\n    else:\n        index = pd.Series(range(matrix.shape[0]), name=\"node\")\n        generalized_degree = pd.DataFrame(index=index)\n        for dim in np.arange(2, max_dim + 1):\n            if \"OUT\" in direction:\n                # getting source participation across dimensions\n                x, y = np.unique(np.array(flagser_out['simplices'][dim])[:, 0], return_counts=True)\n                generalized_degree[f'{dim-1}_out_degree'] = pd.Series(y, index=x)\n            if \"IN\" in direction:\n                # getting sink participation across dimensions\n                x, y = np.unique(np.array(flagser_out['simplices'][dim])[:, dim], return_counts=True)\n                generalized_degree[f'{dim-1}_in_degree'] = pd.Series(y, index=x)\n        return generalized_degree.fillna(0)\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.node_participation","title":"<code>node_participation(adj, node_properties=None, max_simplices=False, threads=8, max_dim=-1, simplex_type='directed', **kwargs)</code>","text":"<p>Compute the number of simplex motifs in the network adj each node is part of. See simplex_counts for details.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>node_properties</code> <code>dataframe</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False (default) counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <code>simplex_type</code> <code>string</code> <p>Type of simplex to consider:</p> <p>\u2019directed\u2019 - directed simplices</p> <p>\u2019undirected\u2019 - simplices in the underlying undirected graph</p> <p>\u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections</p> <code>'directed'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>Indexed by the nodes in adj and with columns de dimension for which node participation is counted</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj has non-zero entries in the diagonal which can produce errors.</p> <code>AssertionError</code> <p>If adj is not square.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def node_participation(adj, node_properties=None, max_simplices=False,\n                       threads=8,max_dim=-1,simplex_type='directed',**kwargs):\n    \"\"\"Compute the number of simplex motifs in the network adj each node is part of.\n    See simplex_counts for details.\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    node_properties : dataframe\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    max_simplices : bool\n        If False (default) counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n    simplex_type : string\n        Type of simplex to consider:\n\n        \u2019directed\u2019 - directed simplices\n\n        \u2019undirected\u2019 - simplices in the underlying undirected graph\n\n        \u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections\n\n    Returns\n    -------\n    data frame\n        Indexed by the nodes in adj and with columns de dimension for which node participation is counted\n\n    Raises\n    -------\n    AssertionError\n        If adj has non-zero entries in the diagonal which can produce errors.\n    AssertionError\n        If adj is not square.\n    \"\"\"\n\n    adj=sp.csr_matrix(adj).astype('bool')\n    assert np.count_nonzero(adj.diagonal()) == 0, 'The diagonal of the matrix is non-zero and this may lead to errors!'\n    N, M = adj.shape\n    assert N == M, 'Dimension mismatch. The matrix must be square.'\n\n\n    #Symmetrize matrix if simplex_type is not 'directed'\n    if simplex_type=='undirected':\n        adj=sp.triu(underlying_undirected_matrix(adj)) #symmtrize and keep upper triangular only\n    elif simplex_type==\"reciprocal\":\n        adj=sp.triu(rc_submatrix(adj)) #symmtrize and keep upper triangular only\n\n    flagser_counts = _flagser_counts(adj, count_node_participation=True, threads=threads,\n                                     max_simplices=max_simplices, max_dim=max_dim)\n    return flagser_counts[\"node_participation\"]\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.normalised_simplex_count","title":"<code>normalised_simplex_count(matrix, dim=2)</code>","text":"<p>Computes the normalised simplex count of the flag complex of the graph with adjacency matrix matrix</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>2d-array</code> <p>Adjacency matrix of a directed network.</p> required <code>dim</code> <code>int</code> <p>The dimension to be computed, default=2</p> <code>2</code> <p>Returns:</p> Type Description <code>float</code> <p>The normalised simplex count of dimension dim of the flag complex of matrix</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def normalised_simplex_count(matrix, dim=2):\n    \"\"\"Computes the normalised simplex count of the flag complex of the graph with adjacency matrix matrix\n\n        Parameters\n        ----------\n        matrix : 2d-array\n            Adjacency matrix of a directed network.\n        dim : int\n             The dimension to be computed, default=2\n\n        Returns\n        -------\n        float\n            The normalised simplex count of dimension dim of the flag complex of matrix\n\n    \"\"\"\n    return normalized_simplex_counts(matrix)[dim]\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.normalized_simplex_counts","title":"<code>normalized_simplex_counts(adj, node_properties=None, max_simplices=False, threads=8, max_dim=-1, **kwargs)</code>","text":"<p>Compute the ratio of directed/undirected simplex counts normalized to be between 0 and 1. See simplex_counts and undirected_simplex_counts for details.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>node_properties</code> <code>dataframe</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <p>Returns:</p> Type Description <code>panda series</code> <p>Normalized simplex counts</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj has non-zero entries in the diagonal which can produce errors.</p> Notes <p>Maybe we should say why we choose this metric</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def normalized_simplex_counts(adj, node_properties=None,\n                   max_simplices=False, threads=8,max_dim=-1,\n                   **kwargs):\n    \"\"\"Compute the ratio of directed/undirected simplex counts normalized to be between 0 and 1.\n    See simplex_counts and undirected_simplex_counts for details.\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j\n        of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.\n    node_properties : dataframe\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    max_simplices : bool\n        If False counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n\n    Returns\n    -------\n    panda series\n        Normalized simplex counts\n\n    Raises\n    ------\n    AssertionError\n        If adj has non-zero entries in the diagonal which can produce errors.\n\n    Notes\n    -----\n    Maybe we should say why we choose this metric\"\"\"\n\n    from scipy.special import factorial\n    denominator=simplex_counts(adj, node_properties=node_properties,max_simplices=max_simplices,\n                                          threads=threads,max_dim=max_dim,simplex_type='undirected', **kwargs).to_numpy()\n    #Global maximum dimension since every directed simplex has an underlying undirected one of the same dimension\n    max_dim_global=denominator.size\n    #Maximum number of possible directed simplices for each undirected simplex across dimensions\n    max_possible_directed=np.array([factorial(i+1) for i in np.arange(max_dim_global)])\n    denominator=np.multiply(denominator, max_possible_directed)\n    numerator=simplex_counts(adj, node_properties=node_properties,max_simplices=max_simplices,\n                             threads=threads,max_dim=max_dim,simple_type='directed', **kwargs).to_numpy()\n    numerator=np.pad(numerator, (0, max_dim_global-len(numerator)), 'constant', constant_values=0)\n    return _series_by_dim(np.divide(numerator,denominator)[1:],name=\"normalized_simplex_counts\",\n                          index=np.arange(1,max_dim_global), name_index=\"dim\")\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.rc_submatrix","title":"<code>rc_submatrix(adj)</code>","text":"<p>Returns the symmetric submatrix of reciprocal connections of adj</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.</p> required <p>Returns:</p> Type Description <code>sparse matrix</code> <p>symmetric matrix of the same dtype as adj of reciprocal connections</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def rc_submatrix(adj):\n    \"\"\"Returns the symmetric submatrix of reciprocal connections of adj\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n\n    Returns\n    -------\n    sparse matrix\n        symmetric matrix of the same dtype as adj of reciprocal connections\n    \"\"\"\n    adj=sp.csr_matrix(adj)\n    if np.count_nonzero(adj.diagonal()) != 0:\n        logging.warning('The diagonal is non-zero and this may lead to errors!')\n    mask=adj.copy().astype('bool')\n    mask=(mask.multiply(mask.T))\n    mask.eliminate_zeros\n    return adj.multiply(mask).astype(adj.dtype)\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.simplex_counts","title":"<code>simplex_counts(adj, node_properties=None, max_simplices=False, threads=8, max_dim=-1, simplex_type='directed', nodes=None, **kwargs)</code>","text":"<p>Compute the number of simplex motifs in the network adj.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>node_properties</code> <code>dataframe</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <code>simplex_type</code> <p>Type of simplex to consider (See Notes):</p> <p>\u2019directed\u2019 - directed simplices</p> <p>\u2019undirected\u2019 - simplices in the underlying undirected graph</p> <p>\u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections</p> <code>'directed'</code> <code>nodes</code> <code>1d array or None(default)</code> <p>Restrict to list only the simplices whose source node is in nodes.  If None list all simplices. This only makes sense for directed simplices.</p> <code>None</code> <p>Returns:</p> Type Description <code>series</code> <p>simplex counts</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj has non-zero entries in the diagonal which can produce errors.</p> <code>AssertionError</code> <p>If adj is not square.</p> Notes <p>A directed simplex of dimension k in adj is a set of (k+1) nodes which are all to all connected in a feedforward manner. That is, they can be ordered from 0 to k such that there is an edge from i to j whenever i &lt; j.</p> <p>An undirected simplex of dimension k in adj is a set of (k+1) nodes in adj which are all to all connected.  That is, they are all to all connected in the underlying undirected graph of adj.  In the literature this is also called a (k+1)-clique of the underlying undirected graph.</p> <p>A reciprocal simplex of dimension k in adj is a set of (k+1) nodes in adj which are all to all reciprocally connected. That is, they are all to all connected in the undirected graph of reciprocal connections of adj.  In the literature this is also called a (k+1)-clique of the undirected graph of reciprocal connections.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def simplex_counts(adj, node_properties=None,max_simplices=False,\n                   threads=8,max_dim=-1, simplex_type='directed', nodes=None, **kwargs):\n    # TODO: ADD TRANSPOSE\n    \"\"\"Compute the number of simplex motifs in the network adj.\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j\n        of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.\n    node_properties : dataframe\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    max_simplices : bool\n        If False counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n    simplex_type: string\n        Type of simplex to consider (See Notes):\n\n        \u2019directed\u2019 - directed simplices\n\n        \u2019undirected\u2019 - simplices in the underlying undirected graph\n\n        \u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections\n    nodes : 1d array or None(default)\n        Restrict to list only the simplices whose source node is in nodes.  If None list all simplices.\n        This only makes sense for directed simplices.\n\n    Returns\n    -------\n    series\n        simplex counts\n\n    Raises\n    ------\n    AssertionError\n        If adj has non-zero entries in the diagonal which can produce errors.\n    AssertionError\n        If adj is not square.\n\n    Notes\n    -----\n    A directed simplex of dimension k in adj is a set of (k+1) nodes which are all to all connected in a feedforward manner.\n    That is, they can be ordered from 0 to k such that there is an edge from i to j whenever i &lt; j.\n\n    An undirected simplex of dimension k in adj is a set of (k+1) nodes in adj which are all to all connected.  That is, they\n    are all to all connected in the underlying undirected graph of adj.  In the literature this is also called a (k+1)-clique\n    of the underlying undirected graph.\n\n    A reciprocal simplex of dimension k in adj is a set of (k+1) nodes in adj which are all to all reciprocally connected.\n    That is, they are all to all connected in the undirected graph of reciprocal connections of adj.  In the literature this is\n    also called a (k+1)-clique of the undirected graph of reciprocal connections.\n    \"\"\"\n    adj=sp.csr_matrix(adj)\n    assert np.count_nonzero(adj.diagonal()) == 0, 'The diagonal of the matrix is non-zero and this may lead to errors!'\n    N, M = adj.shape\n    assert N == M, 'Dimension mismatch. The matrix must be square.'\n\n\n    #Symmetrize matrix if simplex_type is not 'directed'\n    if simplex_type=='undirected':\n        adj=sp.triu(underlying_undirected_matrix(adj)) #symmtrize and keep upper triangular only\n    elif simplex_type==\"reciprocal\":\n        adj=sp.triu(rc_submatrix(adj)) #symmtrize and keep upper triangular only\n\n    # Only the simplices that have sources stored in this temporary file will be considered.\n    # This only makes sense for directed simplices\n    if not nodes is None:\n        assert simplex_type=='directed', \"Sub-selection of source nodes only makes sense for directed simplices\"\n        import tempfile\n        import os\n        tmp_file = tempfile.NamedTemporaryFile(delete=False)\n        vertices_todo = tmp_file.name + \".npy\"\n        np.save(vertices_todo, nodes, allow_pickle=False)\n    else:\n        vertices_todo = ''\n\n    # Count simplices\n    flagser_counts = _flagser_counts(adj, threads=threads, max_simplices=max_simplices, max_dim=max_dim, vertices_todo=vertices_todo)\n    # Remove temporary file\n    if not nodes is None:\n        os.remove(vertices_todo)\n\n    if max_simplices:\n        return flagser_counts[\"max_simplex_counts\"]\n    else:\n        return flagser_counts[\"simplex_counts\"]\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.simplicial_rich_club_curve","title":"<code>simplicial_rich_club_curve(M, maximal=False, sparse_bin_set=False)</code>","text":"<p>Computes the simplicial rich club curve of a network.    Where the i'th entry is the density of the subnetwork induced by the vertices that are contained in    more than i (maximal) simplices.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d-array</code> <p>Adjacency matrix of a directed network.</p> required <code>max_simplices</code> <code>bool</code> <p>If true then vertex participation is the number of maximal simplices each vertex is contained in.</p> required <code>sparse_bin_set</code> <code>bool</code> <p>If true then consecutive entries with same rich club coefficient are grouped into bins together,</p> <code>False</code> <p>Returns:</p> Type Description <code>Series</code> <p>Where the i'th entry is the rich club coefficient of the network induced by all vertices which are contained in more that i (maximal) simplices</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def simplicial_rich_club_curve(M, maximal=False, sparse_bin_set=False):\n    \"\"\"Computes the simplicial rich club curve of a network.\n           Where the i'th entry is the density of the subnetwork induced by the vertices that are contained in\n           more than i (maximal) simplices.\n\n        Parameters\n        ----------\n        adj : 2d-array\n            Adjacency matrix of a directed network.\n        max_simplices : bool\n            If true then vertex participation is the number of maximal simplices each vertex is contained in.\n        sparse_bin_set : bool\n            If true then consecutive entries with same rich club coefficient are grouped into bins together,\n\n        Returns\n        -------\n        pandas.Series\n            Where the i'th entry is the rich club coefficient of the network induced by all vertices which are\n            contained in more that i (maximal) simplices\n\n    \"\"\"\n    import pyflagsercount\n    from .classic import efficient_rich_club_curve\n    vertex_par = pd.DataFrame(pyflagsercount.flagser_count(M, max_simplices=maximal, containment=True)['contain_counts']).replace(np.nan,0).astype(int)\n    return pd.DataFrame([efficient_rich_club_curve(M, pre_calculated_richness=vertex_par[i]) for i in range(vertex_par.shape[1])]).transpose().dropna(how='all')\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.tcc","title":"<code>tcc(matrix, center=0)</code>","text":"<p>Computes the transitive clustering coefficient of the graph induced by the neighbourhood of center <code>center</code> in the matrix</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>2d-array</code> <p>Adjacency matrix of a directed network.</p> required <code>center</code> <code>int</code> <p>The index of the vertex to be considered, default=0</p> <code>0</code> <p>Returns:</p> Type Description <code>float</code> <p>The transitive cluster coefficient of the neighbourhood of center</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def tcc(matrix,center=0):\n    \"\"\"Computes the transitive clustering coefficient of the graph induced by\n        the neighbourhood of center ``center`` in the matrix\n\n        Parameters\n        ----------\n\n        matrix : 2d-array\n            Adjacency matrix of a directed network.\n        center : int\n            The index of the vertex to be considered, default=0\n\n        Returns\n        -------\n        float\n            The transitive cluster coefficient of the neighbourhood of center\n\n    \"\"\"\n    from .classic import reciprocal_connections\n    outdeg = np.count_nonzero(matrix[center])\n    indeg = np.count_nonzero(np.transpose(matrix)[center])\n    repdeg = reciprocal_connections(matrix, chief_only=True)\n    totdeg = indeg+outdeg\n\n    #If matrix is not already neighbourhood of center, then compute neighbourhood.\n    if totdeg-repdeg+1 != len(matrix):\n        matrix = neighbourhood(center, matrix)\n        center = 0\n\n    chief_containment = node_participation(matrix,max_dim=2).iloc[0]\n    numerator = 0 if len(chief_containment) &lt; 3 else chief_containment[2]\n    denominator = (totdeg*(totdeg-1)-(indeg*outdeg+repdeg))\n\n    if denominator == 0:\n        return 0\n    return numerator/denominator\n</code></pre>"},{"location":"network_topology/#src.connalysis.network.topology.underlying_undirected_matrix","title":"<code>underlying_undirected_matrix(adj)</code>","text":"<p>Returns the symmetric matrix of undirected connections of <code>adj</code>.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry in <code>adj[i][j]</code> implies there is an edge from vertex <code>i</code> to vertex <code>j</code>.</p> required <p>Returns:</p> Type Description <code>sparse boolean matrix</code> <p>Corresponding to the symmetric underlying undirected graph</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def underlying_undirected_matrix(adj):\n    \"\"\"Returns the symmetric matrix of undirected connections of `adj`.\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry in `adj[i][j]` implies there is an edge from vertex `i` to vertex `j`.\n\n    Returns\n    -------\n    sparse boolean matrix\n        Corresponding to the symmetric underlying undirected graph\n    \"\"\"\n    adj=sp.csr_matrix(adj)\n    if np.count_nonzero(adj.diagonal()) != 0:\n        logging.warning('The diagonal is non-zero and this may lead to errors!')\n    return (adj+adj.T).astype('bool')\n</code></pre>"},{"location":"randomization/","title":"Randomization","text":""},{"location":"randomization/#this-page-describes-functions-contained-in-the-randomization-module-used-to-generate-of-randomized-controls-of-connectomes","title":"This page describes functions contained in the <code>randomization</code> module used to generate of randomized controls of connectomes.","text":""},{"location":"randomization/#src.connalysis.randomization.randomization.ER_model","title":"<code>ER_model(adj, threads=8, seed=(None, None))</code>","text":"<p>Creates an Erdos Renyi digraph.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse matrix or 2d-array</code> <p>Adjacency matrix</p> required <code>threads</code> <code>int</code> <p>Number of parallel threads to be used to generate model</p> <code>8</code> <code>seed</code> <code>pair of ints</code> <p>Random seed to be used, if none is provided a seed is randomly selected</p> <code>(None, None)</code> <p>Returns:</p> Type Description <code>coo matrix</code> <p>Matrix of the generated control</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj is not square</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def ER_model(adj, threads=8, seed=(None,None)):\n    \"\"\"Creates an Erdos Renyi digraph.\n\n    Parameters\n    ----------\n    adj : sparse matrix or 2d-array\n        Adjacency matrix\n    threads : int\n        Number of parallel threads to be used to generate model\n    seed : pair of ints\n        Random seed to be used, if none is provided a seed is randomly selected\n\n    Returns\n    -------\n    coo matrix\n        Matrix of the generated control\n\n\n    Raises\n    ------\n    AssertionError\n        If adj is not square\n\n    \"\"\"\n    assert adj.shape[0] == adj.shape[1], \"The matrix is not square\"\n    n = adj.shape[0]\n    p = adj.astype(bool).sum()/((n)*(n-1))\n    if isinstance(seed, int):\n      seed=(seed,seed)\n    return run_ER(n=n, p=p, threads=threads, seed=seed)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.ER_shuffle","title":"<code>ER_shuffle(adj, neuron_properties=[], shuffle_type='sparse')</code>","text":"<p>Creates an Erd\u0151s Renyi digraph with exactly the same number of edges and weights as adj by shuffling the non-diagonla entries of adj.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse matrix or 2d array</code> <p>Base digraph for which the ER control is built.</p> required <code>shuffle_type</code> <code>string</code> <p>If <code>dense</code> if first converts adj to a dense array and then shuffle. If <code>sparse</code> it keeps sparsity during the shuffle.</p> <code>'sparse'</code> <code>seed</code> <code>int</code> <p>Random seed to be used, if none is provided a seed is randomly selected</p> required <p>Returns:</p> Type Description <code>coo matrix</code> <p>Matrix of the generated control</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj is not not sparse and <code>shuffle_type</code> is set to sparse.  A sparse shuffle can only be done to a sparse matrix.</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>@_seed_random_state\ndef ER_shuffle(adj, neuron_properties=[], shuffle_type=\"sparse\"):\n    \"\"\"Creates an Erd\u0151s Renyi digraph with exactly the same number of edges and weights\n    as adj by shuffling the non-diagonla entries of adj.\n\n    Parameters\n    ----------\n    adj : sparse matrix or 2d array\n        Base digraph for which the ER control is built.\n    shuffle_type : string\n        If ``dense`` if first converts adj to a dense array and then shuffle.\n        If ``sparse`` it keeps sparsity during the shuffle.\n    seed : int\n        Random seed to be used, if none is provided a seed is randomly selected\n\n    Returns\n    -------\n    coo matrix\n        Matrix of the generated control\n\n    Raises\n    ------\n    AssertionError\n        If adj is not not sparse and ``shuffle_type`` is set to sparse.  A sparse shuffle can only be done to a sparse matrix.\n\n    \"\"\"\n    N = adj.shape[0]\n    if shuffle_type == \"dense\":\n        if sp.issparse(adj):\n            adj = adj.toarray()\n        above_diagonal = adj[np.triu_indices(N, k=1)]\n        below_diagonal = adj[np.tril_indices(N, k=-1)]\n        off_diagonal = np.concatenate([above_diagonal, below_diagonal])\n\n        # Shuffle away from the diagonal\n        np.random.shuffle(off_diagonal)\n        adj[np.triu_indices(N, k=1)] = off_diagonal[0:N * (N - 1) // 2]\n        adj[np.tril_indices(N, k=-1)] = off_diagonal[N * (N - 1) // 2:]\n        # Return matrix\n        return sp.coo_matrix(adj)\n    elif shuffle_type == \"sparse\":\n        # Keep matrix sparse\n        assert sp.issparse(adj), \"Matrix must be sparse for sparse shuffle\"\n        data = adj.data\n        K = adj.nnz  # number of non zero entries\n        T = N * (N - 1)  # number of entries away from the diagonal\n\n        # Select K entries in np.arange(T) representing the indices of the array of\n        # pairs of tuples of entries away from the diagonal\n        # [(0,1), (0,2), (0,3), ...,(0,n),\n        #  (1,0) (1,2) (1,3) ... (1,n),\n        # ....\n        #  (n,0) (n,1) (n,2) ... (n,n-1)]\n        rnd_index = np.random.choice(T, size=K, replace=False)\n\n        # Get corresponding row and column indices\n        rows = rnd_index // (N - 1)  # row indices\n        alpha = rnd_index % (N - 1)  # colum indices candidates shifted by 1\n        cols = alpha + 1 - (alpha &lt; rows).astype(int)  # column indices\n\n        # Return matrix\n        return sp.coo_matrix((data, (rows, cols)), shape=(N, N))\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization._seed_random_state","title":"<code>_seed_random_state(shuffler, seeder=np.random.seed)</code>","text":"<p>Decorate a connectivity shuffler to seed it's random-state before execution.</p> <p>It is expected that the generator can be seeded calling <code>seeder(seed)</code>.</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def _seed_random_state(shuffler, seeder=np.random.seed):\n    \"\"\"Decorate a connectivity shuffler to seed it's random-state before execution.\n\n    It is expected that the generator can be seeded calling `seeder(seed)`.\n    \"\"\"\n    def seed_and_run_method(adj, neuron_properties=[], seed=None, **kwargs):\n        \"\"\"Reinitialize numpy random state using the value of seed among `kwargs`.\n        doing nothing if no `seed` provided --- expecting an external initialization.\n        \"\"\"\n        if seed is None:\n            LOG.warning(\"No seed among keyword arguments\")\n        else:\n            seeder(seed)\n\n        return shuffler(adj, neuron_properties, **kwargs)\n\n    return seed_and_run_method\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.add_connections","title":"<code>add_connections(adj, nc, seed=0, sparse_mode=True, max_iter=30)</code>","text":"<p>Function add connections at random</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>matrix</code> <p>Adjacency matrix of a directed network</p> required <code>nc</code> <code>Number of connections to be added</code> required <code>seed</code> <code>int</code> <p>Random seed to be used to selecte edges that will become reciprocal</p> <code>0</code> <code>sparse_mode</code> <p>If sparse_mode is <code>True</code> the matrix is generated iteratively restricting to a sparse format. If <code>False</code> adj is converted to dense and edges are added in a single step</p> <code>True</code> <p>Returns:</p> Type Description <code>bool matrix</code> <p>Digraph with nc more edges than adj</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def add_connections(adj,nc, seed=0,sparse_mode=True, max_iter=30):\n    \"\"\"Function add connections at random\n\n    Parameters\n    ----------\n    adj : matrix\n        Adjacency matrix of a directed network\n    nc : Number of connections to be added\n    seed : int\n        Random seed to be used to selecte edges that will become reciprocal\n    sparse_mode: bool\n        If sparse_mode is ``True`` the matrix is generated iteratively restricting to a sparse format.\n        If ``False`` adj is converted to dense and edges are added in a single step\n\n    Returns\n    -------\n    bool matrix\n        Digraph with nc more edges than adj\n    \"\"\"\n    adj=adj.astype(bool)\n    # Add bidirectional connections\n    if sparse_mode:\n        # TODO: Search for more efficient way to do this\n        N=adj.shape[0]; E=adj.sum(); k=0 # Number nodes, target edges and iteration counter\n        while adj.sum()&lt; E +nc: #target number of edges\n            if k&gt;max_iter:\n                print(\"More than max_it iterations tried, increase number of iterations or try sparse_mode =False\")\n                break\n            den=(E+nc-adj.sum())/(N*N) #density of matrix added\n            generator = np.random.default_rng(seed)\n            A=sp.random(*adj.shape, density=den, format='csr', dtype = 'bool', random_state = generator)\n            A.setdiag(0)\n            adj=adj+A; k+=1\n        adj.eliminate_zeros()\n    else:\n        if sp.issparse(adj): adj=adj.toarray()\n        ul_ind = np.where(np.eye(*adj.shape) == 0) # non-diagonal indices\n        zero_ind=np.where(adj[ul_ind]==0)\n        generator = np.random.default_rng(seed)\n        selection=zero_ind[0][generator.choice(zero_ind[0].shape[0], replace=False, size=nc)]\n        adj[(ul_ind[0][selection],ul_ind[1][selection])]=1\n    return adj\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.add_rc_connections","title":"<code>add_rc_connections(adj, n_rc, seed=0)</code>","text":"<p>Function to turn a fixed amount of unidirectional connections of adj into reciprocal connections.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse matrix</code> <p>Adjacency matrix of a directed network</p> required <code>n_rc</code> <code>Number of reciprocal connections to be added</code> required <code>seed</code> <code>int</code> <p>Random seed to be used to selecte edges that will become reciprocal</p> <code>0</code> <p>Returns:</p> Type Description <code>matrix</code> <p>Digraph with n_rc more edges than adj, all of which form reciprocal connections</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def add_rc_connections(adj,n_rc, seed=0):\n    \"\"\"Function to turn a fixed amount of unidirectional connections of adj into reciprocal connections.\n\n    Parameters\n    ----------\n    adj : sparse matrix\n        Adjacency matrix of a directed network\n    n_rc : Number of reciprocal connections to be added\n    seed : int\n        Random seed to be used to selecte edges that will become reciprocal\n\n    Returns\n    -------\n    matrix\n        Digraph with n_rc more edges than adj, all of which form reciprocal connections\n    \"\"\"\n    # TODO: Move this function from utils and change dependencies\n    from .rand_utils import add_bidirectional_connections\n    # Add bidirectional connections\n    generator=np.random.default_rng(seed)\n    return add_bidirectional_connections(adj, n_rc, generator).astype(bool)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.add_rc_connections_skeleta","title":"<code>add_rc_connections_skeleta(adj, factors, dimensions=None, skeleta=None, threads=8, seed=0, return_skeleta=False)</code>","text":"<p>Function to add reciprocal connections at random to adj on the skeleta of maximal simplices of adj</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse matrix</code> <p>Adjacency matrix of a directed network</p> required <code>factors</code> <p>Factor by which to multiply the reciprocal connections on the <code>k</code>-skeleta of adj.  If factors is an int the same factor is used on all dimensions.  Otherwise, factors can be a dictionary with keys dimensions and values the factor by which to multiply the number of reciprocal connections on that dimensions.</p> required <code>dimensions</code> <p>The dimensions at which to increase the number of reciprocal connections.  If <code>None</code> then all dimensions will be used</p> <code>None</code> <code>skeleta</code> <p>Dictionary with keys f'dimension_{dim}' for dim in dimensions and values binary sparse sub-matrices of adj on which reciprocal connections will be added.</p> <code>None</code> <code>threads</code> <p>Number of threads on which to parallelize the skeleta computation if not pre-computed</p> <code>8</code> <code>seed</code> <code>int</code> <p>Random seed to be used to selecte edges that will become reciprocal</p> <code>0</code> <p>Returns:</p> Type Description <code>(csc_matrix, dict)</code> <p>Digraph with add reciprocal connections If return_skeleta=True it also returns the skeleta of maximal simplices of adj in the dimensions selected</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def add_rc_connections_skeleta(adj,factors,dimensions=None, skeleta=None, threads=8, seed=0, return_skeleta=False):\n    \"\"\"Function to add reciprocal connections at random to adj on the skeleta of maximal simplices of adj\n\n    Parameters\n    ----------\n    adj : sparse matrix\n        Adjacency matrix of a directed network\n    factors: int or dict\n        Factor by which to multiply the reciprocal connections on the ``k``-skeleta of adj.  If factors is an int\n        the same factor is used on all dimensions.  Otherwise, factors can be a dictionary with keys dimensions\n        and values the factor by which to multiply the number of reciprocal connections on that dimensions.\n    dimensions: array\n        The dimensions at which to increase the number of reciprocal connections.  If ``None`` then all dimensions\n        will be used\n    skeleta: dict\n        Dictionary with keys f'dimension_{dim}' for dim in dimensions and values binary sparse sub-matrices of adj\n        on which reciprocal connections will be added.\n    threads: int\n        Number of threads on which to parallelize the skeleta computation if not pre-computed\n    seed : int\n        Random seed to be used to selecte edges that will become reciprocal\n\n    Returns\n    -------\n    csc_matrix, dict\n        Digraph with add reciprocal connections\n        If return_skeleta=True it also returns the skeleta of maximal simplices of adj in the dimensions selected\n\n    \"\"\"\n    adj=adj.tocsr()\n    from connalysis.network.topology import rc_submatrix\n    from .rand_utils import add_bidirectional_connections\n    # Compute skeleton graphs if not precomputed\n    if skeleta is None:\n        from connalysis.network.topology import get_k_skeleta_graph\n        max_simplices=True # Add option for all simplices?\n        skeleta=get_k_skeleta_graph(adj, max_simplices=max_simplices, dimensions=dimensions, threads=threads)\n    # Restrict to dimensions that contain simplices\n    if dimensions is None:\n        dimensions = np.array([int(key[10:]) for key in skeleta.keys()])\n    else:\n        dimensions = np.intersect1d(np.array([int(key[10:]) for key in skeleta.keys()]), dimensions)\n    # Generate mapping between factors and dimensions or check the one provided\n    if isinstance(factors,int):\n        factors={dim:factors for dim in dimensions}\n    else:\n        assert isinstance(factors, dict), \"factors must be int or dictionary\"\n        assert np.isin(dimensions, np.array(list(factors.keys()))).all(), \"all dimensions must be a key in factors\"\n\n    # Add bidirectional connections\n    generator=np.random.default_rng(seed)\n    rc_add={dim:((factors[dim]-1)*(rc_submatrix(skeleta[f'dimension_{dim}']).sum()))//2 for dim in dimensions}\n    print(\"Number of reciprocal connections added per dimension\"); print(rc_add) # Remove or add verbose option\n    M=adj.copy()\n    for dim in dimensions:\n        M+=add_bidirectional_connections(skeleta[f'dimension_{dim}'], rc_add[dim], generator).astype(bool)\n    if return_skeleta:\n        return M, skeleta\n    else:\n        return M\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.adjusted_ER","title":"<code>adjusted_ER(adj, seed=None)</code>","text":"<p>Function to generate an Erdos  Renyi model with adjusted bidirectional connections.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code> csc_matrix</code> <p>Adjacency matrix of a directed network.</p> required <code>seed</code> <code>int</code> <p>Random seed to be used</p> <code>None</code> <p>Returns:</p> Type Description <code>csc_matrix</code> <p>Erdos Renyi shuffled control with additional reciprocal connections added at random to match the number of reciprocal connections of the original matrix.</p> See Also <p>underlying_model : Function which returns a digraph with the same  underlying undirected graph and same number of reciprocal connections</p> <p>bishuffled_model : Function which returns a digraph with shuffled reciprocal connections</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def adjusted_ER(adj, seed=None):\n    \"\"\"Function to generate an Erdos  Renyi model with adjusted bidirectional connections.\n\n    Parameters\n    ----------\n    adj :  csc_matrix\n        Adjacency matrix of a directed network.\n    seed : int\n        Random seed to be used\n\n    Returns\n    -------\n    csc_matrix\n        Erdos Renyi shuffled control with additional reciprocal connections added at random\n        to match the number of reciprocal connections of the original matrix.\n\n    See Also\n    --------\n    [underlying_model](randomization.md#src.connalysis.randomization.randomization.underlying_model) :\n    Function which returns a digraph with the same  underlying undirected graph\n    and same number of reciprocal connections\n\n    [bishuffled_model](randomization.md#src.connalysis.randomization.randomization.bishuffled_model) :\n    Function which returns a digraph with shuffled reciprocal connections\n    \"\"\"\n    from connalysis.network.topology import rc_submatrix\n    from .rand_utils import adjust_bidirectional_connections\n    generator = np.random.default_rng(seed)\n    ER_matrix = ER_shuffle(adj, seed=seed).tocsc()\n    bedges_to_add = int(rc_submatrix(adj).count_nonzero() -rc_submatrix(ER_matrix).count_nonzero())//2\n    if bedges_to_add &gt;= 0:\n        return adjust_bidirectional_connections(ER_matrix, bedges_to_add, generator)\n    else:\n        LOG.info(\"Erdos-Renyi control has more reciprocal connections than original, so they are not adjusted.\")\n        return ER_matrix\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.bishuffled_model","title":"<code>bishuffled_model(adj, seed=None)</code>","text":"<p>Function to generate a digraph with shuffled reciprocal connections</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>csc_matrix</code> <p>Adjacency matrix of a directed network.</p> required <code>seed</code> <code>int</code> <p>Random seed to be used</p> <code>None</code> <p>Returns:</p> Type Description <code>csc_matrix</code> <p>Digraph with shuffled reciprocal connections</p> See Also <p>adjusted_ER : Function to generate an Erdos  Renyi model with adjusted bidirectional connections</p> <p>underlying_model : Function which returns a digraph with the same  underlying undirected graph and same number of reciprocal connections</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def bishuffled_model(adj, seed = None):\n    \"\"\"Function to generate a digraph with shuffled reciprocal connections\n\n    Parameters\n    ----------\n    adj : csc_matrix\n        Adjacency matrix of a directed network.\n    seed : int\n        Random seed to be used\n\n    Returns\n    -------\n    csc_matrix\n        Digraph with shuffled reciprocal connections\n\n    See Also\n    --------\n    [adjusted_ER](randomization.md#src.connalysis.randomization.randomization.adjusted_ER) :\n    Function to generate an Erdos  Renyi model with adjusted bidirectional connections\n\n    [underlying_model](randomization.md#src.connalysis.randomization.randomization.underlying_model) :\n    Function which returns a digraph with the same  underlying undirected graph\n    and same number of reciprocal connections\n    \"\"\"\n    from connalysis.network.topology import rc_submatrix\n    from .rand_utils import  add_bidirectional_connections, half_matrix\n    generator = np.random.default_rng(seed)\n    ut_bedges = sp.triu(rc_submatrix(adj))\n    target_bedges = ut_bedges.count_nonzero()\n    bedges1, bedges2 = half_matrix(ut_bedges, generator)\n    return add_bidirectional_connections(adj - bedges1 - bedges2.T, target_bedges, generator)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.configuration_model","title":"<code>configuration_model(M, seed=None)</code>","text":"<p>Function to generate the configuration control model, obtained by shuffling the row and column of coo format independently, to create new coo matrix, then removing any multiple edges and loops.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>coo - matrix</code> <p>Adjacency matrix of a directed network.</p> required <code>seed</code> <code>int</code> <p>Random seed to be used</p> <code>None</code> <p>Returns:</p> Type Description <code>csr matrix</code> <p>Configuration model control of adj</p> See Also <p>run_SBM : Function which runs the stochastic block model</p> <p>run_DD2 : Function which runs the 2nd distance dependent model</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def configuration_model(M, seed = None):\n    \"\"\"Function to generate the configuration control model, obtained by\n    shuffling the row and column of coo format independently, to create\n    new coo matrix, then removing any multiple edges and loops.\n\n    Parameters\n    ----------\n    adj : coo-matrix\n        Adjacency matrix of a directed network.\n    seed : int\n        Random seed to be used\n\n    Returns\n    -------\n    csr matrix\n        Configuration model control of adj\n\n    See Also\n    --------\n    [run_SBM](randomization.md#src.connalysis.randomization.randomization.run_SBM) :\n    Function which runs the stochastic block model\n\n    [run_DD2](randomization.md#src.connalysis.randomization.randomization.run_DD2) :\n    Function which runs the 2nd distance dependent model\n    \"\"\"\n    adj=M.copy().tocoo()\n    generator = np.random.default_rng(seed)\n    R = adj.row\n    C = adj.col\n    generator.shuffle(R)\n    generator.shuffle(C)\n    CM_matrix = sp.coo_matrix(([1]*len(R),(R,C)),shape=adj.shape).tocsr()\n    CM_matrix.setdiag(0)\n    CM_matrix.eliminate_zeros()\n    return CM_matrix\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_DD2","title":"<code>run_DD2(n, a, b, xyz, threads=8, seed=(None, None))</code>","text":"<p>Creates a random digraph using the 2nd-order probability model.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of vertices</p> required <code>a</code> <code>float</code> <p>Coefficient of probability function</p> required <code>b</code> <code>float</code> <p>Absolute value of power of exponent in probability function</p> required <code>xyz</code> <code>(n,3)-numpy array of floats</code> <p>Co-ordinates of vertices in \\(\\mathbb{R}^3\\)</p> required <code>threads</code> <code>int</code> <p>Number of parallel threads to be used</p> <code>8</code> <code>seed</code> <code>pair of ints</code> <p>Random seed to be used, if none is provided a seed is randomly selected</p> <code>(None, None)</code> <p>Returns:</p> Type Description <code>coo matrix</code> <p>Matrix of the generated control</p> See Also <p>conn_prob_2nd_order_model : The modelling function from which the parameters <code>a</code> and <code>b</code>can be obtained.</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_DD2(n,a,b,xyz,threads=8, seed=(None,None)):\n    r\"\"\"Creates a random digraph using the 2nd-order probability model.\n\n    Parameters\n    ----------\n    n : int\n        Number of vertices\n    a : float\n        Coefficient of probability function\n    b : float\n        Absolute value of power of exponent in probability function\n    xyz : (n,3)-numpy array of floats\n        Co-ordinates of vertices in $\\mathbb{R}^3$\n    threads : int\n        Number of parallel threads to be used\n    seed : pair of ints\n        Random seed to be used, if none is provided a seed is randomly selected\n\n    Returns\n    -------\n    coo matrix\n        Matrix of the generated control\n\n    See Also\n    --------\n    [conn_prob_2nd_order_model](modelling.md#src.connalysis.modelling.modelling.conn_prob_2nd_order_model) :\n    The modelling function from which the parameters ``a`` and ``b``can be obtained.\n\n\n    \"\"\"\n    if seed[0]==None or seed[1]==None:\n        adj = gm.DD2(n,a,b,xyz,threads)\n    else:\n        adj = gm.DD2(n,a,b,xyz,threads,seed[0],seed[1])\n    return _dict_to_coo(adj,n)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_DD2_block","title":"<code>run_DD2_block(n, probs, blocks, xyz, threads, seed=(None, None))</code>","text":"<p>Creates a random digraph using a combination of the stochastic block model    and the 2nd order distance dependent model. Such that the probability of an edge    is given by the distance dependent equation, but the parameters of that equation    vary depending on the block of the source of the edge and block of the target.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of vertices</p> required <code>probs</code> <code>numpy array of floats</code> <p>shape=(m,m,2) where m is the number of blocks. For source vertex i and target vertex j probs[i][j][0] is the coefficient of the distance dependent equation (value a) and probs[i][j][0] is the absolute value of power of exponent in the distance dependent equation (value b)</p> required <code>blocks</code> <code>numpy array of ints</code> <p>shape=(n,). The i'th entry is which block vertex i belongs to.</p> required <code>xyz</code> <code>(n,3)-numpy array of floats</code> <p>Co-ordinates of vertices in \\(\\mathbb{R}^3\\)</p> required <code>threads</code> <code>int</code> <p>Number of parallel threads to be used</p> required <code>seed</code> <code>pair of ints</code> <p>Random seed to be used, if none is provided a seed is randomly selected</p> <code>(None, None)</code> <p>Returns:</p> Type Description <code>coo matrix</code> <p>Matrix of the generated control</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If blocks contains non-integers</p> See Also <p>run_DD2 : Function which runs the 2nd distance dependent model</p> <p>run_SBM : Function which runs the stochastic block model</p> <p>run_DD2_block_pre : Similar function that only accounts for the block of the source vertex</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_DD2_block(n, probs, blocks, xyz, threads, seed=(None,None)):\n    r\"\"\"Creates a random digraph using a combination of the stochastic block model\n       and the 2nd order distance dependent model. Such that the probability of an edge\n       is given by the distance dependent equation, but the parameters of that equation\n       vary depending on the block of the source of the edge and block of the target.\n\n    Parameters\n    ----------\n    n : int\n        Number of vertices\n    probs : numpy array of floats\n        shape=(m,m,2) where m is the number of blocks. For source vertex i and target vertex j\n        probs[i][j][0] is the coefficient of the distance dependent equation (value a) and\n        probs[i][j][0] is the absolute value of power of exponent in the distance dependent equation (value b)\n    blocks : numpy array of ints\n        shape=(n,). The i'th entry is which block vertex i belongs to.\n    xyz : (n,3)-numpy array of floats\n        Co-ordinates of vertices in $\\mathbb{R}^3$\n    threads : int\n        Number of parallel threads to be used\n    seed : pair of ints\n        Random seed to be used, if none is provided a seed is randomly selected\n\n    Returns\n    -------\n    coo matrix\n        Matrix of the generated control\n\n\n    Raises\n    ------\n    TypeError\n        If blocks contains non-integers\n\n    See Also\n    --------\n    [run_DD2](randomization.md#src.connalysis.randomization.randomization.run_DD2) :\n    Function which runs the 2nd distance dependent model\n\n    [run_SBM](randomization.md#src.connalysis.randomization.randomization.run_SBM) :\n    Function which runs the stochastic block model\n\n    [run_DD2_block_pre](randomization.md#src.connalysis.randomization.randomization.run_DD2_block_pre) :\n    Similar function that only accounts for the block of the source vertex\n\n    \"\"\"\n    if seed[0]==None or seed[1]==None:\n        adj = gm.DD2_block(n, probs, blocks, xyz, threads)\n    else:\n        adj = gm.DD2_block(n, probs, blocks, xyz, threads, seed[0], seed[1])\n    return _dict_to_coo(adj,n)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_DD2_block_pre","title":"<code>run_DD2_block_pre(n, probs, blocks, xyz, threads=8, seed=(None, None))</code>","text":"<p>Creates a random digraph using a combination of the stochastic block model    and the 2nd order distance dependent model. Such that the probability of an edge    is given by the distance dependent equation, but the parameters of that equation    vary depending on the block of the source of the edge.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of vertices</p> required <code>probs</code> <code>numpy array of floats</code> <p>shape=(m,2) where m is the number of blocks. probs[i][0] is the coefficient of the distance dependent equation (value a) for source vertex i and probs[i][0] is the absolute value of power of exponent in the distance dependent equation (value b)</p> required <code>blocks</code> <code>numpy array of ints</code> <p>shape=(n,). The i'th entry is which block vertex i belongs to.</p> required <code>xyz</code> <code>(n,3)-numpy array of floats</code> <p>Co-ordinates of vertices in \\(\\mathbb{R}^3\\)</p> required <code>threads</code> <code>int</code> <p>Number of parallel threads to be used</p> <code>8</code> <code>seed</code> <code>pair of ints</code> <p>Random seed to be used, if none is provided a seed is randomly selected</p> <code>(None, None)</code> <p>Returns:</p> Type Description <code>coo matrix</code> <p>Matrix of the generated control</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If blocks contains non-integers</p> See Also <p>run_SBM: Function which runs the stochastic block model</p> <p>run_DD2 : Function which runs the 2nd distance dependent model</p> <p>run_DD2_block : Similar function that also accounts for the block of the target vertex</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_DD2_block_pre(n, probs, blocks, xyz, threads=8, seed=(None,None)):\n    r\"\"\"Creates a random digraph using a combination of the stochastic block model\n       and the 2nd order distance dependent model. Such that the probability of an edge\n       is given by the distance dependent equation, but the parameters of that equation\n       vary depending on the block of the source of the edge.\n\n    Parameters\n    ----------\n    n : int\n        Number of vertices\n    probs : numpy array of floats\n        shape=(m,2) where m is the number of blocks.\n        probs[i][0] is the coefficient of the distance dependent equation (value a) for source vertex i and\n        probs[i][0] is the absolute value of power of exponent in the distance dependent equation (value b)\n    blocks : numpy array of ints\n        shape=(n,). The i'th entry is which block vertex i belongs to.\n    xyz : (n,3)-numpy array of floats\n        Co-ordinates of vertices in $\\mathbb{R}^3$\n    threads : int\n        Number of parallel threads to be used\n    seed : pair of ints\n        Random seed to be used, if none is provided a seed is randomly selected\n\n    Returns\n    -------\n    coo matrix\n        Matrix of the generated control\n\n\n    Raises\n    ------\n    TypeError\n        If blocks contains non-integers\n\n    See Also\n    --------\n    [run_SBM](randomization.md#src.connalysis.randomization.randomization.run_SBM):\n    Function which runs the stochastic block model\n\n    [run_DD2](randomization.md#src.connalysis.randomization.randomization.run_DD2) :\n    Function which runs the 2nd distance dependent model\n\n    [run_DD2_block](randomization.md#src.connalysis.randomization.randomization.run_DD2_block) :\n    Similar function that also accounts for the block of the target vertex\n\n    \"\"\"\n\n    if seed[0]==None or seed[1]==None:\n        adj = gm.DD2_block_pre(n, probs, blocks, xyz, threads)\n    else:\n        adj = gm.DD2_block_pre(n, probs, blocks, xyz, threads, seed[0], seed[1])\n    return _dict_to_coo(adj,n)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_DD2_model","title":"<code>run_DD2_model(adj, node_properties, model_params_dd2=None, coord_names=['x', 'y', 'z'], threads=8, return_params=False, **config_dict)</code>","text":"<p>Wrapper for fitting a model and generating a random control graph based on 2nd order distance dependence model.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>sparse matrix or 2d-array</code> <p>Adjacency matrix. If model_params_dd2 have already been computed, one can pass an empty matrix of the right size.</p> required <code>node_properties</code> <code>DataFrame</code> <p>DataFrame with information on the vertices of adj. It must have columns corresponding to the names of the coord_names to be used for distance computation (Default: ['x', 'y', 'z']).</p> required <code>model_params_dd2</code> <code>DataFrame</code> <p>Optional input of pre-computed model parameters as data frame with rows corresponding to seeds of model estimation (single row if subsampling is not used) and columns 'exp_model_scale' and 'exp_model_exponent' for the model parameters. See modelling.conn_prob_2nd_order_model for details.</p> <code>None</code> <code>coord_names</code> <code>list</code> <p>Names of the coordinates (corresponding to columns in neuron properties table) based on which to compute Euclidean distance. Default: ['x', 'y', 'z']</p> <code>['x', 'y', 'z']</code> <code>threads</code> <code>int</code> <p>Number of parallel threads to be used.</p> <code>8</code> <code>return_params</code> <code>bool</code> <p>If True, returns model_params_dd2 in addition to the generated control.</p> <code>False</code> <code>config_dict</code> <code>dict</code> <p>Dictionary with 2nd order model building settings. See modelling.conn_prob_2nd_order_model for details.</p> <code>{}</code> <p>Returns:</p> Type Description <code>coo_matrix</code> <p>Matrix of the generated control</p> <code>model_params_dd2</code> <p>pandas.DataFrame with model parameters (optional; if return_params is True)</p> See Also <p>conn_prob_2nd_order_model : The modelling function from which model_params_dd2 can be obtained.</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_DD2_model(adj, node_properties,\n                  model_params_dd2=None, #an analysis that could be loaded from the pipeline\n                  coord_names= ['x', 'y', 'z'],\n                  threads=8, return_params=False, **config_dict):\n    \"\"\"Wrapper for fitting a model and generating a random control graph based on 2nd order distance dependence model.\n\n    Parameters\n    ----------\n    adj : sparse matrix or 2d-array\n        Adjacency matrix.\n        If model_params_dd2 have already been computed, one can pass an empty matrix of the right size.\n    node_properties : pandas.DataFrame\n        DataFrame with information on the vertices of adj.\n        It must have columns corresponding to the names of the coord_names to be used for distance computation (Default: ['x', 'y', 'z']).\n    model_params_dd2 : pandas.DataFrame\n        Optional input of pre-computed model parameters as data frame with rows corresponding to seeds of model estimation\n        (single row if subsampling is not used) and columns 'exp_model_scale' and 'exp_model_exponent' for the model parameters.\n        See modelling.conn_prob_2nd_order_model for details.\n    coord_names : list\n        Names of the coordinates (corresponding to columns in neuron properties table) based on which to compute Euclidean distance.\n        Default: ['x', 'y', 'z']\n    threads : int\n        Number of parallel threads to be used.\n    return_params : bool\n        If True, returns model_params_dd2 in addition to the generated control.\n    config_dict : dict\n        Dictionary with 2nd order model building settings.\n        See modelling.conn_prob_2nd_order_model for details.\n\n    Returns\n    -------\n    coo_matrix\n        Matrix of the generated control\n    model_params_dd2\n        pandas.DataFrame with model parameters (optional; if return_params is True)\n\n    See Also\n    --------\n    [conn_prob_2nd_order_model](modelling.md#src.connalysis.modelling.modelling.conn_prob_2nd_order_model) :\n    The modelling function from which model_params_dd2 can be obtained.\n\n    \"\"\"\n\n    if model_params_dd2 is None:\n        from .import modelling\n        #TODO:  What to do if coord_names are also given in configdict and do not match coord_names?\n        config_dict[\"coord_names\"]=coord_names\n        model_params_dd2 = modelling.conn_prob_2nd_order_model(adj, node_properties,**config_dict)\n        LOG.warning(\"Fit parameters are used directly but should be checked by hand if the proper fit is obtained!\")\n\n    LOG.info(\"Run DD2 model with parameters: \\n%s\", model_params_dd2)\n\n    n = adj.shape[0]\n    a = model_params_dd2.mean(axis=0)['exp_model_scale']\n    b = model_params_dd2.mean(axis=0)['exp_model_exponent']\n    xyz = node_properties.loc[:,coord_names].to_numpy() #Make and assert that checks these columns exist!\n    if len(coord_names)&lt;3: #Extend by zeros if lower dimensional data was used to compute distance\n        xyz=np.hstack([xyz,np.zeros((xyz.shape[0],3-xyz.shape[1]))])\n    C=gm.DD2(n,a,b,xyz,threads)\n    i=C['row']\n    j=C['col']\n    data=np.ones(len(i))\n    if return_params==True:\n        return sp.coo_matrix((data, (i, j)), [n,n]), model_params_dd2\n    else:\n        return sp.coo_matrix((data, (i, j)), [n,n])\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_DD3","title":"<code>run_DD3(n, a1, b1, a2, b2, xyz, depths, threads=8, seed=(None, None))</code>","text":"<p>Creates a random digraph using the 2nd-order probability model.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of vertices</p> required <code>a1</code> <code>float</code> <p>Coefficient of probability function for negative depth</p> required <code>b1</code> <code>float</code> <p>Absolute value of power of exponent in probability function for negative depth</p> required <code>a2</code> <code>float</code> <p>Coefficient of probability function for positive depth</p> required <code>b2</code> <code>float</code> <p>Absolute value of power of exponent in probability function for positive depth</p> required <code>xyz</code> <code>(n,3)-numpy array of floats</code> <p>Co-ordinates of vertices in \\(\\mathbb{R}^3\\)</p> required <code>threads</code> <code>int</code> <p>Number of parallel threads to be used</p> <code>8</code> <code>seed</code> <code>pair of ints</code> <p>Random seed to be used, if none is provided a seed is randomly selected</p> <code>(None, None)</code> <p>Returns:</p> Type Description <code>coo matrix</code> <p>Matrix of the generated control</p> See Also <p>conn_prob_3rd_order_model : The modelling function from which the parameters <code>a1/a2</code> and <code>b1/b2</code>can be obtained.</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_DD3(n,a1,b1,a2,b2,xyz,depths,threads=8, seed=(None,None)):\n    r\"\"\"Creates a random digraph using the 2nd-order probability model.\n\n    Parameters\n    ----------\n    n : int\n        Number of vertices\n    a1 : float\n        Coefficient of probability function for negative depth\n    b1 : float\n        Absolute value of power of exponent in probability function for negative depth\n    a2 : float\n        Coefficient of probability function for positive depth\n    b2 : float\n        Absolute value of power of exponent in probability function for positive depth\n    xyz : (n,3)-numpy array of floats\n        Co-ordinates of vertices in $\\mathbb{R}^3$\n    threads : int\n        Number of parallel threads to be used\n    seed : pair of ints\n        Random seed to be used, if none is provided a seed is randomly selected\n\n    Returns\n    -------\n    coo matrix\n        Matrix of the generated control\n\n\n    See Also\n    --------\n    [conn_prob_3rd_order_model](modelling.md#src.connalysis.modelling.modelling.conn_prob_3rd_order_model) :\n    The modelling function from which the parameters ``a1/a2`` and ``b1/b2``can be obtained.\n\n\n\n    \"\"\"\n    if seed[0]==None or seed[1]==None:\n        adj = gm.DD3(n,a1,b1,a2,b2,xyz,depths,threads)\n    else:\n        adj = gm.DD3(n,a1,b1,a2,b2,xyz,depths,threads,seed[0],seed[1])\n    return _dict_to_coo(adj,n)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_ER","title":"<code>run_ER(n, p, threads=8, seed=(None, None))</code>","text":"<p>Creates an Erdos Renyi digraph.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of vertices</p> required <code>p</code> <code>float</code> <p>Edge probablity, must satisfy \\(0 \\le p \\le 1\\)</p> required <code>threads</code> <code>int</code> <p>Number of parallel threads to be used</p> <code>8</code> <code>seed</code> <code>pair of ints</code> <p>Random seed to be used, if none is provided a seed is randomly selected</p> <code>(None, None)</code> <p>Returns:</p> Type Description <code>coo matrix</code> <p>Matrix of the generated control</p> <p>Examples:</p> <p>Setting n=3 and p=1 gives the complete digraph on 3 vertices:</p> <pre><code>&gt;&gt;&gt; connalysis.randomization.run_ER(3,1)\n{'row': [0, 0, 1, 1, 2, 2], 'col': [1, 2, 0, 2, 0, 1]}\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If p is not between 0 and 1</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_ER(n, p, threads=8, seed=(None,None)):\n    \"\"\"Creates an Erdos Renyi digraph.\n\n    Parameters\n    ----------\n    n : int\n        Number of vertices\n    p : float\n        Edge probablity, must satisfy $0 \\\\le p \\\\le 1$\n    threads : int\n        Number of parallel threads to be used\n    seed : pair of ints\n        Random seed to be used, if none is provided a seed is randomly selected\n\n    Returns\n    -------\n    coo matrix\n        Matrix of the generated control\n\n    Examples\n    --------\n    Setting n=3 and p=1 gives the complete digraph on 3 vertices:\n    &gt;&gt;&gt; connalysis.randomization.run_ER(3,1)\n    {'row': [0, 0, 1, 1, 2, 2], 'col': [1, 2, 0, 2, 0, 1]}\n\n    Raises\n    ------\n    AssertionError\n        If p is not between 0 and 1\n\n    \"\"\"\n    assert (p &gt;= 0 and p &lt;= 1), \"p must be between 0 and 1\"\n    if seed[0]==None or seed[1]==None:\n        adj = gm.ER(n,p,threads)\n    else:\n        adj = gm.ER(n,p,threads,seed[0],seed[1])\n    return _dict_to_coo(adj,n)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_SBM","title":"<code>run_SBM(n, probs, blocks, threads=8, seed=(None, None))</code>","text":"<p>Creates a random digraph using the stochastic block model.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of vertices</p> required <code>probs</code> <code>numpy array of floats</code> <p>shape=(m,m) where m is the number of blocks. probs[i][j] is probability of an edge between block i and block j</p> required <code>blocks</code> <code>numpy array of ints</code> <p>shape=(n,). The i'th entry gives to which block vertex i belongs.</p> required <code>threads</code> <code>int</code> <p>Number of parallel threads to be used</p> <code>8</code> <code>seed</code> <code>pair of ints</code> <p>Random seed to be used, if none is provided a seed is randomly selected</p> <code>(None, None)</code> <p>Returns:</p> Type Description <code>coo matrix</code> <p>Matrix of the generated control</p> <p>Examples:</p> <p>To create an SBM digraph on 4 vertices where the even to odd, or odd to even, vertices connect with high probablity (p=0.9) and the even to evens or odd to odds connect with low probability (p=0.1):</p> <pre><code>&gt;&gt;&gt; connalysis.randomization.run_SBM(4,np.array([[0.1,0.9],[0.9,0.1]]),np.array([0,1,0,1]))\n{'row': [0, 0, 1, 1, 1, 2, 2, 3, 3], 'col': [1, 3, 0, 2, 3, 1, 3, 0, 2]\n</code></pre> <p>Raises:</p> Type Description <code>TypeError</code> <p>If blocks contains non-integers</p> References <p>[1] P.W. Holland, K. Laskey, S. Leinhardt, \"Stochastic Blockmodels: First Steps\", Soc Networks, 5-2, pp. 109-137, 1982</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_SBM(n, probs, blocks, threads=8, seed=(None,None)):\n    \"\"\"Creates a random digraph using the stochastic block model.\n\n    Parameters\n    ----------\n    n : int\n        Number of vertices\n    probs : numpy array of floats\n        shape=(m,m) where m is the number of blocks.\n        probs[i][j] is probability of an edge between block i and block j\n    blocks : numpy array of ints\n        shape=(n,). The i'th entry gives to which block vertex i belongs.\n    threads : int\n        Number of parallel threads to be used\n    seed : pair of ints\n        Random seed to be used, if none is provided a seed is randomly selected\n\n    Returns\n    -------\n    coo matrix\n        Matrix of the generated control\n\n    Examples\n    --------\n    To create an SBM digraph on 4 vertices where the even to\n    odd, or odd to even, vertices connect with high probablity (p=0.9)\n    and the even to evens or odd to odds connect with low probability (p=0.1):\n    &gt;&gt;&gt; connalysis.randomization.run_SBM(4,np.array([[0.1,0.9],[0.9,0.1]]),np.array([0,1,0,1]))\n    {'row': [0, 0, 1, 1, 1, 2, 2, 3, 3], 'col': [1, 3, 0, 2, 3, 1, 3, 0, 2]\n\n\n    Raises\n    ------\n    TypeError\n        If blocks contains non-integers\n\n    References\n    ----------\n    [1] P.W. Holland, K. Laskey, S. Leinhardt,\n    [\"Stochastic Blockmodels: First Steps\"](https://www.sciencedirect.com/science/article/pii/0378873383900217),\n    Soc Networks, 5-2, pp. 109-137, 1982\n\n    \"\"\"\n\n    if seed[0]==None or seed[1]==None:\n        adj = gm.SBM(n, probs, blocks, threads)\n    else:\n        adj = gm.SBM(n, probs, blocks, threads, seed[0], seed[1])\n    return _dict_to_coo(adj, n)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.underlying_model","title":"<code>underlying_model(adj, seed: int = None)</code>","text":"<p>Function to generate a digraph with the same  underlying undirected graph as adj     and the same number of reciprocal connections</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>csc_matrix</code> <p>Adjacency matrix of a directed network.</p> required <code>seed</code> <code>int</code> <p>Random seed to be used</p> <code>None</code> <p>Returns:</p> Type Description <code>csc_matrix</code> <p>Digraph with the same  underlying undirected graph as adj and the same number of reciprocal connections</p> See Also <p>adjusted_ER : Function to generate an Erdos  Renyi model with adjusted bidirectional connections</p> <p>bishuffled_model : Function which returns a digraph with shuffled reciprocal connections</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def underlying_model(adj, seed: int=None):\n    \"\"\"Function to generate a digraph with the same  underlying undirected graph as adj\n        and the same number of reciprocal connections\n\n    Parameters\n    ----------\n    adj : csc_matrix\n        Adjacency matrix of a directed network.\n    seed : int\n        Random seed to be used\n\n    Returns\n    -------\n    csc_matrix\n        Digraph with the same  underlying undirected graph as adj and the same number of reciprocal connections\n\n    See Also\n    --------\n    [adjusted_ER](randomization.md#src.connalysis.randomization.randomization.adjusted_ER) :\n    Function to generate an Erdos  Renyi model with adjusted bidirectional connections\n\n    [bishuffled_model](randomization.md#src.connalysis.randomization.randomization.bishuffled_model) :\n    Function which returns a digraph with shuffled reciprocal connections\n    \"\"\"\n    from connalysis.network.topology import rc_submatrix\n    from .rand_utils import  add_bidirectional_connections\n    generator = np.random.default_rng(seed)\n    target_bedges = int(rc_submatrix(adj).count_nonzero() / 2)\n    ut_matrix = sp.triu(adj + adj.T)\n    return add_bidirectional_connections(ut_matrix, target_bedges, generator)\n</code></pre>"}]}